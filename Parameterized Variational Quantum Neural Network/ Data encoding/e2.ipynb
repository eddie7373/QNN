{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "586edb7a-4b3b-475e-8736-e85f04cf1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as npp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# set the random seed\n",
    "npp.random.seed(42)\n",
    "\n",
    "# create a device to execute the circuit on\n",
    "dev = qml.device(\"default.qubit\", wires=4)\n",
    "\n",
    "@qml.qnode(dev, diff_method=\"parameter-shift\", interface=\"autograd\")\n",
    "def circuit(params,inputs):\n",
    "    qml.RY(inputs[0], wires=0)\n",
    "    qml.RY(inputs[1], wires=1)\n",
    "    qml.RY(inputs[2], wires=2)\n",
    "    qml.RY(inputs[3], wires=3)\n",
    "    \n",
    "\n",
    "    qml.U3(params[0],params[1],params[2], wires=0)\n",
    "    qml.U3(params[3],params[4],params[5], wires=1)\n",
    "    qml.U3(params[6],params[7],params[8], wires=2)\n",
    "    qml.U3(params[9],params[10],params[11], wires=3)\n",
    "    \n",
    "\n",
    "    qml.broadcast(qml.CNOT, wires=[0, 1, 2,3], pattern=\"ring\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    #return qml.expval(qml.PauliX(0) @ qml.PauliI(1)@ qml.PauliY(2)@ qml.PauliI(3))\n",
    "    return qml.expval(qml.PauliX(0) @  qml.PauliY(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a21f8a3e-a4ab-4c9d-bcf5-73c1b9429ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "def norminv(x):\n",
    "    return ((1.0/math.sqrt(2.0*math.pi)) * math.exp(-x*x*0.5))\n",
    "\n",
    "def d1(S0, K, r, T, sigma, q):\n",
    "    deno = (sigma * math.sqrt(T))\n",
    "    if (deno==0):\n",
    "        return 0\n",
    "    logReturns = math.log(S0/float(K)) if ((S0/float(K)) > 0.0) else 0.0\n",
    "    return (float(logReturns) + (float(r) - float(q) + float(sigma)*float(sigma)*0.5)*float(T)) / float(deno)\n",
    "    \n",
    "def d2(S0, K, r, T, sigma, q):\n",
    "        return d1(S0, K, r, T, sigma, q)-sigma*math.sqrt(T)\n",
    "        \n",
    "def bsformula(callput, S0, K, r, T, sigma, q=0):\n",
    "    N = stats.norm.cdf\n",
    "                \n",
    "    def optionValueOfCall(S0, K, r, T, sigma, q):       \n",
    "        _d1 = d1(S0, K, r, T, sigma, q)\n",
    "        _d2 = d2(S0, K, r, T, sigma, q)\n",
    "        return S0*math.exp(-q*T)*N(_d1)- K*math.exp(-r*T)*N(_d2)\n",
    "      \n",
    "    def optionValueOfPut(S0, K, r, T, sigma, q):\n",
    "        _d1 = d1(S0, K, r, T, sigma, q)\n",
    "        _d2 = d2(S0, K, r, T, sigma, q)\n",
    "        return float(K)*math.exp(-float(r)*float(T))*N(-_d2) - float(S0)*math.exp(-float(q)*float(T))*N(-_d1)\n",
    "        \n",
    "    def delta(callput, S0, K, r, T, sigma, q):\n",
    "        _d1 = d1(S0, K, r, T, sigma, q)        \n",
    "        if callput.lower() == \"call\":            \n",
    "            return N(_d1) * math.exp(-q*T)\n",
    "        else:\n",
    "            return (N(_d1)-1)* math.exp(-q*T)\n",
    "    \n",
    "    def vega(S0, K, r, T, sigma, q):\n",
    "        _d1 = d1(S0, K, r, T, sigma, q)\n",
    "        return S0  * math.sqrt(T) * norminv(_d1)  * math.exp(-q*T)\n",
    "    \n",
    "    if callput.lower()==\"call\":\n",
    "        optionValue = optionValueOfCall(S0, K, r, T, sigma, q)\n",
    "    else:\n",
    "        optionValue = optionValueOfPut(S0, K, r, T, sigma, q)\n",
    "        \n",
    "    _delta = delta(callput, S0, K, r, T, sigma, q)\n",
    "    _vega = vega(S0, K, r, T, sigma, q)\n",
    "    \n",
    "    return (optionValue, _delta, _vega)\n",
    "\n",
    "def bsm_iv_generator(num_sample = 100,tao_bound=[0.01,2.0],  sigma_bound=[0.01,2.0], \n",
    "                     money_bound=[0.3,3.0], rr_bound=[0.01,0.2], callput='call', seed=42):\n",
    "    \n",
    "    # input parameters: when callput is not in 'call' or 'put', randomly generate the option price followed by root-finding methods to\n",
    "    # compute the corresponding implied vol\n",
    "    # return: X_input = [time,stock,rr, dividen, option_price]. Y_outpu  = volatility \n",
    "    np.random.seed(seed)\n",
    "    tao_min,tao_max = tao_bound[0],tao_bound[1]\n",
    "    \n",
    "    sigma_min, sigma_max = sigma_bound[0],sigma_bound[1]\n",
    "    moneyness_min,moneyness_max = money_bound[0],money_bound[1]\n",
    "    rr_min,rr_max = rr_bound[0],rr_bound[1]\n",
    "   \n",
    "    \n",
    "\n",
    "    num_sample = int(num_sample)\n",
    "    xx = np.zeros([num_sample,4],dtype='float')\n",
    "    \n",
    "   \n",
    "    xx[:,0] = np.random.uniform(sigma_min, sigma_max,xx.shape[0])\n",
    "    xx[:,1] = np.random.uniform(tao_min,tao_max,xx.shape[0])\n",
    "    xx[:,2] = np.random.uniform(moneyness_min,moneyness_max,xx.shape[0])\n",
    "    xx[:,3] = np.random.uniform(rr_min,rr_max,xx.shape[0])\n",
    "   \n",
    "    \n",
    "   \n",
    "    strike=1.0 #fixed strike\n",
    "    #callput = 'call' # call option\n",
    "    v = np.zeros(xx.shape[0]) # option value\n",
    "    k = np.ones(xx.shape[0]) # strike price, just in order to match the shape of v\n",
    "    \n",
    "    if callput in ['call','put']:        \n",
    "        for i in range(0,xx.shape[0]):        \n",
    "            sigma, T, S0, interest = xx[i,0],xx[i,1],xx[i,2],xx[i,3]\n",
    "            ## use the Black-Schole function in compfin.py\n",
    "            v[i] = bsformula(callput, S0, strike, interest, T, sigma)[0]              \n",
    "            \n",
    "  \n",
    "    v= v.reshape(xx.shape[0],1)     \n",
    "    xx_sample = np.concatenate((xx,v),axis=1) #sigma, time, s, r, v\n",
    "    \n",
    "    \n",
    "    X_input   = xx_sample[:,1:]   # time,stock,rr, option_price\n",
    "    Y_output  =  xx_sample[:,0] # sigma -implied volatility is the predictive variable.\n",
    "  \n",
    "    return X_input,Y_output\n",
    "#  log-transformation of the option value\n",
    "def logscale_vol(x_train_dat,y_train_dat,otm_lower=0.0000001):\n",
    "   # input data: x_train_dat = [time,stock,rr, option_price], y_train_dat = sigma  \n",
    "   \n",
    "    xtv_train_log=x_train_dat.copy()    \n",
    "    ytv_train_log =y_train_dat.copy()\n",
    "    \n",
    "    \n",
    "    #v_lower[v_lower<0.0]=0.0 # V=max(S-E*exp(-rt),0)  \n",
    "    xintrinsic_train=xtv_train_log[:,1]-1.0*np.exp(-1.0*xtv_train_log[:,2]*xtv_train_log[:,0])\n",
    "    xintrinsic_train[xintrinsic_train<0.0]=0.0 ## \\tilde{V} = max(S-E*exp(-rt),0)\n",
    "    xtv_train_log[:,-1] = xtv_train_log[:,-1] -xintrinsic_train\n",
    "    \n",
    "    ## remove intrisinc values below the threshold (otm_lower \\approx machine pricision)  \n",
    "   \n",
    "    ytv_train_log = ytv_train_log[~np.less(xtv_train_log[:,-1],otm_lower)]\n",
    "    xtv_train_log = xtv_train_log[~np.less(xtv_train_log[:,-1],otm_lower),:]\n",
    "    xtv_train_log[:,-1]=np.log(xtv_train_log[:,-1])\n",
    "\n",
    "    return xtv_train_log,ytv_train_log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce82ab6f-af86-47ed-9c2c-ddf97780cd0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maturity time  range:\n",
      "0.500695213053119 0.59856504541106\n",
      "Stock price  range:\n",
      "0.9802024633538488 1.0196021540041706\n",
      "interest rate  range:\n",
      "0.030719674431487792 0.07952525710003366\n",
      "option value  range:\n",
      "0.09931091344863496 0.21642392770778063\n",
      "sigma range:\n",
      "0.30220884684944094 0.6947547746402069\n",
      "(100, 4)\n",
      "maturity time  range:\n",
      "0.500695213053119 0.59856504541106\n",
      "Stock price  range:\n",
      "0.9802024633538488 1.0196021540041706\n",
      "interest rate  range:\n",
      "0.030719674431487792 0.07952525710003366\n",
      "time option-value  range:\n",
      "-2.6542232063018565 -1.5996371627169406\n",
      "sigma range:\n",
      "0.30220884684944094 0.6947547746402069\n",
      "(80, 4)\n",
      "Parameters: [0.10312387 0.90255291 0.50525237 0.82645747 0.3200496  0.89552323\n",
      " 0.38920168 0.01083765 0.90538198 0.09128668 0.31931364 0.95006197]\n",
      "inputs: [0.95060715 0.57343789 0.63183721 0.44844552]\n",
      "Expectation value: -0.05493732499485565\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xin_input,Yin_output=bsm_iv_generator(num_sample = 100,tao_bound=[0.5,0.6],  sigma_bound=[0.3,0.7], \n",
    "                                      money_bound=[0.98,1.02], rr_bound=[0.03,0.08],callput='call')\n",
    "\n",
    "#check the data value range on each dimension\n",
    "## xin = [maturity time, Stock price, interest rate, dividend, option value]\n",
    "\n",
    "xlabel =  ['maturity time', 'Stock price', 'interest rate','option value']\n",
    "for i in range(0, len(xlabel)):\n",
    "    print(xlabel[i]+'  range:')\n",
    "    print(np.min(Xin_input[:,i]),np.max(Xin_input[:,i]))\n",
    "    \n",
    "print('sigma range:')\n",
    "print(np.min(Yin_output),np.max(Yin_output))\n",
    "print(np.shape(Xin_input))\n",
    "\n",
    "# generate and shuffle the data set into training and test part\n",
    "xtv_train_log_all,ytv_train_log_all=logscale_vol(Xin_input,Yin_output,otm_lower=1e-4)\n",
    "'''\n",
    "for i in range(4):\n",
    "    xtv_train_log_all[:,i]= min_max_normalization(xtv_train_log_all[:,i])\n",
    "'''\n",
    "#ytv_train_log_all=ytv_train_log_all/2\n",
    "xtv_train_log,xtv_test_log, ytv_train_log, ytv_test_log   = train_test_split(xtv_train_log_all,ytv_train_log_all,test_size=0.2,random_state=42)\n",
    "\n",
    "xlabel =  ['maturity time', 'Stock price', 'interest rate','time option-value']\n",
    "for i in range(0, len(xlabel)):\n",
    "    print(xlabel[i]+'  range:')\n",
    "    print(np.min(xtv_train_log_all[:,i]),np.max(xtv_train_log_all[:,i]))\n",
    "    \n",
    "print('sigma range:')\n",
    "print(np.min(ytv_train_log),np.max(ytv_train_log))\n",
    "## how many samples after cleaning\n",
    "print(np.shape(xtv_train_log))\n",
    "\n",
    "\n",
    "params = npp.random.random([12], requires_grad=True)\n",
    "inputs = npp.random.random([4], requires_grad=True)\n",
    "print(\"Parameters:\", params)\n",
    "print(\"inputs:\", inputs)\n",
    "print(\"Expectation value:\", circuit(params,inputs))\n",
    "\n",
    "\n",
    "qnode = qml.QNode(circuit, dev)\n",
    "qml.draw_mpl(circuit, decimals=1, style=\"sketch\")(params,inputs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc5c5020-7fea-46b3-85c1-c67a90814673",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.025700163690165256\n",
      "[-0.02570016 -0.01081764 -0.00867174 -0.00826333 -0.05516215 -0.01903371\n",
      " -0.00076792 -0.07438372 -0.04193533 -0.22170751  0.11736578  0.11019483]\n",
      "[-0.02570016 -0.01081764 -0.00867174 -0.00826333 -0.05516215 -0.01903371\n",
      " -0.00076792 -0.07438372 -0.04193533 -0.22170751  0.11736578  0.11019483]\n",
      "[-0.02570016 -0.01081764 -0.00867174 -0.00826333 -0.05516215 -0.01903371\n",
      " -0.00076792 -0.07438372 -0.04193533 -0.22170751  0.11736578  0.11019483]\n"
     ]
    }
   ],
   "source": [
    "def parameter_shift_term(qnode,params,inputs, i):\n",
    "    shifted = params.copy()\n",
    "    shifted[i] += np.pi/2\n",
    "    forward = qnode(shifted,inputs)  # forward evaluation\n",
    "\n",
    "    shifted[i] -= np.pi\n",
    "    backward = qnode(shifted,inputs) # backward evaluation\n",
    "\n",
    "    return 0.5 * (forward - backward)\n",
    "\n",
    "# gradient with respect to the first parameter\n",
    "print(parameter_shift_term(circuit,params,inputs, 0))\n",
    "\n",
    "\n",
    "def parameter_shift(qnode, params,inputs):\n",
    "    gradients = np.zeros([len(params)])\n",
    "\n",
    "    for i in range(len(params)):\n",
    "        gradients[i] = parameter_shift_term(qnode,params,inputs, i)\n",
    "\n",
    "    return gradients\n",
    "\n",
    "print(parameter_shift(circuit, params,inputs))\n",
    "\n",
    "grad_function = qml.grad(circuit)\n",
    "print(grad_function(params,inputs)[0])\n",
    "\n",
    "\n",
    "print(qml.gradients.param_shift(circuit)(params,inputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9715830-fa60-4d48-bbc4-27dc40dbdf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from itertools import chain\n",
    "import time\n",
    "def QNN(weights, angles):\n",
    "    return circuit(weights, angles)\n",
    "\n",
    "def cost(weights, features, labels):\n",
    "    predictions = [QNN(weights, f) for f in features]\n",
    "    \n",
    "    return square_loss(labels, predictions)\n",
    "\n",
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "def R2(labels, predictions):\n",
    "\n",
    "    r2 = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        r2 = r2 + metrics.r2_score(labels, predictions)\n",
    "    r2 = r2 / len(labels)\n",
    "\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48fdbeb0-8d5d-419e-be05-d99d3936b2fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X=xtv_train_log\n",
    "Y=ytv_train_log\n",
    "weights_init = npp.random.random([12], requires_grad=True)\n",
    "opt = qml.AdamOptimizer(0.01)\n",
    "batch_size = 20\n",
    "batches = len (X) // batch_size\n",
    "X_batches = npp.array_split(npp.arange(len(X)) , batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2218b3c-8997-4af9-84d6-318bffadf9d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.24850120018806598 R2: -16.900801691210585 time: 1703094450.5583222\n",
      "batch_idx: 1 loss: 0.2505436725576472 R2: -16.790075165284264 time: 1703094453.1460407\n",
      "batch_idx: 2 loss: 0.21430992614260275 R2: -16.666695969301394 time: 1703094455.8294244\n",
      "batch_idx: 3 loss: 0.24866205578410033 R2: -16.529870542272565 time: 1703094458.5143075\n",
      "Training [0%] Loss: 0.2405042136681041 time: 1703094458.5143075\n",
      "weight: [0.1430577  0.94252358 0.54516041 0.80224478 0.36000892 0.93549491\n",
      " 0.42767808 0.05090107 0.94532329 0.13131762 0.27934895 0.91009921]\n",
      "epoch 2\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.24154029658672246 R2: -16.378793929459633 time: 1703094461.1920297\n",
      "batch_idx: 1 loss: 0.24210651035302969 R2: -16.212932747629473 time: 1703094463.9313338\n",
      "batch_idx: 2 loss: 0.20646159754610985 R2: -16.031707624209954 time: 1703094466.5464997\n",
      "batch_idx: 3 loss: 0.23894005635573184 R2: -15.83436594874782 time: 1703094469.230832\n",
      "Training [0%] Loss: 0.23226211521039847 time: 1703094469.230832\n",
      "weight: [0.18343415 0.98300524 0.58399837 0.82832404 0.40048807 0.97599485\n",
      " 0.46583664 0.09147607 0.98561124 0.17184586 0.23886956 0.86962486]\n",
      "epoch 3\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.23138192503528554 R2: -15.620276385668145 time: 1703094471.805782\n",
      "batch_idx: 1 loss: 0.23016356837323335 R2: -15.388801823191372 time: 1703094474.501829\n",
      "batch_idx: 2 loss: 0.19539032478491497 R2: -15.139783455670038 time: 1703094477.1750178\n",
      "batch_idx: 3 loss: 0.22547643422030994 R2: -14.872142791589116 time: 1703094479.8545525\n",
      "Training [1%] Loss: 0.22060306310343597 time: 1703094479.8545525\n",
      "weight: [0.22478864 1.02419571 0.60906901 0.86516004 0.44194106 1.01742839\n",
      " 0.50562864 0.13297311 1.02638647 0.21322879 0.19745973 0.82822889]\n",
      "epoch 4\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.21747663220705266 R2: -14.585392548389583 time: 1703094482.475742\n",
      "batch_idx: 1 loss: 0.21418241366843085 R2: -14.278768383194796 time: 1703094485.1587145\n",
      "batch_idx: 2 loss: 0.18061696695276255 R2: -13.95304290123477 time: 1703094487.7794409\n",
      "batch_idx: 3 loss: 0.20776094908652917 R2: -13.607183094900645 time: 1703094490.4155529\n",
      "Training [1%] Loss: 0.20500924047869382 time: 1703094490.4155529\n",
      "weight: [0.26753324 1.06606508 0.59799254 0.90577677 0.48455883 1.05990339\n",
      " 0.5474526  0.17565351 1.06747529 0.25559935 0.15488351 0.7856855 ]\n",
      "epoch 5\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.19937122002626995 R2: -13.241594950661442 time: 1703094493.0380208\n",
      "batch_idx: 1 loss: 0.19380322348940332 R2: -12.856047864716379 time: 1703094495.6693785\n",
      "batch_idx: 2 loss: 0.16191059631008434 R2: -12.452756214345865 time: 1703094498.2795575\n",
      "batch_idx: 3 loss: 0.1856617789729374 R2: -12.030889943724763 time: 1703094501.001406\n",
      "Training [1%] Loss: 0.18518670469967374 time: 1703094501.001406\n",
      "weight: [0.31189123 1.1091661  0.56589384 0.94827008 0.52815195 1.10308753\n",
      " 0.59139466 0.21962964 1.10837298 0.29882985 0.11107929 0.74194647]\n",
      "epoch 6\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.17708840572607673 R2: -11.591931049328284 time: 1703094503.7078488\n",
      "batch_idx: 1 loss: 0.16926960175780975 R2: -11.136316661881015 time: 1703094506.4256074\n",
      "batch_idx: 2 loss: 0.1396535813883284 R2: -10.667926014780615 time: 1703094509.2620928\n",
      "batch_idx: 3 loss: 0.15975997659452743 R2: -10.18612122223456 time: 1703094511.9072523\n",
      "Training [1%] Loss: 0.16144289136668558 time: 1703094511.9072523\n",
      "weight: [0.35787966 1.15402428 0.5251749  0.99171955 0.57192981 1.14594545\n",
      " 0.63736344 0.26485476 1.14817256 0.34242616 0.06618112 0.69716447]\n",
      "epoch 7\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.15137954816927995 R2: -9.693556125127882 time: 1703094514.5333352\n",
      "batch_idx: 1 loss: 0.1416110914895264 R2: -9.191399576236332 time: 1703094517.2003255\n",
      "batch_idx: 2 loss: 0.11497479854742124 R2: -8.685081693140278 time: 1703094519.7972734\n",
      "batch_idx: 3 loss: 0.13144223580447245 R2: -8.17391431223424 time: 1703094522.4374838\n",
      "Training [1%] Loss: 0.134851918502675 time: 1703094522.4374838\n",
      "weight: [0.40529756 1.20060288 0.48010593 1.0358261  0.61405139 1.18631256\n",
      " 0.68510011 0.31110169 1.18551939 0.38532516 0.02055598 0.65173344]\n",
      "epoch 8\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.12377853860402488 R2: -7.661408559345534 time: 1703094525.0630054\n",
      "batch_idx: 1 loss: 0.11263217142257151 R2: -7.149181764613938 time: 1703094527.61592\n",
      "batch_idx: 2 loss: 0.08969432816264031 R2: -6.6436013693752445 time: 1703094530.253116\n",
      "batch_idx: 3 loss: 0.1027608145404442 R2: -6.143361160401708 time: 1703094533.0012825\n",
      "Training [2%] Loss: 0.10721646318242023 time: 1703094533.0012825\n",
      "weight: [ 0.45369576  1.24832461  0.4327596   1.07888165  0.65101715  1.22071139\n",
      "  0.73414643  0.35793451  1.2187173   0.42557266 -0.02515187  0.6063363 ]\n",
      "epoch 9\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.09638779220015263 R2: -5.652133384547752 time: 1703094535.6816063\n",
      "batch_idx: 1 loss: 0.08459730813196284 R2: -5.171469130596747 time: 1703094538.2955692\n",
      "batch_idx: 2 loss: 0.06596259707676318 R2: -4.707831538469402 time: 1703094541.0188053\n",
      "batch_idx: 3 loss: 0.07598030793843419 R2: -4.258709296625263 time: 1703094543.6461754\n",
      "Training [2%] Loss: 0.0807320013368282 time: 1703094543.6461754\n",
      "weight: [ 0.50233139  1.29598636  0.38460913  1.11228726  0.67788565  1.24549266\n",
      "  0.78379438  0.40467851  1.24611136  0.45998055 -0.06999042  0.56197605]\n",
      "epoch 10\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.07138087613346818 R2: -3.8272724990160882 time: 1703094546.3149915\n",
      "batch_idx: 1 loss: 0.05968830509487992 R2: -3.414709023912401 time: 1703094548.9189792\n",
      "batch_idx: 2 loss: 0.04572250168670161 R2: -3.026838937761518 time: 1703094551.512082\n",
      "batch_idx: 3 loss: 0.05304066332480717 R2: -2.6598348666315212 time: 1703094554.1137133\n",
      "Training [2%] Loss: 0.05745808655996422 time: 1703094554.1137133\n",
      "weight: [ 0.55012776  1.34164355  0.33703203  1.12741901  0.69105014  1.25887347\n",
      "  0.83303317  0.45039898  1.26668484  0.48426787 -0.11272383  0.51994442]\n",
      "epoch 11\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.05049841437705057 R2: -2.315932471638063 time: 1703094556.6384656\n",
      "batch_idx: 1 loss: 0.03954605725370207 R2: -1.9958539976613672 time: 1703094559.297057\n",
      "batch_idx: 2 loss: 0.030270160930767464 R2: -1.703889856694077 time: 1703094561.9914992\n",
      "batch_idx: 3 loss: 0.035207409360414374 R2: -1.4352877637183072 time: 1703094564.5761504\n",
      "Training [2%] Loss: 0.03888051048048362 time: 1703094564.5761504\n",
      "weight: [ 0.59567578  1.38282361  0.29155345  1.13156445  0.6915251   1.26194559\n",
      "  0.880517    0.49390486  1.28050589  0.49450327 -0.1519618   0.48167423]\n",
      "epoch 12\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.03473125107747556 R2: -1.1909031624313637 time: 1703094567.2633097\n",
      "batch_idx: 1 loss: 0.024985238393862956 R2: -0.9708639758299951 time: 1703094569.9092083\n",
      "batch_idx: 2 loss: 0.019966660984805828 R2: -0.7771614302769474 time: 1703094572.6159475\n",
      "batch_idx: 3 loss: 0.022845365603165347 R2: -0.6049004158476812 time: 1703094575.2834826\n",
      "Training [2%] Loss: 0.02563212901482742 time: 1703094575.2834826\n",
      "weight: [ 0.63733623  1.41735275  0.24995224  1.12862733  0.68406338  1.2584422\n",
      "  0.92460174  0.53380936  1.28868092  0.48929789 -0.18642321  0.44845896]\n",
      "epoch 13\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.024124443581465878 R2: -0.4534527709092727 time: 1703094578.0282416\n",
      "batch_idx: 1 loss: 0.015808652745037798 R2: -0.32236388420450895 time: 1703094580.7281098\n",
      "batch_idx: 2 loss: 0.014158923160904368 R2: -0.21136389849761464 time: 1703094583.3501961\n",
      "batch_idx: 3 loss: 0.015342189404390694 R2: -0.11631662764033379 time: 1703094586.0868366\n",
      "Training [3%] Loss: 0.017358552222949686 time: 1703094586.0868366\n",
      "weight: [ 0.67349878  1.44432957  0.21406168  1.1209057   0.67391211  1.25245516\n",
      "  0.96352395  0.56869373  1.29288253  0.47027668 -0.2152745   0.4211206 ]\n",
      "epoch 14\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.017811195551348573 R2: -0.03560490114851289 time: 1703094588.7361844\n",
      "batch_idx: 1 loss: 0.010899899938681042 R2: 0.03147759430325951 time: 1703094591.459502\n",
      "batch_idx: 2 loss: 0.011467916087051227 R2: 0.08663108074820254 time: 1703094594.112893\n",
      "batch_idx: 3 loss: 0.011368147851880447 R2: 0.13255988365648008 time: 1703094596.828802\n",
      "Training [3%] Loss: 0.012886789857240323 time: 1703094596.828802\n",
      "weight: [ 0.70295907  1.4642786   0.18521139  1.11379631  0.66467625  1.24667712\n",
      "  0.99575059  0.5973825   1.29479517  0.44048717 -0.23836319  0.3998002 ]\n",
      "epoch 15\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.014387521303332668 R2: 0.17108177996335833 time: 1703094599.4190545\n",
      "batch_idx: 1 loss: 0.008684490855427348 R2: 0.20271852044853889 time: 1703094602.0701919\n",
      "batch_idx: 2 loss: 0.010351789633641159 R2: 0.22957813419611622 time: 1703094604.7068875\n",
      "batch_idx: 3 loss: 0.009411265081343741 R2: 0.25275507336398695 time: 1703094607.3285873\n",
      "Training [3%] Loss: 0.010708766718436229 time: 1703094607.3285873\n",
      "weight: [ 0.72524107  1.47849684  0.16373335  1.11038137  0.65781558  1.24224067\n",
      "  1.02039031  0.61924697  1.29574182  0.40290335 -0.25618744  0.38399611]\n",
      "epoch 16\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.012458202941653302 R2: 0.2736687834590263 time: 1703094609.882343\n",
      "batch_idx: 1 loss: 0.007709885250261994 R2: 0.2924295508206915 time: 1703094612.4656804\n",
      "batch_idx: 2 loss: 0.009623522237232238 R2: 0.3108521239832658 time: 1703094615.2203724\n",
      "batch_idx: 3 loss: 0.008271948178217182 R2: 0.3287312645291739 time: 1703094617.9050345\n",
      "Training [3%] Loss: 0.009515889651841178 time: 1703094617.9050345\n",
      "weight: [ 0.74069017  1.48840857  0.14897958  1.11194853  0.65328001  1.23915609\n",
      "  1.03745525  0.6343935   1.29655462  0.35994423 -0.26963885  0.37280001]\n",
      "epoch 17\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011043584182887024 R2: 0.34688778629331907 time: 1703094620.6516213\n",
      "batch_idx: 1 loss: 0.007019349845278439 R2: 0.3650641801091654 time: 1703094623.460369\n",
      "batch_idx: 2 loss: 0.008678805393928252 R2: 0.38453173005599617 time: 1703094626.127039\n",
      "batch_idx: 3 loss: 0.007280070173769762 R2: 0.40434208186028525 time: 1703094628.7645545\n",
      "Training [3%] Loss: 0.00850545239896587 time: 1703094628.7645545\n",
      "weight: [ 0.7503222   1.4952789   0.13972686  1.12111393  0.6504249   1.23686321\n",
      "  1.04783907  0.64364542  1.29762032  0.31357845 -0.27969931  0.36517106]\n",
      "epoch 18\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.009690204762483617 R2: 0.4248299029607671 time: 1703094631.485409\n",
      "batch_idx: 1 loss: 0.006209782345211688 R2: 0.44556536527290536 time: 1703094634.188281\n",
      "batch_idx: 2 loss: 0.007425973711166807 R2: 0.4673355658326793 time: 1703094636.7984083\n",
      "batch_idx: 3 loss: 0.006235507282550196 R2: 0.4889898677041565 time: 1703094639.4818614\n",
      "Training [4%] Loss: 0.0073903670253530765 time: 1703094639.4818614\n",
      "weight: [ 0.75555212  1.50013282  0.13457399  1.1422506   0.64860432  1.23462792\n",
      "  1.05306398  0.64834784  1.29899976  0.2655523  -0.28723643  0.36012989]\n",
      "epoch 19\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.008345378945376281 R2: 0.5106045703120073 time: 1703094642.2284822\n",
      "batch_idx: 1 loss: 0.005270810188329471 R2: 0.5317705260346086 time: 1703094645.0387545\n",
      "batch_idx: 2 loss: 0.006062261822963294 R2: 0.552938473857653 time: 1703094647.7656271\n",
      "batch_idx: 3 loss: 0.005211873567135496 R2: 0.57313803249753 time: 1703094650.3743124\n",
      "Training [4%] Loss: 0.006222581130951136 time: 1703094650.3743124\n",
      "weight: [ 0.75793015  1.50371071  0.13214672  1.17636913  0.64738959  1.23187805\n",
      "  1.05495081  0.65010054  1.30055127  0.21757053 -0.29292883  0.35684914]\n",
      "epoch 20\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.007146700805576301 R2: 0.5923963021748897 time: 1703094652.9930058\n",
      "batch_idx: 1 loss: 0.004366748093568387 R2: 0.6104346848209818 time: 1703094655.6149824\n",
      "batch_idx: 2 loss: 0.00485311816207283 R2: 0.6275157356768358 time: 1703094658.3386908\n",
      "batch_idx: 3 loss: 0.004359530839145488 R2: 0.6430945908532374 time: 1703094661.0280974\n",
      "Training [4%] Loss: 0.005181524475090751 time: 1703094661.0280974\n",
      "weight: [ 0.75893771  1.50642593  0.13115658  1.21486757  0.64657748  1.22883554\n",
      "  1.05532335  0.65050592  1.30203268  0.17140151 -0.29728005  0.35467405]\n",
      "epoch 21\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.006235300832798529 R2: 0.657373630157224 time: 1703094663.7142951\n",
      "batch_idx: 1 loss: 0.0036577538060781093 R2: 0.6702166158990595 time: 1703094666.4920888\n",
      "batch_idx: 2 loss: 0.0039734345637790726 R2: 0.6818367833822399 time: 1703094669.1440737\n",
      "batch_idx: 3 loss: 0.003750821194023579 R2: 0.6922612497644428 time: 1703094671.9769294\n",
      "Training [4%] Loss: 0.004404327599169823 time: 1703094671.9769294\n",
      "weight: [ 0.75982173  1.50838212  0.13045109  1.23559295  0.64609587  1.22704472\n",
      "  1.0557551   0.65094615  1.30318861  0.12884927 -0.30066407  0.35311609]\n",
      "epoch 22\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.005616474092755125 R2: 0.701919472312006 time: 1703094674.5930297\n",
      "batch_idx: 1 loss: 0.0031725764277712857 R2: 0.7106210637157651 time: 1703094677.2074223\n",
      "batch_idx: 2 loss: 0.003410535198427962 R2: 0.718521236162305 time: 1703094679.8454502\n",
      "batch_idx: 3 loss: 0.0033160109392489376 R2: 0.7258548246252674 time: 1703094682.5061388\n",
      "Training [4%] Loss: 0.0038788991645508277 time: 1703094682.5061388\n",
      "weight: [ 0.76143258  1.50952497  0.12916921  1.22045584  0.64590373  1.22843637\n",
      "  1.05732576  0.65236978  1.30383534  0.09146529 -0.30336033  0.35184694]\n",
      "epoch 23\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.005164253726222633 R2: 0.7329398724500801 time: 1703094685.3452456\n",
      "batch_idx: 1 loss: 0.0028243077234567228 R2: 0.7394887308304304 time: 1703094688.0696158\n",
      "batch_idx: 2 loss: 0.0030421676784683762 R2: 0.745549448927404 time: 1703094690.6684303\n",
      "batch_idx: 3 loss: 0.0029683836552050038 R2: 0.7513361520440397 time: 1703094693.224148\n",
      "Training [5%] Loss: 0.0034997781958381836 time: 1703094693.224148\n",
      "weight: [ 0.76414764  1.50986847  0.12691009  1.18180083  0.64597471  1.23323692\n",
      "  1.06050088  0.65518881  1.30391281  0.06013774 -0.3055707   0.35068345]\n",
      "epoch 24\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.004787184957353781 R2: 0.7571016347554502 time: 1703094695.8295295\n",
      "batch_idx: 1 loss: 0.0025443665368981164 R2: 0.7625181624546701 time: 1703094698.4861054\n",
      "batch_idx: 2 loss: 0.002777585781692567 R2: 0.7675973476854024 time: 1703094701.0713975\n",
      "batch_idx: 3 loss: 0.0026697126741824554 R2: 0.772524969824376 time: 1703094703.7122183\n",
      "Training [5%] Loss: 0.00319471248753173 time: 1703094703.7122183\n",
      "weight: [ 0.7679775   1.50960395  0.1237066   1.13566871  0.64624703  1.23970988\n",
      "  1.06526622  0.65939755  1.30346597  0.03498601 -0.30743478  0.34954658]\n",
      "epoch 25\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0044488581933000825 R2: 0.777537131500317 time: 1703094706.301608\n",
      "batch_idx: 1 loss: 0.0023035684269242787 R2: 0.7823050536449347 time: 1703094708.9567862\n",
      "batch_idx: 2 loss: 0.002566996193263196 R2: 0.7868427417105228 time: 1703094711.4394963\n",
      "batch_idx: 3 loss: 0.0024029490236523155 R2: 0.7913039949555694 time: 1703094714.0275562\n",
      "Training [5%] Loss: 0.0029305929592849684 time: 1703094714.0275562\n",
      "weight: [ 0.77272054  1.50905495  0.1198652   1.0955417   0.64657672  1.24550159\n",
      "  1.07133109  0.66474956  1.30260133  0.01548192 -0.30904381  0.34841907]\n",
      "epoch 26\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.004128800845862165 R2: 0.7959152261063385 time: 1703094716.6064126\n",
      "batch_idx: 1 loss: 0.0020874470277890775 R2: 0.8003407771255839 time: 1703094719.232227\n",
      "batch_idx: 2 loss: 0.0023815712141749207 R2: 0.8045910158947228 time: 1703094721.8854945\n",
      "batch_idx: 3 loss: 0.0021576603957594344 R2: 0.8087731072219816 time: 1703094724.5698524\n",
      "Training [5%] Loss: 0.0026888698708963992 time: 1703094724.5698524\n",
      "weight: [ 7.78066417e-01  1.50856107e+00  1.15796878e-01  1.07180016e+00\n",
      "  6.46797119e-01  1.24897793e+00  1.07826313e+00  6.70873499e-01\n",
      "  1.30145564e+00  5.90640332e-04 -3.10450359e-01  3.47317359e-01]\n",
      "epoch 27\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0038151840219959924 R2: 0.8130780883450812 time: 1703094727.2212505\n",
      "batch_idx: 1 loss: 0.0018925533792918665 R2: 0.8171685540609577 time: 1703094729.905494\n",
      "batch_idx: 2 loss: 0.0022123229095803087 R2: 0.8210507184492768 time: 1703094732.584052\n",
      "batch_idx: 3 loss: 0.0019373675731729715 R2: 0.8247988573227175 time: 1703094735.2707825\n",
      "Training [5%] Loss: 0.0024643569710102848 time: 1703094735.2707825\n",
      "weight: [ 0.78365447  1.50836551  0.11188689  1.06615878  0.64682387  1.24986997\n",
      "  1.08556182  0.67733512  1.30017735 -0.01109378 -0.3116762   0.34627489]\n",
      "epoch 28\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0035179988641678484 R2: 0.8285612850492345 time: 1703094737.911397\n",
      "batch_idx: 1 loss: 0.0017275970882368896 R2: 0.8320699209057792 time: 1703094740.5202336\n",
      "batch_idx: 2 loss: 0.0020671523227699853 R2: 0.8353370580208834 time: 1703094743.1110551\n",
      "batch_idx: 3 loss: 0.0017538900324495295 R2: 0.8384388329423522 time: 1703094745.8128827\n",
      "Training [6%] Loss: 0.002266659576906063 time: 1703094745.8128827\n",
      "weight: [ 0.78913076  1.50855673  0.1084087   1.07249486  0.64668073  1.24896917\n",
      "  1.09273219  0.68370052  1.29890665 -0.02111442 -0.31272362  0.34532823]\n",
      "epoch 29\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.003258211518969471 R2: 0.8414838920752619 time: 1703094748.5126624\n",
      "batch_idx: 1 loss: 0.0015970922707005124 R2: 0.8443137303998329 time: 1703094751.0784957\n",
      "batch_idx: 2 loss: 0.00195194135348597 R2: 0.8469302220134965 time: 1703094753.8163683\n",
      "batch_idx: 3 loss: 0.001607943473211864 R2: 0.8494211641932685 time: 1703094756.4706519\n",
      "Training [6%] Loss: 0.0021037971540919546 time: 1703094756.4706519\n",
      "weight: [ 0.7942177   1.50908929  0.10549081  1.08342581  0.6464338   1.2473175\n",
      "  1.09937997  0.68962012  1.29775119 -0.03080206 -0.3135902   0.34450387]\n",
      "epoch 30\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0030447424282411015 R2: 0.8518478671581511 time: 1703094759.2037888\n",
      "batch_idx: 1 loss: 0.0014917123004983687 R2: 0.8541436954789043 time: 1703094761.898848\n",
      "batch_idx: 2 loss: 0.0018625038413380133 R2: 0.8562766251258218 time: 1703094764.5850976\n",
      "batch_idx: 3 loss: 0.0014881430876821336 R2: 0.8583397287102835 time: 1703094767.2378025\n",
      "Training [6%] Loss: 0.001971775414439904 time: 1703094767.2378025\n",
      "weight: [ 0.79876447  1.5098567   0.10313717  1.09398628  0.64613094  1.24564856\n",
      "  1.10528414  0.6948948   1.29676949 -0.0409738  -0.31428074  0.3438103 ]\n",
      "epoch 31\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0028729931500538373 R2: 0.8603485951020012 time: 1703094769.8110223\n",
      "batch_idx: 1 loss: 0.0013978207580015178 R2: 0.8622905814984057 time: 1703094772.562642\n",
      "batch_idx: 2 loss: 0.001792187817417621 R2: 0.8640972829007388 time: 1703094775.223742\n",
      "batch_idx: 3 loss: 0.001383193108102892 R2: 0.8658618036076137 time: 1703094777.8608568\n",
      "Training [6%] Loss: 0.001861548708393967 time: 1703094777.8608568\n",
      "weight: [ 0.80275373  1.51075892  0.10127368  1.10195234  0.64579132  1.24429648\n",
      "  1.11041068  0.6994906   1.29597046 -0.05182494 -0.31481238  0.34323799]\n",
      "epoch 32\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0027350787964584835 R2: 0.8675738515365261 time: 1703094780.5280807\n",
      "batch_idx: 1 loss: 0.0013084055551811774 R2: 0.8692404455000879 time: 1703094783.212574\n",
      "batch_idx: 2 loss: 0.001738211771950401 R2: 0.8707759415585539 time: 1703094785.8491907\n",
      "batch_idx: 3 loss: 0.0012888163805831908 R2: 0.8722698954461505 time: 1703094788.6833358\n",
      "Training [6%] Loss: 0.0017676281260433132 time: 1703094788.6833358\n",
      "weight: [ 0.80626954  1.51173246  0.09979209  1.10682853  0.6454197   1.24334109\n",
      "  1.11487097  0.70350386  1.29532637 -0.06302698 -0.31521367  0.3427649 ]\n",
      "epoch 33\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002624912005201134 R2: 0.8737136188591925 time: 1703094791.3771458\n",
      "batch_idx: 1 loss: 0.0012244852501272728 R2: 0.8751076846725268 time: 1703094794.2184587\n",
      "batch_idx: 2 loss: 0.0017002824596002303 R2: 0.8763750651464995 time: 1703094797.007251\n",
      "batch_idx: 3 loss: 0.0012059494115538617 R2: 0.8775951978862363 time: 1703094799.6265304\n",
      "Training [7%] Loss: 0.0016889072816206247 time: 1703094799.6265304\n",
      "weight: [ 0.8094457   1.51274701  0.09858082  1.10886991  0.64501883  1.24275385\n",
      "  1.11885014  0.70709815  1.29479187 -0.07395787 -0.31551979  0.34236424]\n",
      "epoch 34\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0025369779095357685 R2: 0.8787813056588678 time: 1703094802.2810414\n",
      "batch_idx: 1 loss: 0.0011502158751987806 R2: 0.8799102472269789 time: 1703094804.8529172\n",
      "batch_idx: 2 loss: 0.0016765165847490993 R2: 0.8809312911041033 time: 1703094807.520938\n",
      "batch_idx: 3 loss: 0.001136178874667925 R2: 0.8819051489318778 time: 1703094810.2362933\n",
      "Training [7%] Loss: 0.0016249723110378933 time: 1703094810.2362933\n",
      "weight: [ 0.81241387  1.51378792  0.09754391  1.10859242  0.64459275  1.24246884\n",
      "  1.12253306  0.710438    1.29432236 -0.08397093 -0.31576577  0.34201157]\n",
      "epoch 35\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002464715465374137 R2: 0.882872118367701 time: 1703094812.9661627\n",
      "batch_idx: 1 loss: 0.0010884159506794144 R2: 0.8837796343617013 time: 1703094815.6014783\n",
      "batch_idx: 2 loss: 0.001662319580432366 R2: 0.8846062316443184 time: 1703094818.1772408\n",
      "batch_idx: 3 loss: 0.0010794585471348565 R2: 0.8853898204496236 time: 1703094820.9755123\n",
      "Training [7%] Loss: 0.0015737273859051935 time: 1703094820.9755123\n",
      "weight: [ 0.81526621  1.51484078  0.09661163  1.10661774  0.64414426  1.24240261\n",
      "  1.12604951  0.71363897  1.29388691 -0.09261971 -0.31597944  0.34168962]\n",
      "epoch 36\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0024016925870309525 R2: 0.886189444360667 time: 1703094823.589934\n",
      "batch_idx: 1 loss: 0.0010394343957647718 R2: 0.8869310000557549 time: 1703094826.3053381\n",
      "batch_idx: 2 loss: 0.001652735468338781 R2: 0.887613003172026 time: 1703094829.083253\n",
      "batch_idx: 3 loss: 0.0010344924485754183 R2: 0.8882559038341353 time: 1703094831.6963181\n",
      "Training [7%] Loss: 0.001532088724927481 time: 1703094831.6963181\n",
      "weight: [ 0.81803995  1.51588545  0.09574414  1.10359321  0.64367156  1.24246312\n",
      "  1.12945044  0.71674565  1.29347355 -0.09977759 -0.3161767   0.34138991]\n",
      "epoch 37\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0023440177931691106 R2: 0.8889228307679022 time: 1703094834.3246155\n",
      "batch_idx: 1 loss: 0.0010020135628843688 R2: 0.8895360401777219 time: 1703094836.850617\n",
      "batch_idx: 2 loss: 0.0016450354720206675 R2: 0.8900993036110572 time: 1703094839.550002\n",
      "batch_idx: 3 loss: 0.0009995924472396458 R2: 0.8906279776947489 time: 1703094842.2332237\n",
      "Training [7%] Loss: 0.0014976648188284483 time: 1703094842.2332237\n",
      "weight: [ 0.82072368  1.51689875  0.09492853  1.10007855  0.64316701  1.24256527\n",
      "  1.13271581  0.71973816  1.29308695 -0.10562728 -0.31636095  0.34111143]\n",
      "epoch 38\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0022911458663049653 R2: 0.8911753280303335 time: 1703094845.0157583\n",
      "batch_idx: 1 loss: 0.0009739965021486839 R2: 0.8916787264538751 time: 1703094847.9413164\n",
      "batch_idx: 2 loss: 0.0016390697891713597 R2: 0.8921336499923724 time: 1703094850.6806757\n",
      "batch_idx: 3 loss: 0.000972757651877382 R2: 0.8925622086918013 time: 1703094853.511852\n",
      "Training [8%] Loss: 0.0014692424523755976 time: 1703094853.511852\n",
      "weight: [ 0.82327836  1.51786089  0.0941703   1.09644151  0.6426189   1.24264741\n",
      "  1.13578461  0.72255908  1.29274117 -0.11054293 -0.31652688  0.34085738]\n",
      "epoch 39\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0022444835109870827 R2: 0.8929990379785917 time: 1703094856.2378054\n",
      "batch_idx: 1 loss: 0.0009525285117986165 R2: 0.893407227776976 time: 1703094859.1398485\n",
      "batch_idx: 2 loss: 0.0016358422226043887 R2: 0.893766282935494 time: 1703094862.106883\n",
      "batch_idx: 3 loss: 0.0009515718056242824 R2: 0.8941102084373208 time: 1703094864.8844724\n",
      "Training [8%] Loss: 0.0014461065127535926 time: 1703094864.8844724\n",
      "weight: [ 0.8256627   1.5187605   0.09348202  1.09283336  0.64201496  1.24267786\n",
      "  1.13859203  0.72514736  1.29245095 -0.11492579 -0.31666645  0.34063166]\n",
      "epoch 40\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002205415595892383 R2: 0.894453509610552 time: 1703094867.5488462\n",
      "batch_idx: 1 loss: 0.000934576085841768 R2: 0.8947841070227378 time: 1703094870.1625006\n",
      "batch_idx: 2 loss: 0.0016361084032432333 R2: 0.89506600336118 time: 1703094872.8026016\n",
      "batch_idx: 3 loss: 0.000933699680209012 R2: 0.8953425254603934 time: 1703094875.3751302\n",
      "Training [8%] Loss: 0.0014274499412965992 time: 1703094875.3751302\n",
      "weight: [ 0.82785288  1.51959598  0.09287317  1.08925138  0.64134596  1.24264904\n",
      "  1.14109904  0.72746547  1.29222508 -0.11906449 -0.31677447  0.34043604]\n",
      "epoch 41\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002174248817242249 R2: 0.8956135416269339 time: 1703094878.0478344\n",
      "batch_idx: 1 loss: 0.0009179221504409621 R2: 0.8958823867709921 time: 1703094880.600928\n",
      "batch_idx: 2 loss: 0.001639862949435943 R2: 0.8961051604556696 time: 1703094883.233886\n",
      "batch_idx: 3 loss: 0.0009176773486398052 R2: 0.8963272888028859 time: 1703094885.8909428\n",
      "Training [8%] Loss: 0.0014124278164397398 time: 1703094885.8909428\n",
      "weight: [ 0.82985075  1.52037334  0.09234439  1.08563563  0.64060807  1.24256573\n",
      "  1.14330518  0.7295115   1.29206361 -0.1230693  -0.3168516   0.3402692 ]\n",
      "epoch 42\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002150169904822425 R2: 0.8965434863449373 time: 1703094888.5123847\n",
      "batch_idx: 1 loss: 0.0009018457304582831 R2: 0.8967604896995391 time: 1703094891.2260532\n",
      "batch_idx: 2 loss: 0.0016463949196869397 R2: 0.8969369220969978 time: 1703094893.8146863\n",
      "batch_idx: 3 loss: 0.0009031916037921188 R2: 0.8971126005949511 time: 1703094896.4978788\n",
      "Training [8%] Loss: 0.0014004005396899418 time: 1703094896.4978788\n",
      "weight: [ 0.83167998  1.52110242  0.09188729  1.08194172  0.63980314  1.24243514\n",
      "  1.14524333  0.73131447  1.29195926 -0.12689126 -0.31690402  0.34012717]\n",
      "epoch 43\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002131617104800517 R2: 0.8972858582056583 time: 1703094899.1040182\n",
      "batch_idx: 1 loss: 0.0008868945742477993 R2: 0.8974575994907228 time: 1703094901.6792533\n",
      "batch_idx: 2 loss: 0.0016544967213394505 R2: 0.8975968644740198 time: 1703094904.3224773\n",
      "batch_idx: 3 loss: 0.0008906454644389735 R2: 0.8977329868844903 time: 1703094906.856754\n",
      "Training [9%] Loss: 0.0013909134662066852 time: 1703094906.856754\n",
      "weight: [ 0.83337447  1.52179343  0.09148859  1.07816215  0.63893682  1.24226243\n",
      "  1.14696254  0.73291867  1.29190128 -0.1304006  -0.31694071  0.34000486]\n",
      "epoch 44\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0021167452502461733 R2: 0.8978721596919226 time: 1703094909.5933945\n",
      "batch_idx: 1 loss: 0.0008740431648679634 R2: 0.8980059925994384 time: 1703094912.175228\n",
      "batch_idx: 2 loss: 0.0016628136514393124 R2: 0.8981160146756559 time: 1703094914.7961106\n",
      "batch_idx: 3 loss: 0.0008804909560864592 R2: 0.8982206769985336 time: 1703094917.370949\n",
      "Training [9%] Loss: 0.0013835232556599771 time: 1703094917.370949\n",
      "weight: [ 0.83496563  1.52245506  0.0911358   1.07430293  0.63801536  1.24205141\n",
      "  1.1485086   0.73436559  1.29187984 -0.13347881 -0.31696996  0.33989772]\n",
      "epoch 45\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0021039544328667137 R2: 0.8983328675483039 time: 1703094920.0228913\n",
      "batch_idx: 1 loss: 0.0008639225098117622 R2: 0.8984372880980803 time: 1703094922.6334295\n",
      "batch_idx: 2 loss: 0.0016703210528598196 R2: 0.8985245248972598 time: 1703094925.300827\n",
      "batch_idx: 3 loss: 0.000872809647395855 R2: 0.8986057155776292 time: 1703094927.828287\n",
      "Training [9%] Loss: 0.0013777519107335378 time: 1703094927.828287\n",
      "weight: [ 0.83647392  1.52309432  0.09082154  1.07034388  0.63704237  1.24180757\n",
      "  1.14991101  0.73568189  1.29188881 -0.13608185 -0.31699673  0.33980276]\n",
      "epoch 46\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0020923233324274407 R2: 0.8986958617497717 time: 1703094930.4319706\n",
      "batch_idx: 1 loss: 0.000856495319539202 R2: 0.8987784997025395 time: 1703094933.0633056\n",
      "batch_idx: 2 loss: 0.001676651182402697 R2: 0.8988473168988744 time: 1703094935.6526003\n",
      "batch_idx: 3 loss: 0.0008672370047262598 R2: 0.8989112780690315 time: 1703094938.2561588\n",
      "Training [9%] Loss: 0.0013731767097738999 time: 1703094938.2561588\n",
      "weight: [ 0.83790743  1.52371732  0.0905447   1.06621431  0.63601734  1.24154016\n",
      "  1.15118053  0.7368769   1.29192629 -0.13825299 -0.31702202  0.33971871]\n",
      "epoch 47\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0020816864430375755 R2: 0.8989826346529016 time: 1703094940.9920263\n",
      "batch_idx: 1 loss: 0.0008511412174912623 R2: 0.8990492015970661 time: 1703094943.6702363\n",
      "batch_idx: 2 loss: 0.0016820301211438221 R2: 0.8991027649876931 time: 1703094946.3525333\n",
      "batch_idx: 3 loss: 0.0008631128614806822 R2: 0.8991539569890736 time: 1703094948.8954155\n",
      "Training [9%] Loss: 0.0013694926607883357 time: 1703094948.8954155\n",
      "weight: [ 0.8392663   1.52433002  0.09030852  1.06180167  0.6349362   1.24126154\n",
      "  1.15231568  0.73794867  1.29199316 -0.14009101 -0.31704408  0.33964543]\n",
      "epoch 48\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.00207232840321382 R2: 0.8992096003161609 time: 1703094951.5444603\n",
      "batch_idx: 1 loss: 0.0008469808432458013 R2: 0.8992641659319812 time: 1703094954.110058\n",
      "batch_idx: 2 loss: 0.0016869192292969736 R2: 0.8993056695092176 time: 1703094956.6166296\n",
      "batch_idx: 3 loss: 0.000859741378093367 R2: 0.8993473264755052 time: 1703094959.3959203\n",
      "Training [10%] Loss: 0.0013664924634624904 time: 1703094959.3959203\n",
      "weight: [ 0.84054943  1.52493838  0.09011703  1.0569873   0.63379384  1.24098421\n",
      "  1.15331318  0.7388937   1.2920907  -0.14169947 -0.31706059  0.33958299]\n",
      "epoch 49\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002064553329192951 R2: 0.8993908742112433 time: 1703094961.9720447\n",
      "batch_idx: 1 loss: 0.0008432636770314157 R2: 0.8994362103153103 time: 1703094964.675023\n",
      "batch_idx: 2 loss: 0.0016916527895426713 R2: 0.8994689942219958 time: 1703094967.2892332\n",
      "batch_idx: 3 loss: 0.0008566433280639494 R2: 0.8995033295353162 time: 1703094969.9822285\n",
      "Training [10%] Loss: 0.0013640282809577466 time: 1703094969.9822285\n",
      "weight: [ 0.84175984  1.52554784  0.08997189  1.05168783  0.63258687  1.24071737\n",
      "  1.15417631  0.73971462  1.29221881 -0.14314609 -0.31707048  0.33953099]\n",
      "epoch 50\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0020584007563024106 R2: 0.8995384084045103 time: 1703094972.588824\n",
      "batch_idx: 1 loss: 0.0008396328663734228 R2: 0.8995762037541274 time: 1703094975.1486576\n",
      "batch_idx: 2 loss: 0.0016962686414440191 R2: 0.8996031683116661 time: 1703094977.8929198\n",
      "batch_idx: 3 loss: 0.0008536696456397333 R2: 0.8996316052118377 time: 1703094980.5548089\n",
      "Training [10%] Loss: 0.0013619929774398964 time: 1703094980.5548089\n",
      "weight: [ 0.84290639  1.52616276  0.08987111  1.04587949  0.63131527  1.24046496\n",
      "  1.15491765  0.74042288  1.29237538 -0.1444498  -0.3170746   0.33948834]\n",
      "epoch 51\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002053598930344322 R2: 0.8996611262335945 time: 1703094983.2674024\n",
      "batch_idx: 1 loss: 0.0008361404559671616 R2: 0.899692523825989 time: 1703094985.8044074\n",
      "batch_idx: 2 loss: 0.0017005486808830488 R2: 0.8997157967494422 time: 1703094988.4579685\n",
      "batch_idx: 3 loss: 0.0008509252130715079 R2: 0.8997394976934606 time: 1703094991.037035\n",
      "Training [10%] Loss: 0.0013603033200665102 time: 1703094991.037035\n",
      "weight: [ 0.84400187  1.52678609  0.08980983  1.03959432  0.62998216  1.24022607\n",
      "  1.15555624  0.74103603  1.29255687 -0.14559523 -0.31707517  0.33945344]\n",
      "epoch 52\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002049697992973809 R2: 0.8997654265583167 time: 1703094993.7459004\n",
      "batch_idx: 1 loss: 0.0008330483299486378 R2: 0.8997916519446625 time: 1703094996.4159403\n",
      "batch_idx: 2 loss: 0.0017041873851585249 R2: 0.8998126286685798 time: 1703094999.1186695\n",
      "batch_idx: 3 loss: 0.0008485840531334062 R2: 0.8998328719920956 time: 1703095001.8264241\n",
      "Training [10%] Loss: 0.0013588794403035945 time: 1703095001.8264241\n",
      "weight: [ 0.84505945  1.52741979  0.08978247  1.03289295  0.62859203  1.23999742\n",
      "  1.15611198  0.74157248  1.29275959 -0.14656021 -0.31707465  0.33942468]\n",
      "epoch 53\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002046271759450336 R2: 0.899856333588013 time: 1703095004.4893167\n",
      "batch_idx: 1 loss: 0.0008305778960360338 R2: 0.8998788589472453 time: 1703095007.3121254\n",
      "batch_idx: 2 loss: 0.0017069762075222895 R2: 0.8998983358019703 time: 1703095009.860867\n",
      "batch_idx: 3 loss: 0.0008467293979229516 R2: 0.8999164340652455 time: 1703095012.3518019\n",
      "Training [11%] Loss: 0.0013576388152329027 time: 1703095012.3518019\n",
      "weight: [ 0.84608968  1.52806551  0.08978472  1.02583146  0.62714868  1.23977649\n",
      "  1.15660077  0.74204698  1.29298068 -0.14733874 -0.31707465  0.3394008 ]\n",
      "epoch 54\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002043072398550437 R2: 0.8999379474558109 time: 1703095014.990765\n",
      "batch_idx: 1 loss: 0.0008287609949770081 R2: 0.8999582024190766 time: 1703095017.6503367\n",
      "batch_idx: 2 loss: 0.0017089009338113685 R2: 0.8999765740756038 time: 1703095020.3787916\n",
      "batch_idx: 3 loss: 0.0008452986942764382 R2: 0.8999935945995585 time: 1703095022.9698324\n",
      "Training [11%] Loss: 0.0013565082554038129 time: 1703095022.9698324\n",
      "weight: [ 0.84709949  1.52872517  0.08981442  1.01844269  0.62565415  1.23956341\n",
      "  1.1570329   0.74246905  1.29321855 -0.14794838 -0.31707558  0.33938101]\n",
      "epoch 55\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002040061364314212 R2: 0.9000134258826427 time: 1703095025.6033692\n",
      "batch_idx: 1 loss: 0.0008274432750914769 R2: 0.900032475569939 time: 1703095028.1728272\n",
      "batch_idx: 2 loss: 0.001710111067006868 R2: 0.9000500125451951 time: 1703095030.6853323\n",
      "batch_idx: 3 loss: 0.0008441315256170326 R2: 0.9000666197091161 time: 1703095033.4382193\n",
      "Training [11%] Loss: 0.0013554368080073973 time: 1703095033.4382193\n",
      "weight: [ 0.84809322  1.529401    0.08987106  1.01074208  0.62410907  1.23936064\n",
      "  1.15741459  0.74284446  1.29347247 -0.14842206 -0.31707701  0.33936489]\n",
      "epoch 56\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.00203732238566875 R2: 0.9000851099006159 time: 1703095036.0392733\n",
      "batch_idx: 1 loss: 0.0008263958190953151 R2: 0.9001035393371604 time: 1703095039.069858\n",
      "batch_idx: 2 loss: 0.0017108070201585185 R2: 0.9001205739104359 time: 1703095041.6788764\n",
      "batch_idx: 3 loss: 0.0008430662200277665 R2: 0.9001370415013655 time: 1703095044.3012605\n",
      "Training [11%] Loss: 0.0013543978612375876 time: 1703095044.3012605\n",
      "weight: [ 0.84907458  1.53009509  0.08995438  1.0027513   0.62251404  1.23917074\n",
      "  1.15775116  0.74317811  1.29374189 -0.14879313 -0.31707835  0.33935207]\n",
      "epoch 57\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002034933986135485 R2: 0.9001546935612327 time: 1703095046.813846\n",
      "batch_idx: 1 loss: 0.0008254452719467693 R2: 0.9001726861254928 time: 1703095049.3810244\n",
      "batch_idx: 2 loss: 0.0017111333309568744 R2: 0.9001895896147213 time: 1703095051.9808986\n",
      "batch_idx: 3 loss: 0.0008420179139248445 R2: 0.900205925956905 time: 1703095054.4985704\n",
      "Training [11%] Loss: 0.001353382625740993 time: 1703095054.4985704\n",
      "weight: [ 0.85004812  1.53080869  0.09006322  0.99452081  0.62087098  1.23899386\n",
      "  1.15804937  0.74347632  1.29402593 -0.1490841  -0.31707943  0.3393421 ]\n",
      "epoch 58\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0020328862826099227 R2: 0.9002232576338984 time: 1703095057.0217092\n",
      "batch_idx: 1 loss: 0.0008245415338879805 R2: 0.900240798662845 time: 1703095059.5757947\n",
      "batch_idx: 2 loss: 0.001711139635295782 R2: 0.9002578518549452 time: 1703095062.1733873\n",
      "batch_idx: 3 loss: 0.0008409942141173307 R2: 0.9002739867143896 time: 1703095064.7910743\n",
      "Training [12%] Loss: 0.001352390416477754 time: 1703095064.7910743\n",
      "weight: [ 0.85101951  1.53154197  0.09019516  0.98613475  0.61918353  1.23882671\n",
      "  1.15831789  0.74374721  1.29432323 -0.1493048  -0.31708059  0.33933436]\n",
      "epoch 59\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002031079015229011 R2: 0.9002913434428453 time: 1703095067.3558357\n",
      "batch_idx: 1 loss: 0.0008237350031181454 R2: 0.9003084434586757 time: 1703095069.943014\n",
      "batch_idx: 2 loss: 0.0017108134644933358 R2: 0.9003257501873827 time: 1703095072.507168\n",
      "batch_idx: 3 loss: 0.000840051617310529 R2: 0.9003416861450171 time: 1703095075.1305215\n",
      "Training [12%] Loss: 0.0013514197750377553 time: 1703095075.1305215\n",
      "weight: [ 0.85199467  1.53229421  0.09034712  0.97769502  0.61745632  1.23866367\n",
      "  1.15856594  0.74399941  1.29463229 -0.14945858 -0.31708241  0.33932818]\n",
      "epoch 60\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0020293809407015357 R2: 0.9003591622281688 time: 1703095077.682517\n",
      "batch_idx: 1 loss: 0.0008230981919573454 R2: 0.9003759639808825 time: 1703095080.2784388\n",
      "batch_idx: 2 loss: 0.001710144942364243 R2: 0.900393458984668 time: 1703095082.7647085\n",
      "batch_idx: 3 loss: 0.0008392339627664961 R2: 0.9004092969416174 time: 1703095085.292238\n",
      "Training [12%] Loss: 0.001350464509447405 time: 1703095085.292238\n",
      "weight: [ 0.85297864  1.53306453  0.09051637  0.96929628  0.61569368  1.23849903\n",
      "  1.15880136  0.74424027  1.29495186 -0.14955063 -0.31708532  0.339323  ]\n",
      "epoch 61\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.00202769869866955 R2: 0.900426793046805 time: 1703095087.9230738\n",
      "batch_idx: 1 loss: 0.0008226552438402328 R2: 0.9004435198778913 time: 1703095090.504629\n",
      "batch_idx: 2 loss: 0.0017091731532304106 R2: 0.900461051891603 time: 1703095093.0222354\n",
      "batch_idx: 3 loss: 0.0008385368344232255 R2: 0.9004769122417935 time: 1703095095.5384333\n",
      "Training [12%] Loss: 0.0013495159825408548 time: 1703095095.5384333\n",
      "weight: [ 0.85397495  1.53385239  0.09070124  0.96100846  0.61389871  1.23832897\n",
      "  1.15902955  0.74447488  1.2952812  -0.1495923  -0.31708932  0.33931847]\n",
      "epoch 62\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0020260079501803166 R2: 0.9004942794247972 time: 1703095098.150785\n",
      "batch_idx: 1 loss: 0.0008223630754760188 R2: 0.9005111146752258 time: 1703095100.7306721\n",
      "batch_idx: 2 loss: 0.001707986625486238 R2: 0.9005285538574318 time: 1703095103.2619417\n",
      "batch_idx: 3 loss: 0.0008379143059431696 R2: 0.9005444906471292 time: 1703095105.823085\n",
      "Training [12%] Loss: 0.0013485679892714358 time: 1703095105.823085\n",
      "weight: [ 0.85498574  1.53465776  0.09090114  0.95287565  0.61207321  1.23815177\n",
      "  1.15925371  0.74470623  1.29562005 -0.14959965 -0.31709409  0.33931438]\n",
      "epoch 63\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0020243339661780916 R2: 0.9005616629478022 time: 1703095108.4304588\n",
      "batch_idx: 1 loss: 0.0008221435948497363 R2: 0.900578672784093 time: 1703095110.93446\n",
      "batch_idx: 2 loss: 0.0017066875181338769 R2: 0.9005959743814616 time: 1703095113.4451358\n",
      "batch_idx: 3 loss: 0.0008373126367982568 R2: 0.9006119509089041 time: 1703095115.931303\n",
      "Training [13%] Loss: 0.0013476194289899904 time: 1703095115.931303\n",
      "weight: [ 0.85601252  1.53548085  0.09111597  0.94492694  0.6102183   1.23796667\n",
      "  1.15947588  0.74493627  1.29596834 -0.14958845 -0.31709922  0.33931063]\n",
      "epoch 64\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0020227065282320887 R2: 0.9006289800654974 time: 1703095118.40257\n",
      "batch_idx: 1 loss: 0.0008219325551842873 R2: 0.90064612418737 time: 1703095120.9092443\n",
      "batch_idx: 2 loss: 0.0017053523695544546 R2: 0.9006633157449058 time: 1703095123.486132\n",
      "batch_idx: 3 loss: 0.0008367011218851068 R2: 0.900679246387597 time: 1703095125.9781523\n",
      "Training [13%] Loss: 0.0013466731437139843 time: 1703095125.9781523\n",
      "weight: [ 0.85705668  1.53632158  0.09134552  0.93718763  0.60833517  1.23777246\n",
      "  1.15969807  0.74516687  1.296326   -0.14956994 -0.31710444  0.33930709]\n",
      "epoch 65\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0020211266798171356 R2: 0.9006962384301159 time: 1703095128.6162136\n",
      "batch_idx: 1 loss: 0.0008217072121016944 R2: 0.9007134430400482 time: 1703095131.2315826\n",
      "batch_idx: 2 loss: 0.0017040168270278562 R2: 0.9007305681471726 time: 1703095133.8473206\n",
      "batch_idx: 3 loss: 0.0008360792353655109 R2: 0.900746386314624 time: 1703095136.5693295\n",
      "Training [13%] Loss: 0.0013457324885780492 time: 1703095136.5693295\n",
      "weight: [ 0.85811977  1.53717952  0.09158929  0.92968214  0.60642528  1.23756699\n",
      "  1.15992246  0.74540013  1.29669287 -0.14954989 -0.31710969  0.33930363]\n",
      "epoch 66\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002019565535297382 R2: 0.9007634152401623 time: 1703095139.1202724\n",
      "batch_idx: 1 loss: 0.0008214787506527147 R2: 0.9007806458674759 time: 1703095141.7636635\n",
      "batch_idx: 2 loss: 0.0017026880429196866 R2: 0.9007977204174955 time: 1703095144.3862808\n",
      "batch_idx: 3 loss: 0.000835461190616921 R2: 0.9008134191678272 time: 1703095147.035779\n",
      "Training [13%] Loss: 0.0013447983798716761 time: 1703095147.035779\n",
      "weight: [ 0.85920323  1.53805406  0.09184675  0.92242893  0.60449007  1.23734795\n",
      "  1.16015111  0.74563798  1.29706882 -0.14953076 -0.317115    0.3393001 ]\n",
      "epoch 67\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0020179873152094676 R2: 0.9008304890176267 time: 1703095149.6230083\n",
      "batch_idx: 1 loss: 0.0008212644033607187 R2: 0.9008477710015308 time: 1703095152.298008\n",
      "batch_idx: 2 loss: 0.0017013680551781734 R2: 0.9008647796606144 time: 1703095154.8993392\n",
      "batch_idx: 3 loss: 0.0008348553990294317 R2: 0.9008803998981799 time: 1703095157.53667\n",
      "Training [13%] Loss: 0.0013438687931944479 time: 1703095157.53667\n",
      "weight: [ 0.86030805  1.53894483  0.09211778  0.9154345   0.60253041  1.23711402\n",
      "  1.16038531  0.74588164  1.29745384 -0.14951444 -0.3171204   0.33929643]\n",
      "epoch 68\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0020163729733559473 R2: 0.9008974705308506 time: 1703095160.1791422\n",
      "batch_idx: 1 loss: 0.0008210654171397457 R2: 0.9009148555355593 time: 1703095162.7322142\n",
      "batch_idx: 2 loss: 0.0017000682022429603 R2: 0.900931778482246 time: 1703095165.3482685\n",
      "batch_idx: 3 loss: 0.0008342555785952864 R2: 0.9009473664690258 time: 1703095167.9204485\n",
      "Training [14%] Loss: 0.001342940542833485 time: 1703095167.9204485\n",
      "weight: [ 0.86143468  1.539852    0.09240288  0.90869231  0.60054631  1.23686545\n",
      "  1.16062542  0.74613147  1.29784813 -0.1495036  -0.3171258   0.33929259]\n",
      "epoch 69\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0020147262147698947 R2: 0.9009644069512606 time: 1703095170.4167185\n",
      "batch_idx: 1 loss: 0.0008208657098471656 R2: 0.9009819267832027 time: 1703095172.887166\n",
      "batch_idx: 2 loss: 0.00169880685625083 R2: 0.9009987670006344 time: 1703095175.466915\n",
      "batch_idx: 3 loss: 0.0008336469296439241 R2: 0.9010143418981806 time: 1703095178.0628326\n",
      "Training [14%] Loss: 0.0013420114276279534 time: 1703095178.0628326\n",
      "weight: [ 0.86258319  1.54077614  0.09270305  0.90218744  0.59853717  1.23660368\n",
      "  1.16087113  0.74638721  1.29825203 -0.14950098 -0.31713106  0.33928861]\n",
      "epoch 70\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002013061777566261 R2: 0.9010313624036851 time: 1703095180.6166308\n",
      "batch_idx: 1 loss: 0.0008206462450763438 R2: 0.9010490099735003 time: 1703095183.1406171\n",
      "batch_idx: 2 loss: 0.0016975977562531882 R2: 0.9010657977167098 time: 1703095185.6288314\n",
      "batch_idx: 3 loss: 0.0008330182674799359 R2: 0.9010813508301835 time: 1703095188.2240345\n",
      "Training [14%] Loss: 0.0013410810115939322 time: 1703095188.2240345\n",
      "weight: [ 0.86375358  1.54171796  0.09301948  0.89590296  0.59650216  1.23633045\n",
      "  1.16112199  0.74664844  1.29866592 -0.14950805 -0.31713609  0.33928455]\n",
      "epoch 71\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002011389700641019 R2: 0.901098392479142 time: 1703095190.7360032\n",
      "batch_idx: 1 loss: 0.0008203992566245008 R2: 0.9011161375942527 time: 1703095193.2965755\n",
      "batch_idx: 2 loss: 0.0016964416335916994 R2: 0.9011329130009594 time: 1703095195.8648927\n",
      "batch_idx: 3 loss: 0.0008323687717607936 R2: 0.9011484299764744 time: 1703095198.4504237\n",
      "Training [14%] Loss: 0.001340149840654503 time: 1703095198.4504237\n",
      "weight: [ 0.8649459   1.5426781   0.09335325  0.8898237   0.59444056  1.23604722\n",
      "  1.16137767  0.74691487  1.29909014 -0.14952438 -0.31714087  0.33928041]\n",
      "epoch 72\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002009708818795285 R2: 0.9011655317713123 time: 1703095201.0831552\n",
      "batch_idx: 1 loss: 0.0008201308011193762 R2: 0.9011833502897051 time: 1703095203.6378167\n",
      "batch_idx: 2 loss: 0.0016953277110542783 R2: 0.9012001432490846 time: 1703095206.2630165\n",
      "batch_idx: 3 loss: 0.0008317053119581728 R2: 0.9012156247438492 time: 1703095208.862424\n",
      "Training [14%] Loss: 0.0013392181607317782 time: 1703095208.862424\n",
      "weight: [ 0.86616034  1.54365709  0.09370539  0.88393623  0.59235176  1.23575514\n",
      "  1.16163797  0.74718633  1.299525   -0.14954819 -0.31714545  0.33927622]\n",
      "epoch 73\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.00200801179294203 R2: 0.9012328007775492 time: 1703095211.361946\n",
      "batch_idx: 1 loss: 0.0008198525522720313 R2: 0.901250690470653 time: 1703095213.9944556\n",
      "batch_idx: 2 loss: 0.0016942419005354363 R2: 0.9012675140673363 time: 1703095216.5365238\n",
      "batch_idx: 3 loss: 0.0008310349806314942 R2: 0.9012829779305763 time: 1703095219.218686\n",
      "Training [15%] Loss: 0.001338285306595248 time: 1703095219.218686\n",
      "weight: [ 0.86739701  1.54465555  0.09407701  0.87822714  0.59023511  1.23545545\n",
      "  1.16190265  0.74746261  1.29997084 -0.14957743 -0.31714989  0.33927199]\n",
      "epoch 74\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002006294078167449 R2: 0.9013002207545437 time: 1703095221.8096077\n",
      "batch_idx: 1 loss: 0.0008195722221069124 R2: 0.9013181949228002 time: 1703095224.456343\n",
      "batch_idx: 2 loss: 0.0016931739237211053 R2: 0.9013350534061763 time: 1703095226.972284\n",
      "batch_idx: 3 loss: 0.0008303600920950902 R2: 0.9013505213845383 time: 1703095229.6377897\n",
      "Training [15%] Loss: 0.001337350079022639 time: 1703095229.6377897\n",
      "weight: [ 0.86865597  1.5456743   0.09446944  0.8726821   0.58808974  1.23514972\n",
      "  1.16217132  0.74774336  1.30042802 -0.14961057 -0.31715422  0.33926774]\n",
      "epoch 75\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002004558144979313 R2: 0.901367821696675 time: 1703095232.243356\n",
      "batch_idx: 1 loss: 0.0008192905395312183 R2: 0.9013858924924796 time: 1703095235.0247962\n",
      "batch_idx: 2 loss: 0.001692118525093752 R2: 0.901402792850069 time: 1703095237.8578002\n",
      "batch_idx: 3 loss: 0.0008296788594162515 R2: 0.901418277273281 time: 1703095240.6279929\n",
      "Training [15%] Loss: 0.0013364115172551335 time: 1703095240.6279929\n",
      "weight: [ 0.86993723  1.54671434  0.0948842   0.8672867   0.58591465  1.23483972\n",
      "  1.16244348  0.74802814  1.30089697 -0.14964658 -0.31715844  0.33926352]\n",
      "epoch 76\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.002002810641483448 R2: 0.9014356391300081 time: 1703095243.2767258\n",
      "batch_idx: 1 loss: 0.0008190051090378517 R2: 0.9014538077919004 time: 1703095245.871577\n",
      "batch_idx: 2 loss: 0.0016910720657805863 R2: 0.901470764121443 time: 1703095248.3577938\n",
      "batch_idx: 3 loss: 0.0008289893648970233 R2: 0.9014862658593911 time: 1703095250.9512916\n",
      "Training [15%] Loss: 0.0013354692952997273 time: 1703095250.9512916\n",
      "weight: [ 0.87124085  1.54777666  0.09532281  0.86202807  0.58370889  1.23452708\n",
      "  1.1627187   0.74831654  1.3013781  -0.14968456 -0.31716256  0.33925935]\n",
      "epoch 77\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0020010567851839003 R2: 0.9015037058940705 time: 1703095253.4747868\n",
      "batch_idx: 1 loss: 0.0008187156845764579 R2: 0.9015219662889958 time: 1703095256.0735013\n",
      "batch_idx: 2 loss: 0.001690029146562157 R2: 0.901538995337788 time: 1703095258.659143\n",
      "batch_idx: 3 loss: 0.000828292386826165 R2: 0.9015545116190161 time: 1703095261.2452772\n",
      "Training [15%] Loss: 0.00133452350078717 time: 1703095261.2452772\n",
      "weight: [ 0.87256695  1.54886214  0.09578673  0.85689582  0.58147175  1.23421303\n",
      "  1.16299672  0.74860836  1.30187178 -0.14972338 -0.31716661  0.33925525]\n",
      "epoch 78\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019992974526044376 R2: 0.9015720475685498 time: 1703095263.738778\n",
      "batch_idx: 1 loss: 0.0008184258130207137 R2: 0.9015903961930716 time: 1703095266.2548187\n",
      "batch_idx: 2 loss: 0.0016889824056758897 R2: 0.9016075105942262 time: 1703095268.7936833\n",
      "batch_idx: 3 loss: 0.000827590925200555 R2: 0.9016230431810671 time: 1703095271.370956\n",
      "Training [16%] Loss: 0.001333574149125399 time: 1703095271.370956\n",
      "weight: [ 0.87391581  1.54997148  0.09627728  0.8518818   0.57920273  1.23389838\n",
      "  1.16327746  0.74890353  1.3023784  -0.14976184 -0.31717062  0.33925124]\n",
      "epoch 79\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001997530610910522 R2: 0.9016406848094943 time: 1703095274.0364878\n",
      "batch_idx: 1 loss: 0.0008181404129363476 R2: 0.9016591266268262 time: 1703095276.621812\n",
      "batch_idx: 2 loss: 0.0016879248817363245 R2: 0.9016763325426117 time: 1703095279.3246703\n",
      "batch_idx: 3 loss: 0.000826887767860645 R2: 0.9016918890295058 time: 1703095281.9310143\n",
      "Training [16%] Loss: 0.0013326209183609598 time: 1703095281.9310143\n",
      "weight: [ 0.87528772  1.55110532  0.09679576  0.84697922  0.57690151  1.23358373\n",
      "  1.16356095  0.74920211  1.30289828 -0.14979891 -0.31717466  0.3392473 ]\n",
      "epoch 80\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019957545435809196 R2: 0.9017096387458908 time: 1703095284.5051785\n",
      "batch_idx: 1 loss: 0.0008178625518658131 R2: 0.90172818476958 time: 1703095287.1849258\n",
      "batch_idx: 2 loss: 0.001686852186628571 R2: 0.9017454848775935 time: 1703095289.8055134\n",
      "batch_idx: 3 loss: 0.0008261837873359872 R2: 0.9017610739550213 time: 1703095292.5053477\n",
      "Training [16%] Loss: 0.001331663267352823 time: 1703095292.5053477\n",
      "weight: [ 0.876683    1.55226425  0.09734351  0.84218196  0.57456785  1.23326958\n",
      "  1.16384722  0.74950414  1.30343181 -0.149834   -0.31717874  0.33924344]\n",
      "epoch 81\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001993969510393487 R2: 0.9017789337847626 time: 1703095295.1890728\n",
      "batch_idx: 1 loss: 0.0008175924823640099 R2: 0.901797594766402 time: 1703095297.8408926\n",
      "batch_idx: 2 loss: 0.0016857626279254416 R2: 0.9018149925318317 time: 1703095300.4775462\n",
      "batch_idx: 3 loss: 0.0008254782544391701 R2: 0.9018306192371297 time: 1703095303.094181\n",
      "Training [16%] Loss: 0.001330700718780527 time: 1703095303.094181\n",
      "weight: [ 0.878102    1.55344886  0.09792187  0.83748456  0.57220157  1.23295636\n",
      "  1.16413635  0.74980972  1.30397936 -0.14986685 -0.31718289  0.33923965]\n",
      "epoch 82\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019921768543474535 R2: 0.9018485960306952 time: 1703095305.671617\n",
      "batch_idx: 1 loss: 0.0008173291340221869 R2: 0.9018673788603737 time: 1703095308.2283788\n",
      "batch_idx: 2 loss: 0.0016846557295364223 R2: 0.9018848800267991 time: 1703095310.9191844\n",
      "batch_idx: 3 loss: 0.0008247702977350208 R2: 0.9019005453264176 time: 1703095313.5386083\n",
      "Training [16%] Loss: 0.0013297330039102708 time: 1703095313.5386083\n",
      "weight: [ 0.87954509  1.5546596   0.09853217  0.83288249  0.56980268  1.23264432\n",
      "  1.16442848  0.75011902  1.30454129 -0.14989736 -0.31718711  0.33923593]\n",
      "epoch 83\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019903771528886785 R2: 0.9019186499274097 time: 1703095316.2728457\n",
      "batch_idx: 1 loss: 0.0008170720193785872 R2: 0.9019375590225126 time: 1703095318.822284\n",
      "batch_idx: 2 loss: 0.00168353086749116 R2: 0.9019551699250407 time: 1703095321.4191093\n",
      "batch_idx: 3 loss: 0.0008240598697892168 R2: 0.9019708737929758 time: 1703095324.0632367\n",
      "Training [17%] Loss: 0.0013287599773869107 time: 1703095324.0632367\n",
      "weight: [ 0.88101269  1.5558968   0.09917566  0.82837219  0.56737137  1.23233351\n",
      "  1.16472386  0.75043229  1.30511796 -0.14992545 -0.31719144  0.33923226]\n",
      "epoch 84\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001988569422108877 R2: 0.9019891166168893 time: 1703095326.6166666\n",
      "batch_idx: 1 loss: 0.000816821744432078 R2: 0.9020081573852453 time: 1703095329.2583885\n",
      "batch_idx: 2 loss: 0.0016823871044000724 R2: 0.9020258827100726 time: 1703095331.9281619\n",
      "batch_idx: 3 loss: 0.0008233475159554759 R2: 0.902041626866315 time: 1703095334.5786896\n",
      "Training [17%] Loss: 0.0013277814467241257 time: 1703095334.5786896\n",
      "weight: [ 0.88250527  1.5571606   0.0998535   0.82395088  0.56490804  1.23202381\n",
      "  1.16502278  0.75074983  1.30570975 -0.14995106 -0.31719589  0.33922862]\n",
      "epoch 85\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001986751839064967 R2: 0.9020600149879184 time: 1703095337.1511915\n",
      "batch_idx: 1 loss: 0.0008165791285239456 R2: 0.902079195391489 time: 1703095339.6613605\n",
      "batch_idx: 2 loss: 0.0016812238772850514 R2: 0.9020970377427974 time: 1703095342.3142922\n",
      "batch_idx: 3 loss: 0.0008226335539440426 R2: 0.9021128255980958 time: 1703095344.838547\n",
      "Training [17%] Loss: 0.0013267970997045016 time: 1703095344.838547\n",
      "weight: [ 0.8840233   1.55845106  0.10056687  0.81961612  0.56241323  1.23171505\n",
      "  1.1653256   0.75107199  1.30631701 -0.14997421 -0.31720049  0.33922498]\n",
      "epoch 86\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019849229056227743 R2: 0.9021313636121988 time: 1703095347.4434752\n",
      "batch_idx: 1 loss: 0.00081634422691554 R2: 0.9021506928027488 time: 1703095350.0291042\n",
      "batch_idx: 2 loss: 0.0016800415075339682 R2: 0.9021686540369119 time: 1703095352.6824522\n",
      "batch_idx: 3 loss: 0.0008219176423144304 R2: 0.9021844887129203 time: 1703095355.2764103\n",
      "Training [17%] Loss: 0.001325806570596678 time: 1703095355.2764103\n",
      "weight: [ 0.88556724  1.55976814  0.10131692  0.81536563  0.55988757  1.23140706\n",
      "  1.16563264  0.75139913  1.30694013 -0.14999505 -0.31720526  0.33922134]\n",
      "epoch 87\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019830818719569652 R2: 0.9022031814397312 time: 1703095357.8333938\n",
      "batch_idx: 1 loss: 0.0008161162578298076 R2: 0.9022226675391793 time: 1703095360.3772976\n",
      "batch_idx: 2 loss: 0.0016788409870489296 R2: 0.90224075013792 time: 1703095362.933818\n",
      "batch_idx: 3 loss: 0.0008211990588931753 R2: 0.9022566330228242 time: 1703095365.459403\n",
      "Training [17%] Loss: 0.0013248095439322193 time: 1703095365.459403\n",
      "weight: [ 0.88713758  1.56111175  0.10210482  0.81119728  0.55733179  1.23109965\n",
      "  1.16594424  0.75173158  1.30757951 -0.15001379 -0.31721021  0.33921766]\n",
      "epoch 88\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001981228275418474 R2: 0.9022754869145707 time: 1703095368.0956128\n",
      "batch_idx: 1 loss: 0.0008158942779465681 R2: 0.9022951362673552 time: 1703095370.716597\n",
      "batch_idx: 2 loss: 0.0016776233616333802 R2: 0.902313343458581 time: 1703095373.3445132\n",
      "batch_idx: 3 loss: 0.0008204772347478442 R2: 0.9023292745435467 time: 1703095375.8629274\n",
      "Training [18%] Loss: 0.0013238057874365666 time: 1703095375.8629274\n",
      "weight: [ 0.88873478  1.56248164  0.10293171  0.80710913  0.55474677  1.23079266\n",
      "  1.16626076  0.75206971  1.30823555 -0.15003066 -0.31721535  0.33921392]\n",
      "epoch 89\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019793613318571735 R2: 0.9023482967928086 time: 1703095378.4839854\n",
      "batch_idx: 1 loss: 0.0008156777706397814 R2: 0.9023681149510218 time: 1703095381.0008006\n",
      "batch_idx: 2 loss: 0.0016763893442339147 R2: 0.9023864498529834 time: 1703095383.6543798\n",
      "batch_idx: 3 loss: 0.0008197519716613685 R2: 0.9024024289997501 time: 1703095386.1585872\n",
      "Training [18%] Loss: 0.0013227951045980595 time: 1703095386.1585872\n",
      "weight: [ 0.89035932  1.56387744  0.10379869  0.80309943  0.55213352  1.23048588\n",
      "  1.16658257  0.7524139   1.30890866 -0.1500458  -0.3172207   0.3392101 ]\n",
      "epoch 90\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019774798377664043 R2: 0.902421625883742 time: 1703095389.056928\n",
      "batch_idx: 1 loss: 0.0008154666323155327 R2: 0.9024416188368664 time: 1703095391.8642201\n",
      "batch_idx: 2 loss: 0.0016751394004450686 R2: 0.9024600837622977 time: 1703095394.5258229\n",
      "batch_idx: 3 loss: 0.000819023242468161 R2: 0.9024761113656071 time: 1703095397.1886382\n",
      "Training [18%] Loss: 0.0013217772782487917 time: 1703095397.1886382\n",
      "weight: [ 0.89201168  1.56529866  0.10470682  0.79916655  0.5494932   1.23017914\n",
      "  1.16691003  0.75276452  1.30959928 -0.15005937 -0.31722628  0.33920617]\n",
      "epoch 91\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019755825435791253 R2: 0.9024954876896688 time: 1703095399.818354\n",
      "batch_idx: 1 loss: 0.0008152607744378376 R2: 0.9025156620503475 time: 1703095402.4022238\n",
      "batch_idx: 2 loss: 0.0016738740333679108 R2: 0.9025342586056778 time: 1703095405.023033\n",
      "batch_idx: 3 loss: 0.0008182909091603737 R2: 0.9025503351459131 time: 1703095407.6593356\n",
      "Training [18%] Loss: 0.0013207520651363118 time: 1703095407.6593356\n",
      "weight: [ 0.89369232  1.56674469  0.10565714  0.79530885  0.54682706  1.22987231\n",
      "  1.1672435   0.75312196  1.31030786 -0.15007151 -0.31723211  0.33920212]\n",
      "epoch 92\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001973668497724569 R2: 0.9025698950299276 time: 1703095410.2174215\n",
      "batch_idx: 1 loss: 0.0008150598677217427 R2: 0.9025902573109773 time: 1703095412.9016292\n",
      "batch_idx: 2 loss: 0.0016725938965613599 R2: 0.9026089869349935 time: 1703095415.4927454\n",
      "batch_idx: 3 loss: 0.0008175546724255433 R2: 0.9026251121787349 time: 1703095418.0299695\n",
      "Training [18%] Loss: 0.0013197192336083037 time: 1703095418.0299695\n",
      "weight: [ 0.89540171  1.56821484  0.10665067  0.7915247   0.54413645  1.22956532\n",
      "  1.16758332  0.75348656  1.31103485 -0.15008235 -0.31723821  0.33919791]\n",
      "epoch 93\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.00197173703071873 R2: 0.9026448600044275 time: 1703095420.6524513\n",
      "batch_idx: 1 loss: 0.0008148634508204907 R2: 0.9026654160083167 time: 1703095423.2259812\n",
      "batch_idx: 2 loss: 0.0016712996621291855 R2: 0.9026842802465167 time: 1703095425.8464072\n",
      "batch_idx: 3 loss: 0.0008168142367825666 R2: 0.9027004530018935 time: 1703095428.4827409\n",
      "Training [19%] Loss: 0.0013186785951127432 time: 1703095428.4827409\n",
      "weight: [ 0.89714028  1.56970827  0.10768841  0.78781244  0.54142285  1.22925815\n",
      "  1.16792983  0.75385867  1.31178075 -0.15009205 -0.31724459  0.33919353]\n",
      "epoch 94\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019697875053559916 R2: 0.9027203935045757 time: 1703095430.9504206\n",
      "batch_idx: 1 loss: 0.0008146711851945417 R2: 0.9027411484400704 time: 1703095433.5197089\n",
      "batch_idx: 2 loss: 0.0016699918530262172 R2: 0.9027601487244115 time: 1703095436.1069381\n",
      "batch_idx: 3 loss: 0.0008160694571723142 R2: 0.9027763672333243 time: 1703095438.696136\n",
      "Training [19%] Loss: 0.0013176300001872662 time: 1703095438.696136\n",
      "weight: [ 0.89890848  1.57122404  0.10877129  0.78417049  0.53868783  1.22895083\n",
      "  1.16828334  0.75423863  1.31254606 -0.15010071 -0.31725127  0.33918896]\n",
      "epoch 95\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001967819159753422 R2: 0.9027965048911911 time: 1703095441.3282835\n",
      "batch_idx: 1 loss: 0.000814482947453889 R2: 0.9028174639012152 time: 1703095443.8770583\n",
      "batch_idx: 2 loss: 0.001668670831376564 R2: 0.9028366011873745 time: 1703095446.5135612\n",
      "batch_idx: 3 loss: 0.0008153203184782384 R2: 0.9028528635501946 time: 1703095449.1332057\n",
      "Training [19%] Loss: 0.0013165733142655282 time: 1703095449.1332057\n",
      "weight: [ 0.90070674  1.57276105  0.10990019  0.78059726  0.53593313  1.22864343\n",
      "  1.16864417  0.75462678  1.31333129 -0.15010842 -0.31725827  0.33918416]\n",
      "epoch 96\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001965831177875775 R2: 0.9028732021181092 time: 1703095451.7112095\n",
      "batch_idx: 1 loss: 0.0008142987065475324 R2: 0.9028943705580433 time: 1703095454.3434489\n",
      "batch_idx: 2 loss: 0.001667336920366376 R2: 0.9029136452252603 time: 1703095456.9626646\n",
      "batch_idx: 3 loss: 0.0008145668159734984 R2: 0.9029299493911086 time: 1703095459.5679908\n",
      "Training [19%] Loss: 0.0013155084051907954 time: 1703095459.5679908\n",
      "weight: [ 0.90253547  1.5743181   0.11107591  0.77709119  0.53316054  1.22833608\n",
      "  1.16901261  0.75502344  1.31413698 -0.15011526 -0.31726561  0.33917913]\n",
      "epoch 97\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001963822844190795 R2: 0.902950492037988 time: 1703095462.2017844\n",
      "batch_idx: 1 loss: 0.0008141183729142798 R2: 0.9029718752904772 time: 1703095464.7449632\n",
      "batch_idx: 2 loss: 0.0016659905055742253 R2: 0.9029912873198022 time: 1703095467.3499506\n",
      "batch_idx: 3 loss: 0.0008138088887247579 R2: 0.9030076307756458 time: 1703095470.0334253\n",
      "Training [19%] Loss: 0.0013144351528510145 time: 1703095470.0334253\n",
      "weight: [ 0.90439509  1.57589385  0.1122992   0.77365068  0.530372    1.22802898\n",
      "  1.16938895  0.75542891  1.31496371 -0.15012132 -0.31727329  0.33917384]\n",
      "epoch 98\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001961793591317871 R2: 0.903028380510135 time: 1703095472.620425\n",
      "batch_idx: 1 loss: 0.0008139417814105113 R2: 0.903049983673496 time: 1703095475.2738123\n",
      "batch_idx: 2 loss: 0.001664632028929035 R2: 0.9030695328240436 time: 1703095477.799266\n",
      "batch_idx: 3 loss: 0.0008130464567895282 R2: 0.9030859124115567 time: 1703095480.4083772\n",
      "Training [20%] Loss: 0.0013133534646117364 time: 1703095480.4083772\n",
      "weight: [ 0.90628598  1.5774869   0.11357075  0.77027413  0.52756951  1.22772239\n",
      "  1.16977345  0.75584348  1.31581204 -0.15012669 -0.31728134  0.33916827]\n",
      "epoch 99\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019597429202164686 R2: 0.9031068722554905 time: 1703095482.9445791\n",
      "batch_idx: 1 loss: 0.0008137687791403147 R2: 0.9031287000766414 time: 1703095485.5494537\n",
      "batch_idx: 2 loss: 0.0016632619276797864 R2: 0.9031483858708865 time: 1703095488.201677\n",
      "batch_idx: 3 loss: 0.0008122794862376991 R2: 0.9031647979069438 time: 1703095490.77322\n",
      "Training [20%] Loss: 0.0013122632783185673 time: 1703095490.77322\n",
      "weight: [ 0.90820854  1.57909569  0.11489112  0.76695991  0.52475517  1.22741661\n",
      "  1.17016636  0.75626744  1.31668261 -0.15013145 -0.31728978  0.3391624 ]\n",
      "epoch 100\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019576703179552558 R2: 0.9031859707041626 time: 1703095493.4272869\n",
      "batch_idx: 1 loss: 0.0008135992845329017 R2: 0.9032080277457674 time: 1703095496.030524\n",
      "batch_idx: 2 loss: 0.0016618806117021448 R2: 0.9032278493483817 time: 1703095498.7001998\n",
      "batch_idx: 3 loss: 0.0008115079984252236 R2: 0.9032442898439909 time: 1703095501.2687058\n",
      "Training [20%] Loss: 0.0013111645531538815 time: 1703095501.2687058\n",
      "weight: [ 0.91016313  1.5807186   0.11626082  0.76370642  0.52193118  1.227112\n",
      "  1.1705679   0.75670104  1.31757603 -0.15013569 -0.3172986   0.33915622]\n",
      "epoch 101\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001955575265896646 R2: 0.9032656780316207 time: 1703095503.906393\n",
      "batch_idx: 1 loss: 0.000813433256106051 R2: 0.9032879687989922 time: 1703095506.5227292\n",
      "batch_idx: 2 loss: 0.0016604884983496757 R2: 0.9033079249765684 time: 1703095509.1128068\n",
      "batch_idx: 3 loss: 0.0008107320270861612 R2: 0.9033243896994791 time: 1703095511.6984808\n",
      "Training [20%] Loss: 0.0013100572618596335 time: 1703095511.6984808\n",
      "weight: [ 0.91215012  1.58235392  0.11768022  0.76051204  0.51909978  1.22680899\n",
      "  1.1709783   0.75714454  1.31849298 -0.15013948 -0.31730783  0.33914972]\n",
      "epoch 102\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001953457304517115 R2: 0.903345995323931 time: 1703095514.272514\n",
      "batch_idx: 1 loss: 0.0008132706298956407 R2: 0.9033685241950595 time: 1703095516.7858982\n",
      "batch_idx: 2 loss: 0.0016590860497259561 R2: 0.9033886134059751 time: 1703095519.4085484\n",
      "batch_idx: 3 loss: 0.0008099515854401347 R2: 0.903405097785267 time: 1703095521.9297311\n",
      "Training [20%] Loss: 0.0013089413923947117 time: 1703095521.9297311\n",
      "weight: [ 0.91416985  1.5839999   0.11914962  0.7573751   0.51626328  1.22650806\n",
      "  1.17139772  0.75759815  1.31943413 -0.15014292 -0.31731748  0.33914287]\n",
      "epoch 103\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019513160672792924 R2: 0.9034269226906796 time: 1703095524.507087\n",
      "batch_idx: 1 loss: 0.0008131113047590143 R2: 0.9034496937579222 time: 1703095527.1350803\n",
      "batch_idx: 2 loss: 0.0016576737667125681 R2: 0.9034699142642273 time: 1703095529.733234\n",
      "batch_idx: 3 loss: 0.000809166677059114 R2: 0.9034864133231932 time: 1703095532.2766404\n",
      "Training [21%] Loss: 0.0013078169539524971 time: 1703095532.2766404\n",
      "weight: [ 0.91622266  1.58565473  0.12066919  0.75429395  0.51342401  1.22620974\n",
      "  1.17182635  0.75806208  1.32040021 -0.15014608 -0.31732755  0.33913567]\n",
      "epoch 104\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019491512565668722 R2: 0.9035084592620753 time: 1703095534.910278\n",
      "batch_idx: 1 loss: 0.0008129551787219545 R2: 0.903531476263459 time: 1703095537.4550412\n",
      "batch_idx: 2 loss: 0.0016562521536157078 R2: 0.9035518261663353 time: 1703095539.9646864\n",
      "batch_idx: 3 loss: 0.0008083773248953065 R2: 0.9035683345901996 time: 1703095542.5563784\n",
      "Training [21%] Loss: 0.0013066839784499605 time: 1703095542.5563784\n",
      "weight: [ 0.91830885  1.58731663  0.12223899  0.75126688  0.51058433  1.2259146\n",
      "  1.17226432  0.75853652  1.32139197 -0.15014907 -0.31733805  0.3391281 ]\n",
      "epoch 105\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019469626093774155 R2: 0.9035906031672457 time: 1703095545.1116157\n",
      "batch_idx: 1 loss: 0.0008128021808162751 R2: 0.9036138695298144 time: 1703095547.7322757\n",
      "batch_idx: 2 loss: 0.001654821694601879 R2: 0.9036343467508917 time: 1703095550.3696535\n",
      "batch_idx: 3 loss: 0.000807583579792431 R2: 0.9036508590102686 time: 1703095552.9098258\n",
      "Training [21%] Loss: 0.0013055425161470002 time: 1703095552.9098258\n",
      "weight: [ 0.92042875  1.58898377  0.12385897  0.7482922   0.50774658  1.22562326\n",
      "  1.17271174  0.75902162  1.32241018 -0.15015195 -0.31734898  0.33912015]\n",
      "epoch 106\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019447498983715713 R2: 0.9036733515943304 time: 1703095555.5022812\n",
      "batch_idx: 1 loss: 0.0008126522652625869 R2: 0.9036968704739221 time: 1703095558.09114\n",
      "batch_idx: 2 loss: 0.0016533828553417528 R2: 0.9037174727639309 time: 1703095560.7959144\n",
      "batch_idx: 3 loss: 0.0008067855057709202 R2: 0.9037339831720601 time: 1703095563.3620923\n",
      "Training [21%] Loss: 0.0013043926311867079 time: 1703095563.3620923\n",
      "weight: [ 0.92258265  1.59065439  0.12552895  0.74536819  0.50491312  1.22533639\n",
      "  1.17316871  0.75951754  1.32345566 -0.15015481 -0.31736036  0.33911181]\n",
      "epoch 107\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019425129579217204 R2: 0.9037567009135751 time: 1703095566.0057452\n",
      "batch_idx: 1 loss: 0.0008125053899440005 R2: 0.9037804751549456 time: 1703095568.5406601\n",
      "batch_idx: 2 loss: 0.0016519360885790726 R2: 0.9038012001543955 time: 1703095571.1464279\n",
      "batch_idx: 3 loss: 0.0008059831682640777 R2: 0.903817702852294 time: 1703095573.7985063\n",
      "Training [21%] Loss: 0.0013032344011772179 time: 1703095573.7985063\n",
      "weight: [ 0.92477081  1.59232679  0.12724861  0.74249312  0.50208625  1.22505468\n",
      "  1.17363529  0.76002439  1.32452927 -0.15015771 -0.31737218  0.33910309]\n",
      "epoch 108\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001940251696403119 R2: 0.9038406467769831 time: 1703095576.414752\n",
      "batch_idx: 1 loss: 0.0008123615140039651 R2: 0.9038646788406431 time: 1703095579.1183395\n",
      "batch_idx: 2 loss: 0.0016504818241643872 R2: 0.9038855241444581 time: 1703095581.8011346\n",
      "batch_idx: 3 loss: 0.0008051766403485195 R2: 0.9039020131003512 time: 1703095584.3913734\n",
      "Training [22%] Loss: 0.0013020679187299977 time: 1703095584.3913734\n",
      "weight: [ 0.92699351  1.59399931  0.12901754  0.73966527  0.49926823  1.22477885\n",
      "  1.17411153  0.76054227  1.32563188 -0.15016072 -0.31738445  0.33909396]\n",
      "epoch 109\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019379660824327682 R2: 0.9039251841619913 time: 1703095586.9315455\n",
      "batch_idx: 1 loss: 0.0008122206157928542 R2: 0.903949476097381 time: 1703095589.5167227\n",
      "batch_idx: 2 loss: 0.0016490204500067007 R2: 0.90397043928087 time: 1703095592.1199808\n",
      "batch_idx: 3 loss: 0.0008043660153179147 R2: 0.9039869083497795 time: 1703095594.5767808\n",
      "Training [22%] Loss: 0.0013008932908875593 time: 1703095594.5767808\n",
      "weight: [ 0.92925101  1.5956704   0.13083517  0.73688289  0.49646124  1.22450964\n",
      "  1.17459744  0.76107125  1.32676442 -0.15016389 -0.31739716  0.33908444]\n",
      "epoch 110\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019356561271182745 R2: 0.9040103074054994 time: 1703095597.227342\n",
      "batch_idx: 1 loss: 0.0008120827062374304 R2: 0.9040348608757268 time: 1703095599.8039758\n",
      "batch_idx: 2 loss: 0.001647552301655575 R2: 0.9040559394955924 time: 1703095602.439452\n",
      "batch_idx: 3 loss: 0.0008035514090286671 R2: 0.9040723824960457 time: 1703095605.06017\n",
      "Training [22%] Loss: 0.0012997106360099868 time: 1703095605.06017\n",
      "weight: [ 0.93154355  1.59733863  0.13270081  0.73414428  0.4936674   1.2242478\n",
      "  1.17509302  0.76161139  1.32792785 -0.15016726 -0.31741032  0.33907451]\n",
      "epoch 111\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019333218817909246 R2: 0.9040960102728611 time: 1703095607.6812232\n",
      "batch_idx: 1 loss: 0.0008119478247429684 R2: 0.9041208265757488 time: 1703095610.2714553\n",
      "batch_idx: 2 loss: 0.0016460776646108042 R2: 0.904142018182602 time: 1703095612.8287308\n",
      "batch_idx: 3 loss: 0.000802732952652551 R2: 0.904158428935992 time: 1703095615.4493957\n",
      "Training [22%] Loss: 0.001298520080949312 time: 1703095615.4493957\n",
      "weight: [ 0.93387137  1.59900266  0.13461366  0.73144773  0.49088872  1.2239941\n",
      "  1.17559823  0.76216273  1.32912317 -0.15017087 -0.31742393  0.33906417]\n",
      "epoch 112\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001930963444300442 R2: 0.9041822860462277 time: 1703095617.9611917\n",
      "batch_idx: 1 loss: 0.0008118160295736245 R2: 0.9042073661029674 time: 1703095620.6252406\n",
      "batch_idx: 2 loss: 0.0016445967788143265 R2: 0.9042286682710584 time: 1703095623.215753\n",
      "batch_idx: 3 loss: 0.0008019107879843966 R2: 0.9042450406107605 time: 1703095625.8680172\n",
      "Training [22%] Loss: 0.0012973217601681974 time: 1703095625.8680172\n",
      "weight: [ 0.93623469  1.60066128  0.13657275  0.72879157  0.48812713  1.2237493\n",
      "  1.17611304  0.7627253   1.33035142 -0.15017472 -0.31743798  0.33905342]\n",
      "epoch 113\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019285809569353125 R2: 0.904269127590377 time: 1703095628.4734716\n",
      "batch_idx: 1 loss: 0.0008116873971186905 R2: 0.9042944719294029 time: 1703095631.0454195\n",
      "batch_idx: 2 loss: 0.0016431098379107172 R2: 0.9043158822795098 time: 1703095633.6714013\n",
      "batch_idx: 3 loss: 0.0008010850700299183 R2: 0.9043322100720902 time: 1703095636.2754521\n",
      "Training [23%] Loss: 0.0012961158154986597 time: 1703095636.2754521\n",
      "weight: [ 0.93863373  1.60231342  0.13857703  0.72617416  0.48538442  1.22351416\n",
      "  1.17663738  0.76329909  1.3316137  -0.15017884 -0.31745248  0.33904227]\n",
      "epoch 114\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001926174594105546 R2: 0.9043565273883921 time: 1703095638.8800228\n",
      "batch_idx: 1 loss: 0.0008115620274723257 R2: 0.904382136156111 time: 1703095641.443606\n",
      "batch_idx: 2 loss: 0.0016416169875690897 R2: 0.9044036523580271 time: 1703095644.0544746\n",
      "batch_idx: 3 loss: 0.0008002559701280633 R2: 0.9044199295476287 time: 1703095646.6331458\n",
      "Training [23%] Loss: 0.0012949023948187561 time: 1703095646.6331458\n",
      "weight: [ 0.9410687   1.60395813  0.14062528  0.7235939   0.48266227  1.22328941\n",
      "  1.17717117  0.76388411  1.33291113 -0.15018323 -0.31746742  0.33903071]\n",
      "epoch 115\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019237445516220694 R2: 0.9044444775741797 time: 1703095649.2027164\n",
      "batch_idx: 1 loss: 0.0008114400450112901 R2: 0.9044703505643797 time: 1703095651.8388813\n",
      "batch_idx: 2 loss: 0.0016401183295026555 R2: 0.9044919703314351 time: 1703095654.4583051\n",
      "batch_idx: 3 loss: 0.0007994236736675176 R2: 0.9045081909797069 time: 1703095657.1257432\n",
      "Training [23%] Loss: 0.0012936816499508831 time: 1703095657.1257432\n",
      "weight: [ 0.94353981  1.60559458  0.1427162   0.72104925  0.47996225  1.22307578\n",
      "  1.17771432  0.76448034  1.33424488 -0.15018787 -0.31748281  0.33901875]\n",
      "epoch 116\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001921291043843043 R2: 0.9045329699775241 time: 1703095659.6680846\n",
      "batch_idx: 1 loss: 0.0008113215916516726 R2: 0.9045591066523156 time: 1703095662.3374004\n",
      "batch_idx: 2 loss: 0.0016386139301638395 R2: 0.9045808277430469 time: 1703095664.8630784\n",
      "batch_idx: 3 loss: 0.0007985883751461739 R2: 0.904596986045694 time: 1703095667.5165865\n",
      "Training [23%] Loss: 0.0012924537352011823 time: 1703095667.5165865\n",
      "weight: [ 0.94604726  1.60722207  0.14484834  0.71853871  0.4772858   1.22287399\n",
      "  1.17826672  0.76508775  1.33561618 -0.15019276 -0.31749865  0.33900638]\n",
      "epoch 117\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019188143027491522 R2: 0.9046219961672379 time: 1703095670.0715976\n",
      "batch_idx: 1 loss: 0.0008112068203056924 R2: 0.9046483956648158 time: 1703095672.6007638\n",
      "batch_idx: 2 loss: 0.0016371038284716626 R2: 0.9046702158886492 time: 1703095675.1722822\n",
      "batch_idx: 3 loss: 0.0007977502759689064 R2: 0.904686306181914 time: 1703095677.8282368\n",
      "Training [23%] Loss: 0.0012912188068738533 time: 1703095677.8282368\n",
      "weight: [ 0.94859125  1.60884001  0.14702016  0.71606083  0.47463421  1.2226847\n",
      "  1.17882826  0.7657063   1.33702628 -0.15019786 -0.31751493  0.33899362]\n",
      "epoch 118\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019163145725562554 R2: 0.9047115474756422 time: 1703095680.4462292\n",
      "batch_idx: 1 loss: 0.0008110958930987657 R2: 0.904738208621936 time: 1703095683.0364\n",
      "batch_idx: 2 loss: 0.0016355880407818526 R2: 0.9047601258378121 time: 1703095685.5783978\n",
      "batch_idx: 3 loss: 0.0007969095850081825 R2: 0.904776142612968 time: 1703095688.1194305\n",
      "Training [24%] Loss: 0.001289977022861264 time: 1703095688.1194305\n",
      "weight: [ 0.95117197  1.6104479   0.14923002  0.71361425  0.47200866  1.22250859\n",
      "  1.17939881  0.76633598  1.33847652 -0.15020316 -0.31753166  0.33898045]\n",
      "epoch 119\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019137921021793226 R2: 0.9048016150097867 time: 1703095690.7473679\n",
      "batch_idx: 1 loss: 0.000810988980733835 R2: 0.9048285363422709 time: 1703095693.4775436\n",
      "batch_idx: 2 loss: 0.0016340665659732461 R2: 0.9048505484495453 time: 1703095696.0984883\n",
      "batch_idx: 3 loss: 0.0007960665182212515 R2: 0.9048664863712995 time: 1703095698.6656861\n",
      "Training [24%] Loss: 0.0012887285417769137 time: 1703095698.6656861\n",
      "weight: [ 0.9537896   1.61204535  0.15147616  0.71119766  0.4694102   1.22234628\n",
      "  1.17997826  0.76697672  1.33996825 -0.15020862 -0.31754883  0.33896689]\n",
      "epoch 120\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001911247141013421 R2: 0.9048921896641735 time: 1703095701.174775\n",
      "batch_idx: 1 loss: 0.0008108862588444985 R2: 0.904919369457305 time: 1703095703.684706\n",
      "batch_idx: 2 loss: 0.001632539392329257 R2: 0.9049414743873696 time: 1703095706.2901955\n",
      "batch_idx: 3 loss: 0.0007952212962921741 R2: 0.9049573283018809 time: 1703095708.941698\n",
      "Training [24%] Loss: 0.0012874735221198377 time: 1703095708.941698\n",
      "weight: [ 0.95644435  1.61363205  0.15375678  0.70880981  0.46683975  1.22219836\n",
      "  1.18056647  0.7676285   1.3415029  -0.15021421 -0.31756645  0.33895294]\n",
      "epoch 121\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019086799380838315 R2: 0.9049832621363979 time: 1703095711.5610352\n",
      "batch_idx: 1 loss: 0.0008107879028919085 R2: 0.9050106984191109 time: 1703095714.1272607\n",
      "batch_idx: 2 loss: 0.0016310065042861518 R2: 0.9050328941311451 time: 1703095716.695067\n",
      "batch_idx: 3 loss: 0.0007943741427294324 R2: 0.9050486590628944 time: 1703095719.2346816\n",
      "Training [24%] Loss: 0.001286212121997831 time: 1703095719.2346816\n",
      "weight: [ 0.95913638  1.61520773  0.15606996  0.70644953  0.46429812  1.22206542\n",
      "  1.1811633   0.76829127  1.34308193 -0.1502199  -0.31758452  0.33893859]\n",
      "epoch 122\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019060907406658017 R2: 0.9050748229362225 time: 1703095721.7771292\n",
      "batch_idx: 1 loss: 0.0008106940851713686 R2: 0.9051025135059435 time: 1703095724.3823545\n",
      "batch_idx: 2 loss: 0.0016294678870535043 R2: 0.9051247979822309 time: 1703095726.9535186\n",
      "batch_idx: 3 loss: 0.0007935252838664807 R2: 0.9051404691304207 time: 1703095729.5587099\n",
      "Training [24%] Loss: 0.0012849444991892887 time: 1703095729.5587099\n",
      "weight: [ 0.96186589  1.61677222  0.15841374  0.70411572  0.46178598  1.22194798\n",
      "  1.18176862  0.76896499  1.34470688 -0.15022565 -0.31760305  0.33892386]\n",
      "epoch 123\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019034797912523816 R2: 0.905166862385626 time: 1703095732.1443543\n",
      "batch_idx: 1 loss: 0.0008106049738656463 R2: 0.9051948048265931 time: 1703095734.804768\n",
      "batch_idx: 2 loss: 0.001627923529786364 R2: 0.9052171760641498 time: 1703095737.3426378\n",
      "batch_idx: 3 loss: 0.0007926749494251182 R2: 0.9052327488023563 time: 1703095739.8517852\n",
      "Training [25%] Loss: 0.0012836708110823777 time: 1703095739.8517852\n",
      "weight: [ 0.96463305  1.61832537  0.16078612  0.70180734  0.4593039   1.22184657\n",
      "  1.18238229  0.76964961  1.34637932 -0.15023142 -0.31762203  0.33890873]\n",
      "epoch 124\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0019008473253042109 R2: 0.9052593706172443 time: 1703095742.4818025\n",
      "batch_idx: 1 loss: 0.0008105207316406159 R2: 0.9052875623215486 time: 1703095745.086267\n",
      "batch_idx: 2 loss: 0.0016263734288178323 R2: 0.9053100183230122 time: 1703095747.736698\n",
      "batch_idx: 3 loss: 0.0007918233722816748 R2: 0.9053254881958223 time: 1703095750.343905\n",
      "Training [25%] Loss: 0.0012823912145110835 time: 1703095750.343905\n",
      "weight: [ 0.96743805  1.61986707  0.16318504  0.69952343  0.45685237  1.22176167\n",
      "  1.18300419  0.77034511  1.34810089 -0.15023719 -0.31764147  0.33889322]\n",
      "epoch 125\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018981935710576689 R2: 0.9053523375758689 time: 1703095752.9793475\n",
      "batch_idx: 1 loss: 0.0008104415132510779 R2: 0.9053807757611306 time: 1703095755.615601\n",
      "batch_idx: 2 loss: 0.001624817590839521 R2: 0.9054033145280711 time: 1703095758.142848\n",
      "batch_idx: 3 loss: 0.0007909707880429408 R2: 0.9054186772413478 time: 1703095760.771742\n",
      "Training [25%] Loss: 0.0012811058657978022 time: 1703095760.771742\n",
      "weight: [ 0.97028106  1.62139725  0.16560841  0.69726308  0.45443173  1.22169374\n",
      "  1.18363416  0.77105144  1.34987331 -0.15024291 -0.31766138  0.33887731]\n",
      "epoch 126\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001895518749828374 R2: 0.9054457530195934 time: 1703095763.446516\n",
      "batch_idx: 1 loss: 0.0008103674637811791 R2: 0.905474434743018 time: 1703095766.0658696\n",
      "batch_idx: 2 loss: 0.0016232560350350803 R2: 0.9054970542704839 time: 1703095768.6266706\n",
      "batch_idx: 3 loss: 0.0007901174354611365 R2: 0.9055123056798129 time: 1703095771.3587403\n",
      "Training [25%] Loss: 0.0012798149210264426 time: 1703095771.3587403\n",
      "weight: [ 0.97316225  1.62291585  0.16805414  0.69502545  0.45204228  1.22164322\n",
      "  1.18427209  0.77176859  1.35169833 -0.15024855 -0.31768176  0.33886101]\n",
      "epoch 127\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018928230754745496 R2: 0.9055396065174586 time: 1703095774.0102355\n",
      "batch_idx: 1 loss: 0.0008102987181546111 R2: 0.9055685286904721 time: 1703095776.5703015\n",
      "batch_idx: 2 loss: 0.0016216887941823438 R2: 0.9055912269604958 time: 1703095779.312809\n",
      "batch_idx: 3 loss: 0.0007892635573904747 R2: 0.9056063630617169 time: 1703095782.042201\n",
      "Training [25%] Loss: 0.0012785185363004949 time: 1703095782.042201\n",
      "weight: [ 0.9760818   1.62442282  0.17052012  0.69280978  0.4496842   1.22161052\n",
      "  1.18491783  0.77249652  1.35357777 -0.15025408 -0.31770262  0.33884433]\n",
      "epoch 128\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001890106753771388 R2: 0.9056338874461703 time: 1703095784.7112422\n",
      "batch_idx: 1 loss: 0.0008102354008026986 R2: 0.905663046850484 time: 1703095787.2996752\n",
      "batch_idx: 2 loss: 0.0016201159155738109 R2: 0.9056858218251511 time: 1703095789.9497676\n",
      "batch_idx: 3 loss: 0.0007884094014410621 R2: 0.9057008387450244 time: 1703095792.4623625\n",
      "Training [26%] Loss: 0.0012772168678972398 time: 1703095792.4623625\n",
      "weight: [ 0.97903988  1.62591812  0.17300424  0.69061534  0.4473576   1.22159603\n",
      "  1.18557126  0.77323522  1.35551354 -0.15025946 -0.31772396  0.33882724]\n",
      "epoch 129\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018873699826722295 R2: 0.9057285849893253 time: 1703095795.0643642\n",
      "batch_idx: 1 loss: 0.0008101776247707351 R2: 0.9057579782915038 time: 1703095797.6907127\n",
      "batch_idx: 2 loss: 0.0016185374620362656 R2: 0.9057808279074268 time: 1703095800.1784897\n",
      "batch_idx: 3 loss: 0.0007875552203404724 R2: 0.9057957218918757 time: 1703095802.8676054\n",
      "Training [26%] Loss: 0.0012759100724549256 time: 1703095802.8676054\n",
      "weight: [ 0.98203666  1.62740171  0.1755044   0.68844149  0.44506253  1.22160014\n",
      "  1.18623225  0.77398467  1.35750758 -0.15026467 -0.31774579  0.33880976]\n",
      "epoch 130\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001884612952991388 R2: 0.9058236881381893 time: 1703095805.4719136\n",
      "batch_idx: 1 loss: 0.0008101254908535306 R2: 0.905853311901595 time: 1703095808.0783138\n",
      "batch_idx: 2 loss: 0.0016169535126399595 R2: 0.9058762340657024 time: 1703095810.7453322\n",
      "batch_idx: 3 loss: 0.0007867012725401945 R2: 0.9058910014672689 time: 1703095813.2887077\n",
      "Training [26%] Loss: 0.0012745983072562682 time: 1703095813.2887077\n",
      "weight: [ 0.98507228  1.62887354  0.17801853  0.68628762  0.44279896  1.22162319\n",
      "  1.18690069  0.77474487  1.35956193 -0.15026967 -0.31776813  0.33879187]\n",
      "epoch 131\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018818358486825484 R2: 0.9059191856918009 time: 1703095815.8792403\n",
      "batch_idx: 1 loss: 0.000810079087273396 R2: 0.9059490363878181 time: 1703095818.5619524\n",
      "batch_idx: 2 loss: 0.0016153641629689612 R2: 0.9059720289732385 time: 1703095821.3417172\n",
      "batch_idx: 3 loss: 0.0007858478230838627 R2: 0.9059866662401582 time: 1703095823.9225423\n",
      "Training [26%] Loss: 0.001273281730502192 time: 1703095823.9225423\n",
      "weight: [ 0.98814692  1.63033354  0.18054458  0.68415319  0.44056684  1.22166553\n",
      "  1.18757644  0.77551581  1.36167868 -0.15027444 -0.31779098  0.33877357]\n",
      "epoch 132\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001879038846877804 R2: 0.9060150662570182 time: 1703095826.5029376\n",
      "batch_idx: 1 loss: 0.0008100384894876717 R2: 0.9060451402764317 time: 1703095829.1161962\n",
      "batch_idx: 2 loss: 0.001613769525320655 R2: 0.906068201118436 time: 1703095831.6173875\n",
      "batch_idx: 3 loss: 0.0007849951443298327 R2: 0.9060827047848224 time: 1703095834.2522957\n",
      "Training [26%] Loss: 0.0012719605015039908 time: 1703095834.2522957\n",
      "weight: [ 0.99126073  1.63178166  0.18308053  0.6820377   0.43836602  1.22172748\n",
      "  1.18825939  0.77629749  1.36386001 -0.15027895 -0.31781436  0.33875486]\n",
      "epoch 133\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001876222118240859 R2: 0.9061113182501735 time: 1703095837.014914\n",
      "batch_idx: 1 loss: 0.0008100037596735938 R2: 0.9061416119135115 time: 1703095839.6837683\n",
      "batch_idx: 2 loss: 0.0016121697290366992 R2: 0.9061647388063803 time: 1703095842.3501308\n",
      "batch_idx: 3 loss: 0.0007841435164519389 R2: 0.9061791054820405 time: 1703095845.320447\n",
      "Training [27%] Loss: 0.0012706347808507728 time: 1703095845.320447\n",
      "weight: [ 0.99441384  1.63321781  0.18562439  0.67994071  0.43619634  1.22180936\n",
      "  1.18894942  0.77708994  1.36610815 -0.15028316 -0.31783827  0.33873572]\n",
      "epoch 134\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018733858275392216 R2: 0.906207929900164 time: 1703095847.854741\n",
      "batch_idx: 1 loss: 0.000809974946101109 R2: 0.9062384394661652 time: 1703095850.4195352\n",
      "batch_idx: 2 loss: 0.001610564920782038 R2: 0.9062616301610408 time: 1703095853.1599936\n",
      "batch_idx: 3 loss: 0.0007832932279817333 R2: 0.9062758565213663 time: 1703095855.7551358\n",
      "Training [27%] Loss: 0.0012693047306010256 time: 1703095855.7551358\n",
      "weight: [ 0.99760641  1.63464187  0.18817422  0.67786181  0.43405761  1.22191146\n",
      "  1.18964643  0.77789315  1.36842543 -0.15028706 -0.31786274  0.33871614]\n",
      "epoch 135\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018705301340163527 R2: 0.9063048892515566 time: 1703095858.4222097\n",
      "batch_idx: 1 loss: 0.0008099520827015613 R2: 0.9063356109245942 time: 1703095861.171077\n",
      "batch_idx: 2 loss: 0.0016089552646542157 R2: 0.9063588631277073 time: 1703095863.8234043\n",
      "batch_idx: 3 loss: 0.0007824445764583132 R2: 0.906372945904822 time: 1703095866.3491392\n",
      "Training [27%] Loss: 0.0012679705144576108 time: 1703095866.3491392\n",
      "weight: [ 1.00083856  1.63605374  0.19072812  0.67580066  0.43194958  1.22203408\n",
      "  1.19035031  0.77870717  1.37081423 -0.15029062 -0.31788776  0.33869613]\n",
      "epoch 136\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001867655191585642 R2: 0.906402184167631 time: 1703095868.9523284\n",
      "batch_idx: 1 loss: 0.0008099351886899491 R2: 0.906433114104743 time: 1703095871.590542\n",
      "batch_idx: 2 loss: 0.0016073409422662764 R2: 0.9064564254758046 time: 1703095874.0868037\n",
      "batch_idx: 3 loss: 0.0007815978690063582 R2: 0.9064703614508586 time: 1703095876.6485558\n",
      "Training [27%] Loss: 0.0012666322978870563 time: 1703095876.6485558\n",
      "weight: [ 1.00411042  1.63745327  0.19328425  0.67375694  0.429872    1.22217749\n",
      "  1.19106094  0.77953201  1.37327703 -0.15029381 -0.31791337  0.33867566]\n",
      "epoch 137\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018647611491312784 R2: 0.9064998023341578 time: 1703095879.312614\n",
      "batch_idx: 1 loss: 0.0008099242680086567 R2: 0.9065309366511654 time: 1703095881.9002776\n",
      "batch_idx: 2 loss: 0.0016057221529083994 R2: 0.9065543048022905 time: 1703095884.4585876\n",
      "batch_idx: 3 loss: 0.0007807534227910215 R2: 0.9065680907979408 time: 1703095887.0168514\n",
      "Training [27%] Loss: 0.001265290248209839 time: 1703095887.0168514\n",
      "weight: [ 1.0074221   1.63884029  0.19584081  0.67173038  0.42782459  1.22234196\n",
      "  1.19177823  0.7803677   1.37581637 -0.15029661 -0.31793957  0.33865473]\n",
      "epoch 138\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018618481509095162 R2: 0.9065977312636575 time: 1703095889.6214807\n",
      "batch_idx: 1 loss: 0.0008099193086759451 R2: 0.9066290660400019 time: 1703095892.1913552\n",
      "batch_idx: 2 loss: 0.0016040991137058745 R2: 0.9066524885351454 time: 1703095894.816767\n",
      "batch_idx: 3 loss: 0.0007799115654764214 R2: 0.9066661214083448 time: 1703095897.3906069\n",
      "Training [28%] Loss: 0.0012639445346919393 time: 1703095897.3906069\n",
      "weight: [ 1.0107737   1.64021462  0.19839602  0.66972075  0.42580705  1.22252774\n",
      "  1.19250207  0.78121431  1.37843487 -0.150299   -0.31796639  0.33863331]\n",
      "epoch 139\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018589163368531946 R2: 0.9066959582993516 time: 1703095900.0261962\n",
      "batch_idx: 1 loss: 0.0008099202821979373 R2: 0.9067274895820547 time: 1703095902.5784843\n",
      "batch_idx: 2 loss: 0.0016024720597107762 R2: 0.9067509639366026 time: 1703095905.1882114\n",
      "batch_idx: 3 loss: 0.000779072635730629 R2: 0.9067644405721845 time: 1703095907.794165\n",
      "Training [28%] Loss: 0.0012625953286231344 time: 1703095907.794165\n",
      "weight: [ 1.01416532  1.64157607  0.20094821  0.66772786  0.42381907  1.22273507\n",
      "  1.19323237  0.78207185  1.38113524 -0.15030095 -0.31799385  0.3386114 ]\n",
      "epoch 140\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018559658427846804 R2: 0.9067944706185782 time: 1703095910.3522265\n",
      "batch_idx: 1 loss: 0.0008099271429857365 R2: 0.9068261944257175 time: 1703095912.9566326\n",
      "batch_idx: 2 loss: 0.0016008412439954417 R2: 0.9068497181060312 time: 1703095915.517991\n",
      "batch_idx: 3 loss: 0.0007782369836943872 R2: 0.9068630354109087 time: 1703095918.1041443\n",
      "Training [28%] Loss: 0.0012612428033650614 time: 1703095918.1041443\n",
      "weight: [ 1.01759703  1.6429244   0.2034957   0.66575153  0.42186035  1.22296419\n",
      "  1.19396904  0.78294041  1.38392025 -0.15030243 -0.31802196  0.33858898]\n",
      "epoch 141\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018529968006820228 R2: 0.9068932552358715 time: 1703095920.614479\n",
      "batch_idx: 1 loss: 0.0008099398276643777 R2: 0.9069251675593494 time: 1703095923.1401474\n",
      "batch_idx: 2 loss: 0.0015992069378047541 R2: 0.9069487379824087 time: 1703095925.7457728\n",
      "batch_idx: 3 loss: 0.0007774049713870831 R2: 0.9069618928798574 time: 1703095928.349908\n",
      "Training [28%] Loss: 0.0012598871343845596 time: 1703095928.349908\n",
      "weight: [ 1.0210689   1.64425936  0.2060369   0.66379165  0.41993057  1.22321531\n",
      "  1.19471197  0.78382003  1.38679276 -0.15030344 -0.31805075  0.33856603]\n",
      "epoch 142\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018500093390041738 R2: 0.9069922990054933 time: 1703095930.9721\n",
      "batch_idx: 1 loss: 0.0008099582543124421 R2: 0.9070243958129819 time: 1703095933.5760634\n",
      "batch_idx: 2 loss: 0.0015975694307350185 R2: 0.9070480103459898 time: 1703095936.1536994\n",
      "batch_idx: 3 loss: 0.0007765769731088483 R2: 0.9070609997699547 time: 1703095938.6197636\n",
      "Training [28%] Loss: 0.0012585284992901206 time: 1703095938.6197636\n",
      "weight: [ 1.02458097  1.6455807   0.20857025  0.66184812  0.4180294   1.22348864\n",
      "  1.19546108  0.78471077  1.3897557  -0.15030394 -0.31808023  0.33854253]\n",
      "epoch 143\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018470035829888753 R2: 0.9070915886228142 time: 1703095941.2589293\n",
      "batch_idx: 1 loss: 0.0008099823217072449 R2: 0.9071238658590852 time: 1703095943.8846824\n",
      "batch_idx: 2 loss: 0.001595929030917533 R2: 0.9071475218188377 time: 1703095946.4283733\n",
      "batch_idx: 3 loss: 0.0007757533758552562 R2: 0.9071603427084088 time: 1703095949.0892198\n",
      "Training [29%] Loss: 0.0012571670778672274 time: 1703095949.0892198\n",
      "weight: [ 1.02813327  1.64688812  0.21109424  0.65992089  0.41615652  1.22378436\n",
      "  1.19621627  0.78561273  1.39281205 -0.15030392 -0.31811045  0.33851847]\n",
      "epoch 144\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018439796549379016 R2: 0.9071911106243119 time: 1703095951.6923454\n",
      "batch_idx: 1 loss: 0.0008100119085451283 R2: 0.9072235642121326 time: 1703095954.2956104\n",
      "batch_idx: 2 loss: 0.0015942860652459125 R2: 0.9072472588639882 time: 1703095956.873623\n",
      "batch_idx: 3 loss: 0.0007749345797019123 R2: 0.9072599081578341 time: 1703095959.4053717\n",
      "Training [29%] Loss: 0.0012558030521077135 time: 1703095959.4053717\n",
      "weight: [ 1.0317258   1.64818133  0.2136074   0.6580099   0.41431162  1.22410265\n",
      "  1.19697747  0.78652596  1.3959649  -0.15030335 -0.31814141  0.33849382]\n",
      "epoch 145\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018409376745683425 R2: 0.907290851386078 time: 1703095962.0096698\n",
      "batch_idx: 1 loss: 0.000810046872586963 R2: 0.9073234772265961 time: 1703095964.58256\n",
      "batch_idx: 2 loss: 0.0015926408796760976 R2: 0.9073472077830109 time: 1703095967.2184887\n",
      "batch_idx: 3 loss: 0.0007741209981468304 R2: 0.9073596824134368 time: 1703095969.7906256\n",
      "Training [29%] Loss: 0.0012544366062445585 time: 1703095969.7906256\n",
      "weight: [ 1.03535855  1.64945999  0.21610833  0.65611516  0.4124944   1.22444366\n",
      "  1.19774459  0.78745056  1.39921737 -0.15030222 -0.31817316  0.33846856]\n",
      "epoch 146\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018378777594376921 R2: 0.9073907971205143 time: 1703095972.4434335\n",
      "batch_idx: 1 loss: 0.0008100870497548672 R2: 0.9074235910930438 time: 1703095975.023474\n",
      "batch_idx: 2 loss: 0.001590993839584639 R2: 0.9074473547116215 time: 1703095977.5585103\n",
      "batch_idx: 3 loss: 0.0007733130584360628 R2: 0.9074596515981265 time: 1703095980.1861346\n",
      "Training [29%] Loss: 0.0012530679268033152 time: 1703095980.1861346\n",
      "weight: [ 1.03903148  1.65072377  0.21859563  0.65423669  0.41070454  1.22480752\n",
      "  1.19851754  0.78838662  1.40257266 -0.1503005  -0.31820571  0.33844266]\n",
      "epoch 147\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018348000254141377 R2: 0.9074909338707805 time: 1703095982.8221612\n",
      "batch_idx: 1 loss: 0.0008101322532177979 R2: 0.9075238918321071 time: 1703095985.36405\n",
      "batch_idx: 2 loss: 0.001589345330178567 R2: 0.9075476856128827 time: 1703095987.9370131\n",
      "batch_idx: 3 loss: 0.0007725112018733634 R2: 0.9075598016552199 time: 1703095990.6331253\n",
      "Training [29%] Loss: 0.0012516972026709666 time: 1703095990.6331253\n",
      "weight: [ 1.0427445   1.65197231  0.22106797  0.65237454  0.40894175  1.22519435\n",
      "  1.19929624  0.78933424  1.40603402 -0.15029816 -0.3182391   0.3384161 ]\n",
      "epoch 148\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018317045872172934 R2: 0.907591247502571 time: 1703095993.3025336\n",
      "batch_idx: 1 loss: 0.0008101822724533838 R2: 0.9076243652858265 time: 1703095996.0063083\n",
      "batch_idx: 2 loss: 0.001587695756976355 R2: 0.9076481862677402 time: 1703095998.6692157\n",
      "batch_idx: 3 loss: 0.000771715884089357 R2: 0.9076601183382612 time: 1703096001.2727375\n",
      "Training [30%] Loss: 0.0012503246251840973 time: 1703096001.2727375\n",
      "weight: [ 1.04649752  1.65320524  0.22352406  0.65052877  0.40720575  1.22560424\n",
      "  1.20008061  0.79029351  1.40960474 -0.1502952  -0.31827336  0.33838886]\n",
      "epoch 149\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001828591559079181 R2: 0.9076917236930628 time: 1703096003.8441677\n",
      "batch_idx: 1 loss: 0.0008102368722727914 R2: 0.9077249971060726 time: 1703096006.4124255\n",
      "batch_idx: 2 loss: 0.001586045546369241 R2: 0.9077488422625161 time: 1703096009.0221965\n",
      "batch_idx: 3 loss: 0.0007709275752633241 R2: 0.9077605871975575 time: 1703096011.5786312\n",
      "Training [30%] Loss: 0.0012489503882461344 time: 1703096011.5786312\n",
      "weight: [ 1.0502904   1.65442215  0.22596264  0.64869948  0.40549626  1.22603725\n",
      "  1.20087058  0.79126454  1.41328816 -0.15029159 -0.31830853  0.33836089]\n",
      "epoch 150\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018254610555385297 R2: 0.9077923479165408 time: 1703096014.1324797\n",
      "batch_idx: 1 loss: 0.0008102957918337605 R2: 0.9078257727395856 time: 1703096016.7616045\n",
      "batch_idx: 2 loss: 0.0015843951462521512 R2: 0.9078496389728974 time: 1703096019.4400454\n",
      "batch_idx: 3 loss: 0.0007701467603048963 R2: 0.9078611935631278 time: 1703096022.070701\n",
      "Training [30%] Loss: 0.0012475746884823345 time: 1703096022.070701\n",
      "weight: [ 1.05412297  1.65562266  0.22838248  0.64688679  0.40381303  1.22649341\n",
      "  1.20166605  0.79224746  1.41708765 -0.1502873  -0.31834464  0.33833218]\n",
      "epoch 151\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018223131923715279 R2: 0.907893105426267 time: 1703096024.6187134\n",
      "batch_idx: 1 loss: 0.0008103587436668616 R2: 0.9079266774092739 time: 1703096027.2540355\n",
      "batch_idx: 2 loss: 0.001582745026719457 R2: 0.9079505615440177 time: 1703096029.8878577\n",
      "batch_idx: 3 loss: 0.0007693739389874071 R2: 0.9079619225236625 time: 1703096032.4484124\n",
      "Training [30%] Loss: 0.0012461977254363136 time: 1703096032.4484124\n",
      "weight: [ 1.05799501  1.65680634  0.23078238  0.64509083  0.4021558   1.22697275\n",
      "  1.20246697  0.79324237  1.42100659 -0.15028232 -0.31838172  0.33830269]\n",
      "epoch 152\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018191480876909461 R2: 0.9079939812321951 time: 1703096035.146108\n",
      "batch_idx: 1 loss: 0.0008104254127176965 R2: 0.9080276960912957 time: 1703096037.7499957\n",
      "batch_idx: 2 loss: 0.0015810956808308344 R2: 0.9080515948662274 time: 1703096040.3397975\n",
      "batch_idx: 3 loss: 0.0007686096260148451 R2: 0.9080627589009949 time: 1703096042.9402847\n",
      "Training [30%] Loss: 0.0012448197018135804 time: 1703096042.9402847\n",
      "weight: [ 1.06190625  1.65797276  0.2331612   0.64331176  0.40052433  1.22747523\n",
      "  1.20327324  0.79424939  1.42504839 -0.15027663 -0.31841981  0.33827239]\n",
      "epoch 153\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018159658632518655 R2: 0.9080949600741798 time: 1703096045.6247027\n",
      "batch_idx: 1 loss: 0.0008104954554139221 R2: 0.9081288134875282 time: 1703096048.1348817\n",
      "batch_idx: 2 loss: 0.001579447625444087 R2: 0.9081527235461241 time: 1703096050.7566798\n",
      "batch_idx: 3 loss: 0.0007678543510140467 R2: 0.9081636872197173 time: 1703096053.3752582\n",
      "Training [31%] Loss: 0.0012434408237809804 time: 1703096053.3752582\n",
      "weight: [ 1.06585638  1.65912147  0.2355178   0.64154976  0.3989184   1.22800081\n",
      "  1.20408479  0.79526867  1.42921642 -0.15027021 -0.31845896  0.33824124]\n",
      "epoch 154\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018127666459831332 R2: 0.9081960263902052 time: 1703096055.9917889\n",
      "batch_idx: 1 loss: 0.0008105684987845247 R2: 0.9082300139929712 time: 1703096058.5870047\n",
      "batch_idx: 2 loss: 0.0015778014021029419 R2: 0.9082539318724226 time: 1703096061.170934\n",
      "batch_idx: 3 loss: 0.0007671086584466434 R2: 0.9082646916715775 time: 1703096063.8119743\n",
      "Training [31%] Loss: 0.001242061301329311 time: 1703096063.8119743\n",
      "weight: [ 1.06984503  1.66025204  0.23785109  0.63980501  0.39733779  1.2285494\n",
      "  1.20490155  0.79630032  1.43351406 -0.15026303 -0.31849921  0.33820921]\n",
      "epoch 155\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.00180955056976406 R2: 0.9082971642792599 time: 1703096066.4130306\n",
      "batch_idx: 1 loss: 0.0008106441396552569 R2: 0.9083312816577184 time: 1703096069.0220757\n",
      "batch_idx: 2 loss: 0.001576157577971556 R2: 0.90835520377624 time: 1703096071.632646\n",
      "batch_idx: 3 loss: 0.0007663731074258851 R2: 0.9083657560742593 time: 1703096074.2492342\n",
      "Training [31%] Loss: 0.0012406813487041894 time: 1703096074.2492342\n",
      "weight: [ 1.07387178  1.66136399  0.24016001  0.63807773  0.39578231  1.22912087\n",
      "  1.20572343  0.79734448  1.43794463 -0.15025509 -0.31854059  0.33817626]\n",
      "epoch 156\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001806317777478966 R2: 0.908398357458468 time: 1703096076.8380618\n",
      "batch_idx: 1 loss: 0.0008107219439355891 R2: 0.9084326001430917 time: 1703096079.3949404\n",
      "batch_idx: 2 loss: 0.001574516746809419 R2: 0.9084565227855185 time: 1703096082.015408\n",
      "batch_idx: 3 loss: 0.0007656482714201927 R2: 0.9084668638241895 time: 1703096084.6022372\n",
      "Training [31%] Loss: 0.0012393011849110417 time: 1703096084.6022372\n",
      "weight: [ 1.07793613  1.66245686  0.2424435   0.63636814  0.39425176  1.22971509\n",
      "  1.20655036  0.79840129  1.44251141 -0.15024635 -0.31858315  0.33814234]\n",
      "epoch 157\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0018030684233798857 R2: 0.9084995892142318 time: 1703096087.1003377\n",
      "batch_idx: 1 loss: 0.000810801446019547 R2: 0.9085339526716328 time: 1703096089.7995975\n",
      "batch_idx: 2 loss: 0.0015728795299736053 R2: 0.9085578719731883 time: 1703096092.4051263\n",
      "batch_idx: 3 loss: 0.0007649347378298969 R2: 0.908567997843201 time: 1703096095.0551107\n",
      "Training [31%] Loss: 0.0012379210343007338 time: 1703096095.0551107\n",
      "weight: [ 1.08203751  1.66353018  0.24470057  0.63467648  0.39274596  1.23033185\n",
      "  1.20738226  0.79947089  1.44721757 -0.1502368  -0.31862694  0.33810743]\n",
      "epoch 158\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001799802675777298 R2: 0.9086008423470547 time: 1703096097.6447754\n",
      "batch_idx: 1 loss: 0.0008108821483302959 R2: 0.9086353219707284 time: 1703096100.3122733\n",
      "batch_idx: 2 loss: 0.0015712465774307843 R2: 0.9086592338989649 time: 1703096102.8868654\n",
      "batch_idx: 3 loss: 0.0007642331074222965 R2: 0.9086691405187454 time: 1703096105.443819\n",
      "Training [32%] Loss: 0.0012365411272401688 time: 1703096105.443819\n",
      "weight: [ 1.08617528  1.66458346  0.24693024  0.63300299  0.39126475  1.23097093\n",
      "  1.20821904  0.80055341  1.4520662  -0.15022642 -0.318672    0.33807148]\n",
      "epoch 159\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017965207200787282 R2: 0.90870209910995 time: 1703096108.0177703\n",
      "batch_idx: 1 loss: 0.0008109635210347459 R2: 0.908736690209684 time: 1703096110.5938816\n",
      "batch_idx: 2 loss: 0.0015696185687620982 R2: 0.9087605905445726 time: 1703096113.1790807\n",
      "batch_idx: 3 loss: 0.0007635439936054951 R2: 0.908770273637755 time: 1703096115.7542684\n",
      "Training [32%] Loss: 0.0012351617008702668 time: 1703096115.7542684\n",
      "weight: [ 1.09034872  1.66561623  0.24913155  0.63134793  0.38980796  1.23163207\n",
      "  1.20906063  0.801649    1.45706026 -0.1502152  -0.31871837  0.33803444]\n",
      "epoch 160\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017932227621972412 R2: 0.9088033411404128 time: 1703096118.3119786\n",
      "batch_idx: 1 loss: 0.0008110450019517899 R2: 0.9088380389303113 time: 1703096120.8907764\n",
      "batch_idx: 2 loss: 0.0015679962141415317 R2: 0.9088619232425105 time: 1703096123.395132\n",
      "batch_idx: 3 loss: 0.000762868021520942 R2: 0.9088713783141029 time: 1703096126.0934734\n",
      "Training [32%] Loss: 0.001233782999952876 time: 1703096126.0934734\n",
      "weight: [ 1.09455701  1.666628    0.25130359  0.62971157  0.38837544  1.23231497\n",
      "  1.20990692  0.80275781  1.46220255 -0.15020311 -0.31876612  0.33799627]\n",
      "epoch 161\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017899090323436999 R2: 0.9089045493860572 time: 1703096128.702781\n",
      "batch_idx: 1 loss: 0.0008111259966817333 R2: 0.9089393489711076 time: 1703096131.3818843\n",
      "batch_idx: 2 loss: 0.0015663802552625095 R2: 0.9089632125985586 time: 1703096134.04997\n",
      "batch_idx: 3 loss: 0.0007622058269372552 R2: 0.908972434910047 time: 1703096136.6328847\n",
      "Training [32%] Loss: 0.0012324052778062995 time: 1703096136.6328847\n",
      "weight: [ 1.09879923  1.6676183   0.25344545  0.62809417  0.38696704  1.23301928\n",
      "  1.21075784  0.80387996  1.46749568 -0.15019015 -0.31881528  0.33795692]\n",
      "epoch 162\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017865797892067472 R2: 0.9090057040242892 time: 1703096139.4937088\n",
      "batch_idx: 1 loss: 0.0008112058789855766 R2: 0.9090406003854745 time: 1703096142.084672\n",
      "batch_idx: 2 loss: 0.0015647714661838712 R2: 0.9090644384084298 time: 1703096144.7911174\n",
      "batch_idx: 3 loss: 0.0007615580549266916 R2: 0.9090734229520822 time: 1703096147.4242198\n",
      "Training [32%] Loss: 0.0012310287973257216 time: 1703096147.4242198\n",
      "weight: [ 1.10307437  1.66858666  0.25555628  0.62649601  0.38558263  1.23374462\n",
      "  1.21161328  0.80501559  1.47294204 -0.15017631 -0.31886591  0.33791636]\n",
      "epoch 163\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017832353245190162 R2: 0.9091067843765485 time: 1703096150.0741847\n",
      "batch_idx: 1 loss: 0.0008112839914386435 R2: 0.909141772354529 time: 1703096152.6343253\n",
      "batch_idx: 2 loss: 0.0015631706540640256 R2: 0.9091655795692807 time: 1703096155.2695966\n",
      "batch_idx: 3 loss: 0.0007609253583051002 R2: 0.909174321042015 time: 1703096158.0163507\n",
      "Training [33%] Loss: 0.0012296538320816963 time: 1703096158.0163507\n",
      "weight: [ 1.10738129  1.6695326   0.25763526  0.62491737  0.38422206  1.23449059\n",
      "  1.21247314  0.80616484  1.4785438  -0.15016156 -0.31891805  0.33787452]\n",
      "epoch 164\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001779875967999597 R2: 0.9092077688180021 time: 1703096160.5850232\n",
      "batch_idx: 1 loss: 0.0008113596463806405 R2: 0.9092428430954351 time: 1703096163.3004518\n",
      "batch_idx: 2 loss: 0.001561578659747476 R2: 0.9092666139870712 time: 1703096165.9621449\n",
      "batch_idx: 3 loss: 0.0007603083958186432 R2: 0.9092751067642963 time: 1703096168.5981765\n",
      "Training [33%] Loss: 0.001228280667486589 time: 1703096168.5981765\n",
      "weight: [ 1.11171874  1.67045567  0.25968158  0.62335851  0.38288519  1.23525673\n",
      "  1.21333731  0.80732783  1.48430282 -0.15014591 -0.31897175  0.33783138]\n",
      "epoch 165\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017765020926483882 R2: 0.9093086346837966 time: 1703096171.229872\n",
      "batch_idx: 1 loss: 0.0008114321271836553 R2: 0.9093437897664872 time: 1703096173.906825\n",
      "batch_idx: 2 loss: 0.0015599963581626225 R2: 0.9093675184810716 time: 1703096176.508205\n",
      "batch_idx: 3 loss: 0.000759707830063713 R2: 0.9093757565910371 time: 1703096179.1002212\n",
      "Training [33%] Loss: 0.0012269096020145946 time: 1703096179.1002212\n",
      "weight: [ 1.11608535  1.67135543  0.26169451  0.6218197   0.38157189  1.23604254\n",
      "  1.21420567  0.80850467  1.49022066 -0.15012934 -0.31902707  0.33778687]\n",
      "epoch 166\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017731141203536764 R2: 0.9094093581734203 time: 1703096181.7193708\n",
      "batch_idx: 1 loss: 0.0008115006898556501 R2: 0.9094445883704895 time: 1703096184.3399088\n",
      "batch_idx: 2 loss: 0.0015584246584858408 R2: 0.9094682686872299 time: 1703096186.9362378\n",
      "batch_idx: 3 loss: 0.0007591243251291456 R2: 0.9094762457864582 time: 1703096189.5486863\n",
      "Training [33%] Loss: 0.0012255409484560782 time: 1703096189.5486863\n",
      "weight: [ 1.1204796   1.67223145  0.26367331  0.62030121  0.380282    1.23684749\n",
      "  1.2150781   0.80969548  1.49629854 -0.15011185 -0.31908405  0.33774095]\n",
      "epoch 167\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017697125277599975 R2: 0.9095099142550312 time: 1703096192.169501\n",
      "batch_idx: 1 loss: 0.0008115645649911289 R2: 0.9095452136584328 time: 1703096194.769733\n",
      "batch_idx: 2 loss: 0.0015568645040231056 R2: 0.909568838962435 time: 1703096197.41381\n",
      "batch_idx: 3 loss: 0.0007585585439537638 R2: 0.9095765483129646 time: 1703096200.0692523\n",
      "Training [33%] Loss: 0.001224175035181999 time: 1703096200.0692523\n",
      "weight: [ 1.12489984  1.67308333  0.26561731  0.6188033   0.37901539  1.23767101\n",
      "  1.21595444  0.81090034  1.5025373  -0.15009344 -0.31914274  0.33769357]\n",
      "epoch 168\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017662978523258713 R2: 0.9096102765720394 time: 1703096202.6777995\n",
      "batch_idx: 1 loss: 0.0008116229600760794 R2: 0.9096456390358382 time: 1703096205.4137163\n",
      "batch_idx: 2 loss: 0.001555316871756226 R2: 0.9096692022922023 time: 1703096207.9823503\n",
      "batch_idx: 3 loss: 0.0007580111453987028 R2: 0.9096766367413937 time: 1703096210.6731548\n",
      "Training [34%] Loss: 0.00122281220738922 time: 1703096210.6731548\n",
      "weight: [ 1.12934428  1.67391067  0.2675259   0.61732618  0.37777188  1.23851249\n",
      "  1.21683457  0.81211933  1.50893736 -0.15007411 -0.31920319  0.33764469]\n",
      "epoch 169\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017628706984812912 R2: 0.9097104173546471 time: 1703096213.3819873\n",
      "batch_idx: 1 loss: 0.0008116750621493775 R2: 0.9097458364745075 time: 1703096215.9083364\n",
      "batch_idx: 2 loss: 0.0015537827714966162 R2: 0.9097693302046407 time: 1703096218.5968637\n",
      "batch_idx: 3 loss: 0.0007574827810406675 R2: 0.909776482168405 time: 1703096221.216964\n",
      "Training [34%] Loss: 0.0012214528282919882 time: 1703096221.216964\n",
      "weight: [ 1.13381094  1.67471313  0.26939847  0.61587009  0.3765513   1.23937128\n",
      "  1.21771829  0.81335253  1.51549871 -0.15005387 -0.31926544  0.33759425]\n",
      "epoch 170\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017594317437752741 R2: 0.9098103073393856 time: 1703096223.8382761\n",
      "batch_idx: 1 loss: 0.0008117200408160735 R2: 0.909845776432879 time: 1703096226.4126298\n",
      "batch_idx: 2 loss: 0.0015522632445878876 R2: 0.9098691926939558 time: 1703096229.057682\n",
      "batch_idx: 3 loss: 0.0007569740916995776 R2: 0.9098760541443541 time: 1703096231.6506066\n",
      "Training [34%] Loss: 0.0012200972802197032 time: 1703096231.6506066\n",
      "weight: [ 1.13829772  1.67549037  0.27123452  0.61443523  0.37535348  1.24024668\n",
      "  1.21860545  0.81459997  1.52222087 -0.15003273 -0.31932953  0.33754223]\n",
      "epoch 171\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017559817448849304 R2: 0.9099099157001073 time: 1703096234.1615148\n",
      "batch_idx: 1 loss: 0.0008117570516015685 R2: 0.909945427788467 time: 1703096236.8359766\n",
      "batch_idx: 2 loss: 0.001550759362096748 R2: 0.9099687581570836 time: 1703096239.3665028\n",
      "batch_idx: 3 loss: 0.0007564857037228444 R2: 0.9099753206152472 time: 1703096241.9857883\n",
      "Training [34%] Loss: 0.0012187459655765228 time: 1703096241.9857883\n",
      "weight: [ 1.14280234  1.67624212  0.27303356  0.61302177  0.37417819  1.24113797\n",
      "  1.21949583  0.81586167  1.52910286 -0.1500107  -0.3193955   0.33748856]\n",
      "epoch 172\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017525215433378191 R2: 0.9100092099941385 time: 1703096244.5789833\n",
      "batch_idx: 1 loss: 0.0008117852396300742 R2: 0.9100447577861731 time: 1703096247.138286\n",
      "batch_idx: 2 loss: 0.001549272222431813 R2: 0.9100679933472963 time: 1703096249.7269123\n",
      "batch_idx: 3 loss: 0.0007560182250583279 R2: 0.9100742478826904 time: 1703096252.2361224\n",
      "Training [34%] Loss: 0.0012173993076145084 time: 1703096252.2361224\n",
      "weight: [ 1.14732235  1.67696812  0.27479518  0.61162986  0.37302521  1.24204436\n",
      "  1.22038922  0.81713763  1.53614321 -0.1499878  -0.31946339  0.33743321]\n",
      "epoch 173\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017490520707815107 R2: 0.9101081561274986 time: 1703096254.9192238\n",
      "batch_idx: 1 loss: 0.0008118037436056029 R2: 0.9101437320064052 time: 1703096257.460114\n",
      "batch_idx: 2 loss: 0.0015478029483321871 R2: 0.9101668633487353 time: 1703096260.1115708\n",
      "batch_idx: 3 loss: 0.0007555722411576054 R2: 0.910172800585749 time: 1703096262.6750958\n",
      "Training [35%] Loss: 0.0012160577509692265 time: 1703096262.6750958\n",
      "weight: [ 1.15185513  1.67766817  0.27651904  0.61025962  0.3718943   1.24296506\n",
      "  1.22128538  0.81842782  1.54333989 -0.14996406 -0.31953323  0.33737614]\n",
      "epoch 174\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017455743536201652 R2: 0.9102067183431958 time: 1703096265.326426\n",
      "batch_idx: 1 loss: 0.0008118117000684693 R2: 0.9102423143570487 time: 1703096267.867016\n",
      "batch_idx: 2 loss: 0.0015463526831723923 R2: 0.9102653315759062 time: 1703096270.4356904\n",
      "batch_idx: 3 loss: 0.0007551483107609563 R2: 0.9102709417087217 time: 1703096273.102197\n",
      "Training [35%] Loss: 0.0012147217619054957 time: 1703096273.102197\n",
      "weight: [ 1.1563979   1.6783421   0.27820483  0.60891112  0.37078515  1.24389919\n",
      "  1.22218405  0.81973218  1.55069037 -0.14993949 -0.31960506  0.33731732]\n",
      "epoch 175\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017420895168272642 R2: 0.9103048592364884 time: 1703096275.6354876\n",
      "batch_idx: 1 loss: 0.0008118082478969215 R2: 0.9103404670931129 time: 1703096278.2717376\n",
      "batch_idx: 2 loss: 0.0015449225865379736 R2: 0.9103633598019046 time: 1703096280.8589208\n",
      "batch_idx: 3 loss: 0.0007547469616250422 R2: 0.9103686326185333 time: 1703096283.4536872\n",
      "Training [35%] Loss: 0.0012133918282218005 time: 1703096283.4536872\n",
      "weight: [ 1.16094769  1.67898983  0.27985236  0.60758441  0.36969747  1.24484588\n",
      "  1.22308495  0.82105061  1.55819154 -0.14991413 -0.31967888  0.3372567 ]\n",
      "epoch 176\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017385987867381055 R2: 0.9104025398007908 time: 1703096286.0001943\n",
      "batch_idx: 1 loss: 0.0008117925330211485 R2: 0.9104381508677616 time: 1703096288.5068603\n",
      "batch_idx: 2 loss: 0.0015435138290370448 R2: 0.9104609082189169 time: 1703096291.0903502\n",
      "batch_idx: 3 loss: 0.0007543686862628155 R2: 0.9104658331351111 time: 1703096293.7372668\n",
      "Training [35%] Loss: 0.0012120684587647785 time: 1703096293.7372668\n",
      "weight: [ 1.1655014   1.67961129  0.28146148  0.60627948  0.36863091  1.24580417\n",
      "  1.22398775  0.82238298  1.56583977 -0.14988802 -0.31975473  0.33719427]\n",
      "epoch 177\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017351034926270854 R2: 0.9104997195074326 time: 1703096296.3814766\n",
      "batch_idx: 1 loss: 0.0008117637133155909 R2: 0.910535324817715 time: 1703096298.9144385\n",
      "batch_idx: 2 loss: 0.0015421275863278835 R2: 0.910557935533903 time: 1703096301.6014636\n",
      "batch_idx: 3 loss: 0.0007540139377719702 R2: 0.9105625016374846 time: 1703096304.157803\n",
      "Training [35%] Loss: 0.0012107521825106325 time: 1703096304.157803\n",
      "weight: [ 1.17005575  1.6802065   0.28303212  0.60499629  0.36758508  1.2467731\n",
      "  1.22489212  0.82372913  1.57363089 -0.14986121 -0.31983262  0.33712999]\n",
      "epoch 178\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017316050668838925 R2: 0.9105963564217715 time: 1703096306.6991065\n",
      "batch_idx: 1 loss: 0.0008117209636348181 R2: 0.9106319466855408 time: 1703096309.4363446\n",
      "batch_idx: 2 loss: 0.0015407650323617668 R2: 0.910654399101617 time: 1703096311.9779787\n",
      "batch_idx: 3 loss: 0.0007536831258332981 R2: 0.9106585952075428 time: 1703096314.634467\n",
      "Training [36%] Loss: 0.001209443547178444 time: 1703096314.634467\n",
      "weight: [ 1.17460732  1.68077554  0.2845643   0.60373474  0.36655956  1.24775166\n",
      "  1.22579769  0.82508885  1.58156021 -0.14983372 -0.31991255  0.33706385]\n",
      "epoch 179\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017281050436202285 R2: 0.9106924073573645 time: 1703096317.2572026\n",
      "batch_idx: 1 loss: 0.0008116634809588217 R2: 0.9107279729802448 time: 1703096319.935001\n",
      "batch_idx: 2 loss: 0.0015394273318630567 R2: 0.9107502550961849 time: 1703096322.5494905\n",
      "batch_idx: 3 loss: 0.000753376612962599 R2: 0.9107540698123696 time: 1703096325.1068227\n",
      "Training [36%] Loss: 0.0012081431173511766 time: 1703096325.1068227\n",
      "weight: [ 1.17915254  1.68131853  0.28605811  0.60249468  0.36555389  1.24873879\n",
      "  1.22670406  0.82646188  1.58962255 -0.14980562 -0.31999452  0.33699583]\n",
      "epoch 180\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.00172460505556533 R2: 0.9107878280687484 time: 1703096327.6642604\n",
      "batch_idx: 1 loss: 0.0008115904896136219 R2: 0.910823359176604 time: 1703096330.25354\n",
      "batch_idx: 2 loss: 0.0015381156320951653 R2: 0.9108454587212771 time: 1703096332.9475312\n",
      "batch_idx: 3 loss: 0.0007530947110992 R2: 0.9108488805248326 time: 1703096335.5504682\n",
      "Training [36%] Loss: 0.0012068514720933294 time: 1703096335.5504682\n",
      "weight: [ 1.18368774  1.68183568  0.28751374  0.60127592  0.36456758  1.24973342\n",
      "  1.22761081  0.82784794  1.59781226 -0.14977696 -0.32007853  0.33692591]\n",
      "epoch 181\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017211068291442678 R2: 0.9108825734821773 time: 1703096338.187392\n",
      "batch_idx: 1 loss: 0.0008115012465330539 R2: 0.9109180599523373 time: 1703096340.7489147\n",
      "batch_idx: 2 loss: 0.001536831053991609 R2: 0.9109399644576112 time: 1703096343.3335001\n",
      "batch_idx: 3 loss: 0.0007528376786101485 R2: 0.9109429817808715 time: 1703096345.9265084\n",
      "Training [36%] Loss: 0.0012055692020697696 time: 1703096345.9265084\n",
      "weight: [ 1.18820914  1.68232726  0.28893144  0.60007819  0.36360007  1.25073442\n",
      "  1.22851747  0.82924667  1.60612326 -0.14974779 -0.32016457  0.3368541 ]\n",
      "epoch 182\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017176121776778762 R2: 0.9109765979622972 time: 1703096348.559699\n",
      "batch_idx: 1 loss: 0.0008113950465259377 R2: 0.9110120294608697 time: 1703096351.1969757\n",
      "batch_idx: 2 loss: 0.0015355746827634652 R2: 0.9110337263451245 time: 1703096353.7399035\n",
      "batch_idx: 3 loss: 0.0007526057177816763 R2: 0.9110363276703735 time: 1703096356.3030996\n",
      "Training [36%] Loss: 0.0012042969061872388 time: 1703096356.3030996\n",
      "weight: [ 1.19271286  1.68279362  0.29031156  0.59890118  0.36265078  1.25174068\n",
      "  1.22942356  0.8306577   1.61454907 -0.14971817 -0.32025262  0.33678039]\n",
      "epoch 183\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017141229926951663 R2: 0.9110698556112675 time: 1703096358.8856063\n",
      "batch_idx: 1 loss: 0.0008112712275096167 R2: 0.9111052216360049 time: 1703096361.491438\n",
      "batch_idx: 2 loss: 0.0015343475581286125 R2: 0.9111266982957444 time: 1703096364.1068027\n",
      "batch_idx: 3 loss: 0.0007523989728589629 R2: 0.9111288722572242 time: 1703096366.72283\n",
      "Training [37%] Loss: 0.0012030351877980897 time: 1703096366.72283\n",
      "weight: [ 1.19719499  1.68323516  0.29165453  0.59774453  0.36171908  1.25275103\n",
      "  1.23032856  0.83208059  1.62308286 -0.14968816 -0.32034266  0.33670478]\n",
      "epoch 184\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017106412334075515 R2: 0.9111623005954863 time: 1703096369.2366962\n",
      "batch_idx: 1 loss: 0.0008111291756653464 R2: 0.9111975905234339 time: 1703096371.805128\n",
      "batch_idx: 2 loss: 0.0015331506643410742 R2: 0.9112188344312481 time: 1703096374.426955\n",
      "batch_idx: 3 loss: 0.000752217528681168 R2: 0.9112205699226953 time: 1703096377.0363648\n",
      "Training [37%] Loss: 0.001201784650523785 time: 1703096377.0363648\n",
      "weight: [ 1.20165157  1.68365234  0.29296085  0.59660782  0.36080432  1.25376429\n",
      "  1.23123191  0.83351487  1.63171753 -0.14965783 -0.32043464  0.33662729]\n",
      "epoch 185\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001707168914456765 R2: 0.9112538874936844 time: 1703096379.5668812\n",
      "batch_idx: 1 loss: 0.0008109683304630318 R2: 0.9112890906326984 time: 1703096382.2548196\n",
      "batch_idx: 2 loss: 0.0015319849202292074 R2: 0.911310089439547 time: 1703096384.8566494\n",
      "batch_idx: 3 loss: 0.0007520614099426345 R2: 0.9113113757250725 time: 1703096387.5072405\n",
      "Training [37%] Loss: 0.0012005458937729098 time: 1703096387.5072405\n",
      "weight: [ 1.20607862  1.68404572  0.29423111  0.59549056  0.35990578  1.2547793\n",
      "  1.23213305  0.83496003  1.6404457  -0.14962722 -0.32052855  0.33654794]\n",
      "epoch 186\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001703708092111525 R2: 0.9113445716590294 time: 1703096390.1421103\n",
      "batch_idx: 1 loss: 0.0008107881894927926 R2: 0.9113796773020921 time: 1703096392.7735775\n",
      "batch_idx: 2 loss: 0.0015308511694772603 R2: 0.9114004189415523 time: 1703096395.340014\n",
      "batch_idx: 3 loss: 0.0007519305810927987 R2: 0.9114012457675976 time: 1703096397.953315\n",
      "Training [37%] Loss: 0.001199319508043594 time: 1703096397.953315\n",
      "weight: [ 1.21047222  1.68441587  0.29546596  0.59439224  0.35902274  1.25579487\n",
      "  1.23303138  0.83641549  1.64925981 -0.1495964  -0.32062432  0.33646675]\n",
      "epoch 187\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0017002608491500232 R2: 0.911434309587008 time: 1703096400.5007477\n",
      "batch_idx: 1 loss: 0.0008105883130294927 R2: 0.9114693070681689 time: 1703096403.156165\n",
      "batch_idx: 2 loss: 0.001529750171402423 R2: 0.9114897798601336 time: 1703096405.8497477\n",
      "batch_idx: 3 loss: 0.0007518249468687185 R2: 0.9114901375658867 time: 1703096408.5185828\n",
      "Training [37%] Loss: 0.0011981060701126644 time: 1703096408.5185828\n",
      "weight: [ 1.21482847  1.68476347  0.29666614  0.59331228  0.35815444  1.25680983\n",
      "  1.23392628  0.83788067  1.65815216 -0.14956544 -0.32072192  0.33638374]\n",
      "epoch 188\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016968292787213335 R2: 0.9115230592801854 time: 1703096411.0979037\n",
      "batch_idx: 1 loss: 0.0008103683282449645 R2: 0.9115579380310012 time: 1703096413.6404424\n",
      "batch_idx: 2 loss: 0.0015286825924881867 R2: 0.9115781307822255 time: 1703096416.274705\n",
      "batch_idx: 3 loss: 0.0007517443534353323 R2: 0.9115780104058849 time: 1703096418.9131758\n",
      "Training [38%] Loss: 0.0011969061382224543 time: 1703096418.9131758\n",
      "weight: [ 1.21914359  1.6850892   0.29783243  0.59225007  0.35730007  1.25782301\n",
      "  1.23481711  0.83935492  1.66711499 -0.14953437 -0.32082129  0.33629895]\n",
      "epoch 189\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001693415467526852 R2: 0.9116107806008875 time: 1703096421.5383663\n",
      "batch_idx: 1 loss: 0.000810127932972466 R2: 0.9116455302061016 time: 1703096424.2609413\n",
      "batch_idx: 2 loss: 0.0015276489989311283 R2: 0.9116654323050144 time: 1703096426.857576\n",
      "batch_idx: 3 loss: 0.0007516885900914778 R2: 0.9116648256833175 time: 1703096429.5124927\n",
      "Training [38%] Loss: 0.001195720247380481 time: 1703096429.5124927\n",
      "weight: [ 1.22341388  1.68539381  0.29896568  0.59120494  0.35645884  1.25883328\n",
      "  1.23570322  0.84083758  1.67614047 -0.14950325 -0.32092237  0.33621241]\n",
      "epoch 190\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016900214786986878 R2: 0.9116974356028604 time: 1703096432.0750449\n",
      "batch_idx: 1 loss: 0.0008098668989211331 R2: 0.9117320458542997 time: 1703096434.6149838\n",
      "batch_idx: 2 loss: 0.0015266498504417773 R2: 0.9117516473576772 time: 1703096437.4192958\n",
      "batch_idx: 3 loss: 0.0007516573914849803 R2: 0.9117505472161787 time: 1703096440.1683087\n",
      "Training [38%] Loss: 0.0011945489048866445 time: 1703096440.1683087\n",
      "weight: [ 1.22763583  1.68567811  0.3000668   0.59017621  0.35562992  1.25983954\n",
      "  1.23658396  0.84232795  1.6852208  -0.14947214 -0.32102511  0.33612418]\n",
      "epoch 191\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016866493347734532 R2: 0.9117829888337446 time: 1703096442.8713782\n",
      "batch_idx: 1 loss: 0.0008095850742361544 R2: 0.9118174497813589 time: 1703096445.5666413\n",
      "batch_idx: 2 loss: 0.0015256854955102395 R2: 0.9118367414907157 time: 1703096448.2505112\n",
      "batch_idx: 3 loss: 0.000751650440268712 R2: 0.9118351415225862 time: 1703096450.8554764\n",
      "Training [38%] Loss: 0.0011933925861971396 time: 1703096450.8554764\n",
      "weight: [ 1.23180607  1.6859429   0.30113671  0.58916317  0.35481247  1.26084071\n",
      "  1.23745865  0.84382531  1.69434828 -0.14944106 -0.32112943  0.3360343 ]\n",
      "epoch 192\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016833010011650569 R2: 0.9118674076010415 time: 1703096453.507284\n",
      "batch_idx: 1 loss: 0.0008092823853048322 R2: 0.911901709600245 time: 1703096456.1170888\n",
      "batch_idx: 2 loss: 0.0015247561683040924 R2: 0.9119206831261314 time: 1703096458.6234038\n",
      "batch_idx: 3 loss: 0.0007516673701224007 R2: 0.9119185780575725 time: 1703096461.201371\n",
      "Training [38%] Loss: 0.0011922517312240957 time: 1703096461.201371\n",
      "weight: [ 1.23592145  1.68618902  0.3021764   0.58816507  0.35400567  1.26183577\n",
      "  1.23832664  0.84532894  1.70351527 -0.14941006 -0.32123528  0.33594281]\n",
      "epoch 193\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016799783705277473 R2: 0.9119506621955994 time: 1703096463.691088\n",
      "batch_idx: 1 loss: 0.000808958837720393 R2: 0.9119847959501944 time: 1703096466.2669406\n",
      "batch_idx: 2 loss: 0.0015238619873120297 R2: 0.912003443763106 time: 1703096468.7570963\n",
      "batch_idx: 3 loss: 0.0007517077690617558 R2: 0.9120008294038703 time: 1703096471.3864515\n",
      "Training [39%] Loss: 0.0011911267411554814 time: 1703096471.3864515\n",
      "weight: [ 1.23997905  1.68641735  0.30318688  0.58718116  0.35320868  1.26282377\n",
      "  1.23918727  0.84683807  1.71271432 -0.14937917 -0.32134259  0.33584976]\n",
      "epoch 194\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001676683248371405 R2: 0.9120327260681824 time: 1703096474.028967\n",
      "batch_idx: 1 loss: 0.0008086145163345949 R2: 0.9120666826683026 time: 1703096476.6964004\n",
      "batch_idx: 2 loss: 0.0015230029557842385 R2: 0.9120849981353414 time: 1703096479.3370576\n",
      "batch_idx: 3 loss: 0.0007517711829578839 R2: 0.9120818714133266 time: 1703096481.925743\n",
      "Training [39%] Loss: 0.0011900179758620305 time: 1703096481.925743\n",
      "weight: [ 1.24397619  1.68662875  0.30416915  0.58621069  0.35242067  1.26380379\n",
      "  1.24003989  0.84835195  1.72193813 -0.14934841 -0.32145129  0.33575522]\n",
      "epoch 195\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016734173402456107 R2: 0.9121135759563769 time: 1703096484.57668\n",
      "batch_idx: 1 loss: 0.0008082495843561213 R2: 0.9121473469110855 time: 1703096487.179984\n",
      "batch_idx: 2 loss: 0.0015221789639532836 R2: 0.9121653243180438 time: 1703096489.820569\n",
      "batch_idx: 3 loss: 0.0007518571191948353 R2: 0.912161683297484 time: 1703096492.536417\n",
      "Training [39%] Loss: 0.0011889257519374628 time: 1703096492.536417\n",
      "weight: [ 1.24791047  1.68682409  0.30512425  0.58525289  0.35164085  1.26477499\n",
      "  1.24088387  0.84986983  1.73117965 -0.1493178  -0.32156132  0.33565925]\n",
      "epoch 196\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016701822407504253 R2: 0.9121931919608164 time: 1703096495.1245239\n",
      "batch_idx: 1 loss: 0.000807864281483037 R2: 0.9122267692251429 time: 1703096497.760791\n",
      "batch_idx: 2 loss: 0.0015213897929522897 R2: 0.9122444037842146 time: 1703096500.2955532\n",
      "batch_idx: 3 loss: 0.0007519650504010701 R2: 0.9122402476674504 time: 1703096502.9213076\n",
      "Training [39%] Loss: 0.0011878503413967055 time: 1703096502.9213076\n",
      "weight: [ 1.25177973  1.68700425  0.30605322  0.58430699  0.35086844  1.26573661\n",
      "  1.24171859  0.85139096  1.74043203 -0.14928735 -0.32167261  0.33556188]\n",
      "epoch 197\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001666979424563108 R2: 0.9122715575714564 time: 1703096505.4810514\n",
      "batch_idx: 1 loss: 0.0008074589210919561 R2: 0.9123049335677808 time: 1703096508.1500304\n",
      "batch_idx: 2 loss: 0.001520635120284284 R2: 0.9123222214115729 time: 1703096510.8489785\n",
      "batch_idx: 3 loss: 0.0007520944182003077 R2: 0.9123175505249164 time: 1703096513.4115734\n",
      "Training [39%] Loss: 0.001186791971034914 time: 1703096513.4115734\n",
      "weight: [ 1.25558213  1.68717007  0.30695708  0.58337226  0.35010266  1.26668795\n",
      "  1.24254348  0.85291458  1.74968873 -0.14925708 -0.3217851   0.33546319]\n",
      "epoch 198\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016638102395956187 R2: 0.9123486596461274 time: 1703096516.0588794\n",
      "batch_idx: 1 loss: 0.0008070338865412529 R2: 0.912381827280028 time: 1703096518.6177893\n",
      "batch_idx: 2 loss: 0.0015199145266424363 R2: 0.9123987654429154 time: 1703096521.1971657\n",
      "batch_idx: 3 loss: 0.0007522446369378048 R2: 0.9123935812075894 time: 1703096523.725316\n",
      "Training [40%] Loss: 0.001185750822429278 time: 1703096523.725316\n",
      "weight: [ 1.2593161   1.6873224   0.30783685  0.58244797  0.34934279  1.26762841\n",
      "  1.24335797  0.85443997  1.75894343 -0.14922699 -0.32189872  0.33536324]\n",
      "epoch 199\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001660675902321647 R2: 0.9124244883450435 time: 1703096526.328936\n",
      "batch_idx: 1 loss: 0.0008065896266781997 R2: 0.9124574410157548 time: 1703096528.909445\n",
      "batch_idx: 2 loss: 0.001519227503838853 R2: 0.9124740274039753 time: 1703096531.4347503\n",
      "batch_idx: 3 loss: 0.0007524150973482718 R2: 0.9124683322935331 time: 1703096534.0582297\n",
      "Training [40%] Loss: 0.0011847270325467429 time: 1703096534.0582297\n",
      "weight: [ 1.26298036  1.68746206  0.30869353  0.58153339  0.34858815  1.26855745\n",
      "  1.24416155  0.85596641  1.76819015 -0.14919708 -0.32201341  0.33526207]\n",
      "epoch 200\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016575774952388613 R2: 0.9124990370259096 time: 1703096536.6169703\n",
      "batch_idx: 1 loss: 0.0008061266506673361 R2: 0.9125317686317154 time: 1703096539.2934911\n",
      "batch_idx: 2 loss: 0.0015185734635725131 R2: 0.9125480019838774 time: 1703096541.8412533\n",
      "batch_idx: 3 loss: 0.0007526051701404669 R2: 0.9125417994697168 time: 1703096544.4016054\n",
      "Training [40%] Loss: 0.0011837206949047944 time: 1703096544.4016054\n",
      "weight: [ 1.26657392  1.68758983  0.3095281   0.58062785  0.34783808  1.26947462\n",
      "  1.24495372  0.85749322  1.77742318 -0.14916734 -0.32212911  0.33515974]\n",
      "epoch 201\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001654515966366167 R2: 0.9125723021051956 time: 1703096547.0046144\n",
      "batch_idx: 1 loss: 0.0008056455222779089 R2: 0.912604807044095 time: 1703096549.5630207\n",
      "batch_idx: 2 loss: 0.0015179517467547709 R2: 0.9126206868838045 time: 1703096552.198689\n",
      "batch_idx: 3 loss: 0.0007528142094805995 R2: 0.9126139813708013 time: 1703096554.740569\n",
      "Training [40%] Loss: 0.0011827318612198615 time: 1703096554.740569\n",
      "weight: [ 1.27009605  1.68770647  0.31034151  0.57973069  0.34709196  1.27037955\n",
      "  1.24573403  0.85901972  1.78663712 -0.14913777 -0.32224577  0.33505632]\n",
      "epoch 202\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016514921306191644 R2: 0.9126442828915187 time: 1703096557.4084895\n",
      "batch_idx: 1 loss: 0.0008051468537807133 R2: 0.9126765560576062 time: 1703096559.9709408\n",
      "batch_idx: 2 loss: 0.001517361633115016 R2: 0.9126920826401188 time: 1703096562.526183\n",
      "batch_idx: 3 loss: 0.0007530415563615572 R2: 0.9126848793943181 time: 1703096565.23974\n",
      "Training [40%] Loss: 0.0011817605434691126 time: 1703096565.23974\n",
      "weight: [ 1.27354629  1.68781271  0.31113468  0.57884128  0.34634922  1.27127195\n",
      "  1.24650208  0.86054528  1.79582688 -0.14910836 -0.32236334  0.33495185]\n",
      "epoch 203\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016485066728632873 R2: 0.9127149813973198 time: 1703096567.7977142\n",
      "batch_idx: 1 loss: 0.0008046312996082177 R2: 0.9127470181733643 time: 1703096570.4206343\n",
      "batch_idx: 2 loss: 0.0015168023508262821 R2: 0.912762192427954 time: 1703096573.0084388\n",
      "batch_idx: 3 loss: 0.0007532865418475498 R2: 0.9127544974985271 time: 1703096575.6745915\n",
      "Training [41%] Loss: 0.0011808067162863342 time: 1703096575.6745915\n",
      "weight: [ 1.27692444  1.68790922  0.31190849  0.57795903  0.34560933  1.27215161\n",
      "  1.24725751  0.86206928  1.80498766 -0.14907911 -0.32248176  0.33484638]\n",
      "epoch 204\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016455601524131668 R2: 0.9127844021349402 time: 1703096578.3290133\n",
      "batch_idx: 1 loss: 0.0008040995499274147 R2: 0.9128161983816211 time: 1703096580.9767175\n",
      "batch_idx: 2 loss: 0.0015162730859195552 R2: 0.9128310218514374 time: 1703096583.5218785\n",
      "batch_idx: 3 loss: 0.0007535484901849166 R2: 0.9128228419889078 time: 1703096586.0648286\n",
      "Training [41%] Loss: 0.0011798703196112632 time: 1703096586.0648286\n",
      "weight: [ 1.28023053  1.68799666  0.31266379  0.57708338  0.34487181  1.27301838\n",
      "  1.24799998  0.86359116  1.81411498 -0.14905    -0.32260098  0.33473996]\n",
      "epoch 205\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016426530087294965 R2: 0.9128525519028441 time: 1703096588.9206932\n",
      "batch_idx: 1 loss: 0.0008035523242632861 R2: 0.9128841039451826 time: 1703096591.6390908\n",
      "batch_idx: 2 loss: 0.0015157729912922114 R2: 0.91289857872608 time: 1703096594.4641519\n",
      "batch_idx: 3 loss: 0.0007538267217692574 R2: 0.9128899212987625 time: 1703096597.2059305\n",
      "Training [41%] Loss: 0.0011789512615135629 time: 1703096597.2059305\n",
      "weight: [ 1.28346479  1.68807563  0.31340139  0.57621382  0.34413621  1.27387221\n",
      "  1.24872922  0.86511036  1.82320464 -0.14902103 -0.32272095  0.33463265]\n",
      "epoch 206\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016397855680596346 R2: 0.9129194395673274 time: 1703096599.7629778\n",
      "batch_idx: 1 loss: 0.0008029903652939192 R2: 0.9129507441787945 time: 1703096602.4257238\n",
      "batch_idx: 2 loss: 0.0015153011951573323 R2: 0.9129648728584895 time: 1703096604.958064\n",
      "batch_idx: 3 loss: 0.0007541205559582539 R2: 0.9129557457689541 time: 1703096607.5330915\n",
      "Training [41%] Loss: 0.001178049421117285 time: 1703096607.5330915\n",
      "weight: [ 1.28662768  1.6881467   0.31412207  0.57534986  0.34340214  1.27471309\n",
      "  1.24944499  0.86662638  1.83225273 -0.14899218 -0.32284164  0.33452448]\n",
      "epoch 207\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016369580507742821 R2: 0.9129850758443881 time: 1703096610.121718\n",
      "batch_idx: 1 loss: 0.0008024144329175246 R2: 0.9130161302291718 time: 1703096612.7587001\n",
      "batch_idx: 2 loss: 0.0015148568088230651 R2: 0.9130299158278502 time: 1703096615.3942802\n",
      "batch_idx: 3 loss: 0.0007544293137183593 R2: 0.9130203274309553 time: 1703096617.9208224\n",
      "Training [41%] Loss: 0.0011771646515583078 time: 1703096617.9208224\n",
      "weight: [ 1.28971982  1.6882104   0.31482654  0.57449105  0.34266922  1.27554109\n",
      "  1.25014709  0.86813873  1.84125564 -0.14896345 -0.32296299  0.33441549]\n",
      "epoch 208\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016341705791675337 R2: 0.913049473085749 time: 1703096620.5578985\n",
      "batch_idx: 1 loss: 0.0008018252986699304 R2: 0.9130802748596396 time: 1703096623.0992718\n",
      "batch_idx: 2 loss: 0.001514438933731835 R2: 0.913093720772944 time: 1703096625.6734169\n",
      "batch_idx: 3 loss: 0.000754752320093079 R2: 0.9130836797967687 time: 1703096628.2459247\n",
      "Training [42%] Loss: 0.0011762967829155944 time: 1703096628.2459247\n",
      "weight: [ 1.292742    1.68826721  0.31551552  0.57363698  0.34193715  1.27635633\n",
      "  1.25083538  0.86964698  1.85021001 -0.14893483 -0.32308496  0.33430574]\n",
      "epoch 209\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016314231855089102 R2: 0.9131126450723597 time: 1703096630.8512974\n",
      "batch_idx: 1 loss: 0.0008012237405489498 R2: 0.9131431922426488 time: 1703096633.3790379\n",
      "batch_idx: 2 loss: 0.0015140466677257397 R2: 0.913156302187725 time: 1703096635.9412897\n",
      "batch_idx: 3 loss: 0.0007550889064808231 R2: 0.913145817658487 time: 1703096638.6189847\n",
      "Training [42%] Loss: 0.0011754456250661057 time: 1703096638.6189847\n",
      "weight: [ 1.29569514  1.6883176   0.31618964  0.57278726  0.34120564  1.27715897\n",
      "  1.25150973  0.8711507   1.85911275 -0.1489063  -0.32320752  0.33419525]\n",
      "epoch 210\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016287158201621008 R2: 0.9131746068179044 time: 1703096641.2247448\n",
      "batch_idx: 1 loss: 0.0008006105382816995 R2: 0.9132048977626885 time: 1703096643.9658496\n",
      "batch_idx: 2 loss: 0.0015136791105354731 R2: 0.9132176757278229 time: 1703096646.5595\n",
      "batch_idx: 3 loss: 0.00075543841271144 R2: 0.9132067568995963 time: 1703096649.1656842\n",
      "Training [42%] Loss: 0.0011746109704226784 time: 1703096649.1656842\n",
      "weight: [ 1.29858029  1.68836198  0.31684953  0.57194157  0.34047445  1.27794925\n",
      "  1.25217007  0.87264953  1.86796102 -0.14887788 -0.32333063  0.33408407]\n",
      "epoch 211\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016260483596131528 R2: 0.9132353743842808 time: 1703096651.7702327\n",
      "batch_idx: 1 loss: 0.0007999864690529952 R2: 0.9132654078314824 time: 1703096654.4328575\n",
      "batch_idx: 2 loss: 0.0015133353685146459 R2: 0.91327785802963 time: 1703096657.0988574\n",
      "batch_idx: 3 loss: 0.0007558001889130896 R2: 0.9132665143194517 time: 1703096659.710426\n",
      "Training [42%] Loss: 0.0011737925965234707 time: 1703096659.710426\n",
      "weight: [ 1.30139862  1.68840072  0.31749576  0.57109957  0.33974336  1.27872743\n",
      "  1.25281635  0.87414311  1.87675223 -0.14884954 -0.32345424  0.33397223]\n",
      "epoch 212\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016234206142794517 R2: 0.9132949647102796 time: 1703096662.4608047\n",
      "batch_idx: 1 loss: 0.000799352303698095 R2: 0.913324739716676 time: 1703096665.1102836\n",
      "batch_idx: 2 loss: 0.0015130145586597977 R2: 0.9133368665430673 time: 1703096667.7754507\n",
      "batch_idx: 3 loss: 0.0007561735971637723 R2: 0.9133251074718155 time: 1703096670.4136624\n",
      "Training [42%] Loss: 0.001172990268450279 time: 1703096670.4136624\n",
      "weight: [ 1.30415136  1.68843418  0.31812889  0.57026099  0.33901219  1.27949381\n",
      "  1.25344858  0.87563111  1.88548399 -0.14882129 -0.32357832  0.33385976]\n",
      "epoch 213\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016208323359986163 R2: 0.9133533954542736 time: 1703096672.9548976\n",
      "batch_idx: 1 loss: 0.0007987088033511742 R2: 0.913382911384758 time: 1703096675.5374858\n",
      "batch_idx: 2 loss: 0.001512715811968192 R2: 0.9133947193786301 time: 1703096678.0083134\n",
      "batch_idx: 3 loss: 0.0007565580129258803 R2: 0.9133825545178256 time: 1703096680.645222\n",
      "Training [43%] Loss: 0.0011722037410609655 time: 1703096680.645222\n",
      "weight: [ 1.30683986  1.68846267  0.31874942  0.56942558  0.33828078  1.28024873\n",
      "  1.25406676  0.87711326  1.89415417 -0.14879313 -0.32370284  0.3337467 ]\n",
      "epoch 214\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016182832251218828 R2: 0.9134106848511724 time: 1703096683.2664266\n",
      "batch_idx: 1 loss: 0.0007980567165323304 R2: 0.9134399413584345 time: 1703096686.0012512\n",
      "batch_idx: 2 loss: 0.0015124382761929653 R2: 0.9134514351688191 time: 1703096688.6638622\n",
      "batch_idx: 3 loss: 0.0007569528262656789 R2: 0.913438874093373 time: 1703096691.2707343\n",
      "Training [43%] Loss: 0.0011714327610282144 time: 1703096691.2707343\n",
      "weight: [ 1.30946547  1.68848649  0.31935783  0.56859309  0.33754901  1.28099254\n",
      "  1.25467096  0.87858928  1.90276081 -0.14876505 -0.32382775  0.33363308]\n",
      "epoch 215\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016157729371598713 R2: 0.9134668515835644 time: 1703096693.8733044\n",
      "batch_idx: 1 loss: 0.0007973967766497798 R2: 0.9134958485883482 time: 1703096696.4600484\n",
      "batch_idx: 2 loss: 0.0015121811180576164 R2: 0.9135070329437708 time: 1703096699.079903\n",
      "batch_idx: 3 loss: 0.0007573574428635557 R2: 0.9134940851905494 time: 1703096701.673403\n",
      "Training [43%] Loss: 0.0011706770686827057 time: 1703096701.673403\n",
      "weight: [ 1.31202964  1.6885059   0.31995458  0.56776334  0.33681678  1.28172563\n",
      "  1.25526126  0.88005893  1.91130216 -0.14873705 -0.32395303  0.33351892]\n",
      "epoch 216\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016133010889481873 R2: 0.913521914666647 time: 1703096704.2960858\n",
      "batch_idx: 1 loss: 0.0007967296998900823 R2: 0.9135506523386935 time: 1703096706.900151\n",
      "batch_idx: 2 loss: 0.0015119435249917707 R2: 0.9135615320206083 time: 1703096709.4876366\n",
      "batch_idx: 3 loss: 0.0007577712848243885 R2: 0.9135482070525713 time: 1703096712.1401725\n",
      "Training [43%] Loss: 0.0011699363996636072 time: 1703096712.1401725\n",
      "weight: [ 1.31453381  1.68852114  0.32054008  0.56693612  0.33608399  1.28244841\n",
      "  1.25583776  0.881522    1.91977665 -0.14870914 -0.32407864  0.33340427]\n",
      "epoch 217\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016108672643174132 R2: 0.9135758933463605 time: 1703096714.652177\n",
      "batch_idx: 1 loss: 0.0007960561834671868 R2: 0.9136043720861251 time: 1703096717.245017\n",
      "batch_idx: 2 loss: 0.0015117247064473734 R2: 0.9136149519057944 time: 1703096719.797589\n",
      "batch_idx: 3 loss: 0.0007581937913000451 R2: 0.9136012590814173 time: 1703096722.484066\n",
      "Training [43%] Loss: 0.0011692104863830048 time: 1703096722.484066\n",
      "weight: [ 1.31697948  1.68853243  0.32111473  0.56611129  0.33535058  1.2831613\n",
      "  1.25640059  0.88297828  1.92818289 -0.14868132 -0.32420454  0.33328914]\n",
      "epoch 218\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016084710192655685 R2: 0.9136288070098916 time: 1703096725.0719697\n",
      "batch_idx: 1 loss: 0.0007953769042002203 R2: 0.9136570274310936 time: 1703096727.6127148\n",
      "batch_idx: 2 loss: 0.0015115238948508437 R2: 0.9136673122097013 time: 1703096730.0607035\n",
      "batch_idx: 3 loss: 0.0007586244189385073 R2: 0.9136532607572588 time: 1703096732.212574\n",
      "Training [44%] Loss: 0.001168499059313785 time: 1703096732.212574\n",
      "weight: [ 1.31936813  1.68853997  0.32167891  0.56528869  0.33461651  1.28386472\n",
      "  1.2569499   0.88442761  1.93651964 -0.14865358 -0.3243307   0.33317357]\n",
      "epoch 219\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016061118866420871 R2: 0.9136806751077465 time: 1703096734.5026524\n",
      "batch_idx: 1 loss: 0.000794692517390089 R2: 0.9137086380207707 time: 1703096736.6594098\n",
      "batch_idx: 2 loss: 0.001511340346241776 R2: 0.913718632572426 time: 1703096738.8653798\n",
      "batch_idx: 3 loss: 0.000759062642175432 R2: 0.9137042315687877 time: 1703096741.006517\n",
      "Training [44%] Loss: 0.001167801848112346 time: 1703096741.006517\n",
      "weight: [ 1.32170128  1.68854392  0.32223296  0.56446819  0.33388174  1.28455911\n",
      "  1.25748585  0.88586983  1.94478582 -0.14862594 -0.32445708  0.33305759]\n",
      "epoch 220\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016037893803604766 R2: 0.9137315170863486 time: 1703096743.134225\n",
      "batch_idx: 1 loss: 0.0007940036559656423 R2: 0.9137592234825437 time: 1703096745.4102662\n",
      "batch_idx: 2 loss: 0.0015111733406442819 R2: 0.9137689325999409 time: 1703096747.6252046\n",
      "batch_idx: 3 loss: 0.0007595079533847322 R2: 0.9137541909534074 time: 1703096749.9191346\n",
      "Training [44%] Loss: 0.0011671185825887832 time: 1703096749.9191346\n",
      "weight: [ 1.32398041  1.68854446  0.3227772   0.56364969  0.33314626  1.2852449\n",
      "  1.25800863  0.88730479  1.9529805  -0.1485984  -0.32458364  0.33294123]\n",
      "epoch 221\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0016015029991630458 R2: 0.9137813523302837 time: 1703096752.1629348\n",
      "batch_idx: 1 loss: 0.0007933109298712213 R2: 0.9138088033671407 time: 1703096754.4594824\n",
      "batch_idx: 2 loss: 0.0015110221822119056 R2: 0.9138182318095511 time: 1703096756.6448467\n",
      "batch_idx: 3 loss: 0.0007599598629050347 R2: 0.9138031582463386 time: 1703096758.7542539\n",
      "Training [44%] Loss: 0.0011664489935378018 time: 1703096758.7542539\n",
      "weight: [ 1.32620702  1.68854174  0.32331195  0.56283308  0.33241004  1.28592253\n",
      "  1.25851844  0.88873238  1.96110286 -0.14857097 -0.32471036  0.33282451]\n",
      "epoch 222\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015992522299650431 R2: 0.9138302001131822 time: 1703096761.3464365\n",
      "batch_idx: 1 loss: 0.0007926149256687221 R2: 0.9138573971003913 time: 1703096763.6812828\n",
      "batch_idx: 2 loss: 0.0015108861991828823 R2: 0.9138665495837573 time: 1703096765.9800072\n",
      "batch_idx: 3 loss: 0.00076041789895838 R2: 0.91385115263769 time: 1703096768.1386218\n",
      "Training [44%] Loss: 0.001165792813443757 time: 1703096768.1386218\n",
      "weight: [ 1.32838258  1.68853588  0.3238375   0.56201827  0.3316731   1.28659244\n",
      "  1.25901549  0.8901525   1.96915222 -0.14854364 -0.3248372   0.33270747]\n",
      "epoch 223\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015970365508081227 R2: 0.9138780795563489 time: 1703096770.3273413\n",
      "batch_idx: 1 loss: 0.000791916206328887 R2: 0.9139050239427119 time: 1703096772.4576266\n",
      "batch_idx: 2 loss: 0.001510764743677925 R2: 0.9139139051315522 time: 1703096774.6072915\n",
      "batch_idx: 3 loss: 0.0007608816074768096 R2: 0.9138981931365764 time: 1703096776.7958736\n",
      "Training [45%] Loss: 0.001165149777072936 time: 1703096776.7958736\n",
      "weight: [ 1.33050854  1.688527    0.32435411  0.56120519  0.33093544  1.28725505\n",
      "  1.2595      0.89156503  1.97712802 -0.14851642 -0.32496412  0.33259014]\n",
      "epoch 224\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.00159485543345429 R2: 0.9139250095942281 time: 1703096778.968757\n",
      "batch_idx: 1 loss: 0.0007912153111879329 R2: 0.9139517029554236 time: 1703096781.301482\n",
      "batch_idx: 2 loss: 0.0015106571913692302 R2: 0.9139603174563385 time: 1703096783.441635\n",
      "batch_idx: 3 loss: 0.0007613505518514112 R2: 0.9139442985414806 time: 1703096785.6375115\n",
      "Training [45%] Loss: 0.001164519621965716 time: 1703096785.6375115\n",
      "weight: [ 1.33258634  1.68851522  0.32486204  0.56039378  0.33019708  1.28791079\n",
      "  1.25997221  0.89296991  1.9850298  -0.14848932 -0.32509108  0.33247255]\n",
      "epoch 225\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015927083456518485 R2: 0.9139710089459131 time: 1703096787.7855337\n",
      "batch_idx: 1 loss: 0.00079051275604737 R2: 0.913997452973051 time: 1703096789.9362104\n",
      "batch_idx: 2 loss: 0.0015105629410455603 R2: 0.9140058053295961 time: 1703096792.1037369\n",
      "batch_idx: 3 loss: 0.000761824312617222 R2: 0.9139894874160127 time: 1703096794.2750757\n",
      "Training [45%] Loss: 0.0011639020888405001 time: 1703096794.2750757\n",
      "weight: [ 1.33461737  1.68850064  0.32536155  0.55958397  0.32945803  1.28856004\n",
      "  1.26043235  0.89436705  1.99285718 -0.14846235 -0.32521804  0.33235473]\n",
      "epoch 226\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015905947531040406 R2: 0.9140160960919225 time: 1703096796.4652314\n",
      "batch_idx: 1 loss: 0.0007898090333966345 R2: 0.9140422925808893 time: 1703096798.6948538\n",
      "batch_idx: 2 loss: 0.0015104814140952474 R2: 0.9140503872696316 time: 1703096800.8778121\n",
      "batch_idx: 3 loss: 0.0007623024870859819 R2: 0.9140337780694322 time: 1703096803.0105367\n",
      "Training [45%] Loss: 0.001163296921920476 time: 1703096803.0105367\n",
      "weight: [ 1.33660302  1.68848334  0.32585285  0.5587757   0.32871833  1.28920322\n",
      "  1.26088068  0.89575641  2.00060991 -0.1484355  -0.32534497  0.33223672]\n",
      "epoch 227\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001588514121170136 R2: 0.9140602892555642 time: 1703096805.1438859\n",
      "batch_idx: 1 loss: 0.0007891046127396096 R2: 0.9140862400970933 time: 1703096807.3261778\n",
      "batch_idx: 2 loss: 0.0015104120539259902 R2: 0.9140940815246615 time: 1703096809.455953\n",
      "batch_idx: 3 loss: 0.0007627846889375552 R2: 0.9140771885412408 time: 1703096811.632868\n",
      "Training [45%] Loss: 0.0011627038691933227 time: 1703096811.632868\n",
      "weight: [ 1.33854463  1.68846341  0.32633616  0.55796895  0.32797799  1.2898407\n",
      "  1.26131744  0.89713793  2.0082878  -0.14840879 -0.32547184  0.33211856]\n",
      "epoch 228\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015864659163272628 R2: 0.9141036063882539 time: 1703096813.8668082\n",
      "batch_idx: 1 loss: 0.0007883999410080644 R2: 0.9141293135587036 time: 1703096816.0502539\n",
      "batch_idx: 2 loss: 0.0015103543253376709 R2: 0.9141369060597017 time: 1703096818.1844385\n",
      "batch_idx: 3 loss: 0.0007632705477794105 R2: 0.9141197365892847 time: 1703096820.3341136\n",
      "Training [46%] Loss: 0.0011621226826131021 time: 1703096820.3341136\n",
      "weight: [ 1.34044351  1.68844094  0.32681169  0.55716366  0.32723707  1.29047284\n",
      "  1.26174289  0.89851157  2.01589073 -0.14838222 -0.32559859  0.33200028]\n",
      "epoch 229\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015844496074191785 R2: 0.9141460651582414 time: 1703096822.5311804\n",
      "batch_idx: 1 loss: 0.0007876954430466274 R2: 0.9141715307110255 time: 1703096824.6661205\n",
      "batch_idx: 2 loss: 0.0015103077138619273 R2: 0.9141788785466453 time: 1703096826.8638668\n",
      "batch_idx: 3 loss: 0.0007637597086825751 R2: 0.9141614396808615 time: 1703096829.1112325\n",
      "Training [46%] Loss: 0.001161553118252577 time: 1703096829.1112325\n",
      "weight: [ 1.34230094  1.68841598  0.32727963  0.55635981  0.32649557  1.29109998\n",
      "  1.26215731  0.89987729  2.02341869 -0.1483558  -0.32572519  0.33188192]\n",
      "epoch 230\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015824646667164446 R2: 0.9141876829422285 time: 1703096831.2969792\n",
      "batch_idx: 1 loss: 0.0007869915221555562 R2: 0.9142129089998845 time: 1703096833.439865\n",
      "batch_idx: 2 loss: 0.001510271725080191 R2: 0.9142200163571059 time: 1703096835.6121926\n",
      "batch_idx: 3 loss: 0.0007642518317011027 R2: 0.9142023149863693 time: 1703096837.8629804\n",
      "Training [46%] Loss: 0.0011609949364133237 time: 1703096837.8629804\n",
      "weight: [ 1.34411815  1.6883886   0.32774018  0.55555736  0.32575356  1.29172246\n",
      "  1.26256094  0.90123507  2.0308717  -0.14832953 -0.32585161  0.33176352]\n",
      "epoch 231\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015805105708103396 R2: 0.9142284768194726 time: 1703096840.0132382\n",
      "batch_idx: 1 loss: 0.0007862885606790424 R2: 0.9142534655662884 time: 1703096842.1931975\n",
      "batch_idx: 2 loss: 0.0015102458839298199 R2: 0.9142603365575811 time: 1703096844.313949\n",
      "batch_idx: 3 loss: 0.0007647465913813389 R2: 0.914242379375119 time: 1703096846.5693924\n",
      "Training [46%] Loss: 0.0011604479017001351 time: 1703096846.5693924\n",
      "weight: [ 1.34589637  1.68835887  0.3281935   0.55475629  0.32501105  1.29234057\n",
      "  1.26295407  0.90258488  2.03824984 -0.14830341 -0.32597779  0.33164512]\n",
      "epoch 232\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015785868013608567 R2: 0.9142684635679439 time: 1703096848.7228608\n",
      "batch_idx: 1 loss: 0.0007855869206285371 R2: 0.9142932172431559 time: 1703096850.8783453\n",
      "batch_idx: 2 loss: 0.0015102297340063833 R2: 0.9142998559065629 time: 1703096853.0617855\n",
      "batch_idx: 3 loss: 0.0007652436762662012 R2: 0.9142816494129337 time: 1703096855.1962695\n",
      "Training [46%] Loss: 0.0011599117830654946 time: 1703096855.1962695\n",
      "weight: [ 1.34763674  1.68832684  0.32863976  0.55395657  0.32426809  1.29295461\n",
      "  1.26333694  0.90392672  2.04555329 -0.14827746 -0.3261037   0.33152677]\n",
      "epoch 233\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015766928457170038 R2: 0.9143076596622659 time: 1703096857.3854795\n",
      "batch_idx: 1 loss: 0.0007848869443316207 R2: 0.9143321805537429 time: 1703096859.5202606\n",
      "batch_idx: 2 loss: 0.001510222836868407 R2: 0.9143385908532953 time: 1703096861.7794874\n",
      "batch_idx: 3 loss: 0.0007657427883989317 R2: 0.9143201413612815 time: 1703096863.9077508\n",
      "Training [47%] Loss: 0.0011593863538289907 time: 1703096863.9077508\n",
      "weight: [ 1.34934043  1.68829256  0.32907914  0.55315819  0.32352471  1.29356483\n",
      "  1.26370985  0.90526056  2.05278222 -0.14825167 -0.32622929  0.33140851]\n",
      "epoch 234\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015748281974259405 R2: 0.9143460812730855 time: 1703096866.0522351\n",
      "batch_idx: 1 loss: 0.0007841889550983916 R2: 0.9143703717114862 time: 1703096868.2289755\n",
      "batch_idx: 2 loss: 0.0015102247713500382 R2: 0.9143765575378696 time: 1703096870.3786361\n",
      "batch_idx: 3 loss: 0.0007662436428300061 R2: 0.9143578711776443 time: 1703096872.5618064\n",
      "Training [47%] Loss: 0.0011588713916760942 time: 1703096872.5618064\n",
      "weight: [ 1.35100852  1.68825609  0.32951177  0.55236112  0.32278095  1.2941715\n",
      "  1.26407304  0.90658642  2.0599369  -0.14822606 -0.32635453  0.33129039]\n",
      "epoch 235\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.00157299235664551 R2: 0.9143837442676468 time: 1703096874.6893435\n",
      "batch_idx: 1 loss: 0.0007834932578985472 R2: 0.914407806621028 time: 1703096876.9070308\n",
      "batch_idx: 2 loss: 0.0015102351328854194 R2: 0.9144137717924595 time: 1703096879.0791266\n",
      "batch_idx: 3 loss: 0.0007667459671303524 R2: 0.914394854516922 time: 1703096881.2208166\n",
      "Training [47%] Loss: 0.0011583666786399572 time: 1703096881.2208166\n",
      "weight: [ 1.35264207  1.68821747  0.32993782  0.55156536  0.32203685  1.29477483\n",
      "  1.26442679  0.90790428  2.06701762 -0.14820062 -0.32647937  0.33117246]\n",
      "epoch 236\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001571184830473174 R2: 0.9144206642113822 time: 1703096883.3874745\n",
      "batch_idx: 1 loss: 0.0007828001400430962 R2: 0.9144445008801799 time: 1703096885.5223987\n",
      "batch_idx: 2 loss: 0.0015102535328481528 R2: 0.9144502491434473 time: 1703096887.705277\n",
      "batch_idx: 3 loss: 0.0007672495009132175 R2: 0.91443110673366 time: 1703096889.8528438\n",
      "Training [47%] Loss: 0.00115787200106941 time: 1703096889.8528438\n",
      "weight: [ 1.35424213  1.68817674  0.33035741  0.55077088  0.32129244  1.29537503\n",
      "  1.26477137  0.90921416  2.07402469 -0.14817536 -0.32660376  0.33105478]\n",
      "epoch 237\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015694051332028462 R2: 0.9144568563702851 time: 1703096892.019096\n",
      "batch_idx: 1 loss: 0.0007821098718658126 R2: 0.9144804697827039 time: 1703096894.2312021\n",
      "batch_idx: 2 loss: 0.0015102795979081633 R2: 0.9144860048143013 time: 1703096896.3715413\n",
      "batch_idx: 3 loss: 0.0007677539953667782 R2: 0.9144666428849886 time: 1703096898.532644\n",
      "Training [47%] Loss: 0.0011573871495859 time: 1703096898.532644\n",
      "weight: [ 1.35580969  1.68813396  0.3307707   0.54997767  0.32054777  1.2959723\n",
      "  1.26510703  0.91051606  2.0809585  -0.14815028 -0.32672767  0.33093739]\n",
      "epoch 238\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015676527865196507 R2: 0.9144923357139451 time: 1703096900.6655226\n",
      "batch_idx: 1 loss: 0.0007814227074001793 R2: 0.9145157283216687 time: 1703096902.847083\n",
      "batch_idx: 2 loss: 0.0015103129694076802 R2: 0.9145210537290384 time: 1703096904.9813728\n",
      "batch_idx: 3 loss: 0.0007682592127989737 R2: 0.9145014777340637 time: 1703096907.164062\n",
      "Training [48%] Loss: 0.001156911919031621 time: 1703096907.164062\n",
      "weight: [ 1.35734572  1.68808915  0.3311778   0.54918573  0.31980285  1.2965668\n",
      "  1.26543405  0.91180999  2.08781944 -0.14812539 -0.32685104  0.33082035]\n",
      "epoch 239\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015659273196415135 R2: 0.9145271169191019 time: 1703096909.3755689\n",
      "batch_idx: 1 loss: 0.0007807388850483073 R2: 0.9145502911933173 time: 1703096911.5101488\n",
      "batch_idx: 2 loss: 0.0015103533027576443 R2: 0.9145554105161582 time: 1703096913.6904404\n",
      "batch_idx: 3 loss: 0.0007687649261956384 R2: 0.9145356257539655 time: 1703096915.8243215\n",
      "Training [48%] Loss: 0.001156446108410776 time: 1703096915.8243215\n",
      "weight: [ 1.35885114  1.68804236  0.33157885  0.54839503  0.31905773  1.29715869\n",
      "  1.26575267  0.91309596  2.09460793 -0.14810069 -0.32697384  0.33070372]\n",
      "epoch 240\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015642282694152874 R2: 0.9145612143735905 time: 1703096917.9905531\n",
      "batch_idx: 1 loss: 0.0007800586282388998 R2: 0.9145841728013359 time: 1703096920.1395452\n",
      "batch_idx: 2 loss: 0.0015104002668553025 R2: 0.9145890895129304 time: 1703096922.3237228\n",
      "batch_idx: 3 loss: 0.000769270918792827 R2: 0.9145691011319125 time: 1703096924.4536815\n",
      "Training [48%] Loss: 0.0011559895208255791 time: 1703096924.4536815\n",
      "weight: [ 1.36032686  1.68799363  0.33197397  0.54760557  0.31831243  1.29774812\n",
      "  1.26606315  0.914374    2.10132443 -0.14807618 -0.32709601  0.33058757]\n",
      "epoch 241\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015625551803741089 R2: 0.9145946421806302 time: 1703096926.6636078\n",
      "batch_idx: 1 loss: 0.0007793821460720049 R2: 0.9146173872613732 time: 1703096928.8500974\n",
      "batch_idx: 2 loss: 0.0015104535435234538 R2: 0.9146221047699659 time: 1703096930.9742613\n",
      "batch_idx: 3 loss: 0.0007697769836637913 R2: 0.9146019177737242 time: 1703096933.146476\n",
      "Training [48%] Loss: 0.0011555419634083396 time: 1703096933.146476\n",
      "weight: [ 1.36177374  1.687943    0.33236328  0.54681734  0.31756699  1.2983352\n",
      "  1.26636576  0.91564412  2.10796943 -0.14805186 -0.32721752  0.33047194]\n",
      "epoch 242\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015609076047618524 R2: 0.9146274141633196 time: 1703096935.276158\n",
      "batch_idx: 1 loss: 0.0007787096339484988 R2: 0.9146499484058112 time: 1703096937.448963\n",
      "batch_idx: 2 loss: 0.0015105128269715417 R2: 0.9146544700559691 time: 1703096939.5840297\n",
      "batch_idx: 3 loss: 0.0007702829233208338 R2: 0.9146340893084701 time: 1703096941.79628\n",
      "Training [48%] Loss: 0.0011551032472506817 time: 1703096941.79628\n",
      "weight: [ 1.36319264  1.68789049  0.33274688  0.54603032  0.31682143  1.29892006\n",
      "  1.26666073  0.91690635  2.11454341 -0.14802775 -0.32733831  0.33035692]\n",
      "epoch 243\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015592851025298747 R2: 0.9146595438693137 time: 1703096943.9762878\n",
      "batch_idx: 1 loss: 0.0007780412741828735 R2: 0.9146818697886705 time: 1703096946.102039\n",
      "batch_idx: 2 loss: 0.0015105778232785728 R2: 0.9146861988626588 time: 1703096948.292837\n",
      "batch_idx: 3 loss: 0.0007707885493322414 R2: 0.9146656290932438 time: 1703096950.4304302\n",
      "Training [49%] Loss: 0.0011546731873308905 time: 1703096950.4304302\n",
      "weight: [ 1.36458435  1.68783614  0.3331249   0.54524451  0.31607579  1.29950279\n",
      "  1.26694831  0.91816071  2.12104689 -0.14800383 -0.32745835  0.33024256]\n",
      "epoch 244\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015576872413102232 R2: 0.9146910445756203 time: 1703096952.6087294\n",
      "batch_idx: 1 loss: 0.0007773772365982761 R2: 0.9147131646906045 time: 1703096954.7274992\n",
      "batch_idx: 2 loss: 0.0015106482498975927 R2: 0.9147173044097577 time: 1703096956.9840245\n",
      "batch_idx: 3 loss: 0.0007712936819540915 R2: 0.9146965502180106 time: 1703096959.1689606\n",
      "Training [49%] Loss: 0.0011542516024400458 time: 1703096959.1689606\n",
      "weight: [ 1.36594966  1.68778     0.33349744  0.54445989  0.31533009  1.3000835\n",
      "  1.26722875  0.91940723  2.12748041 -0.14798011 -0.32757758  0.33012894]\n",
      "epoch 245\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015561135963692552 R2: 0.9147219292934649 time: 1703096961.2885985\n",
      "batch_idx: 1 loss: 0.0007767176791027206 R2: 0.9147438461239827 time: 1703096963.453407\n",
      "batch_idx: 2 loss: 0.0015107238351814533 R2: 0.9147477996500444 time: 1703096965.5873897\n",
      "batch_idx: 3 loss: 0.0007717981497767582 R2: 0.9147268655105106 time: 1703096967.7676556\n",
      "Training [49%] Loss: 0.0011538383151075467 time: 1703096967.7676556\n",
      "weight: [ 1.36728933  1.68772208  0.33386459  0.54367646  0.31458435  1.30066227\n",
      "  1.26750228  0.92064592  2.13384453 -0.1479566  -0.32769596  0.33001613]\n",
      "epoch 246\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001554563750544955 R2: 0.9147522107731924 time: 1703096969.9021184\n",
      "batch_idx: 1 loss: 0.00077606274824608 R2: 0.9147739268379619 time: 1703096972.0346627\n",
      "batch_idx: 2 loss: 0.001510804317929307 R2: 0.9147776972744122 time: 1703096974.3267922\n",
      "batch_idx: 3 loss: 0.0007723017893858889 R2: 0.914756587541169 time: 1703096976.4759188\n",
      "Training [49%] Loss: 0.0011534331515265577 time: 1703096976.4759188\n",
      "weight: [ 1.36860409  1.68766243  0.33422647  0.5428942   0.3138386   1.30123917\n",
      "  1.26776914  0.92187683  2.1401398  -0.14793329 -0.32781344  0.32990419]\n",
      "epoch 247\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015530372941706494 R2: 0.9147819015091725 time: 1703096978.6621673\n",
      "batch_idx: 1 loss: 0.0007754125797574756 R2: 0.9148034193235807 time: 1703096980.814012\n",
      "batch_idx: 2 loss: 0.001510889446953363 R2: 0.9148070097169333 time: 1703096982.9853005\n",
      "batch_idx: 3 loss: 0.0007728044450373166 R2: 0.9147857286279842 time: 1703096985.136461\n",
      "Training [49%] Loss: 0.001153035941479701 time: 1703096985.136461\n",
      "weight: [ 1.36989465  1.68760108  0.33458317  0.5421131   0.31309286  1.30181428\n",
      "  1.26802955  0.92309999  2.1463668  -0.14791019 -0.32792998  0.32979322]\n",
      "epoch 248\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015515338249876653 R2: 0.9148110137447046 time: 1703096987.3072085\n",
      "batch_idx: 1 loss: 0.0007747672990627708 R2: 0.9148323358188011 time: 1703096989.628762\n",
      "batch_idx: 2 loss: 0.0015109789806652552 R2: 0.9148357491598788 time: 1703096991.7775452\n",
      "batch_idx: 3 loss: 0.0007733059683456328 R2: 0.9148143008413994 time: 1703096993.9470303\n",
      "Training [50%] Loss: 0.001152646518265331 time: 1703096993.9470303\n",
      "weight: [ 1.37116168  1.68753805  0.33493479  0.54133316  0.31234715  1.30238768\n",
      "  1.26828374  0.92431542  2.15252612 -0.14788729 -0.32804554  0.32968327]\n",
      "epoch 249\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015500529480490187 R2: 0.9148395594768697 time: 1703096996.0838785\n",
      "batch_idx: 1 loss: 0.0007741270217823011 R2: 0.9148606883135327 time: 1703096998.2581046\n",
      "batch_idx: 2 loss: 0.0015110726866813847 R2: 0.9148639275387053 time: 1703097000.555865\n",
      "batch_idx: 3 loss: 0.0007738062179858355 R2: 0.9148423160091198 time: 1703097002.7004766\n",
      "Training [50%] Loss: 0.001152264718624635 time: 1703097002.7004766\n",
      "weight: [ 1.37240583  1.68747338  0.33528142  0.54055435  0.3116015   1.30295942\n",
      "  1.26853193  0.92552316  2.15861834 -0.1478646  -0.32816007  0.32957444]\n",
      "epoch 250\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015485942756158465 R2: 0.9148675504613472 time: 1703097004.9762998\n",
      "batch_idx: 1 loss: 0.0007734918542088749 R2: 0.9148884885545716 time: 1703097007.1268282\n",
      "batch_idx: 2 loss: 0.001511170341446529 R2: 0.9148915565469672 time: 1703097009.3565893\n",
      "batch_idx: 3 loss: 0.0007743050594076866 R2: 0.914869785720869 time: 1703097011.4930663\n",
      "Training [50%] Loss: 0.0011518903826697343 time: 1703097011.4930663\n",
      "weight: [ 1.37362775  1.68740711  0.33562315  0.53977668  0.31085592  1.30352957\n",
      "  1.26877434  0.92672325  2.16464408 -0.14784212 -0.32827352  0.32946681]\n",
      "epoch 251\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015471574270483003 R2: 0.9148949982171514 time: 1703097013.6839166\n",
      "batch_idx: 1 loss: 0.0007728618937663567 R2: 0.9149157480504849 time: 1703097015.823557\n",
      "batch_idx: 2 loss: 0.0015112717298750593 R2: 0.9149186476411646 time: 1703097017.9525256\n",
      "batch_idx: 3 loss: 0.0007748023645621478 R2: 0.9148967213330884 time: 1703097020.245785\n",
      "Training [50%] Loss: 0.001151523353812966 time: 1703097020.245785\n",
      "weight: [ 1.37482805  1.68733926  0.33596006  0.53900011  0.31011043  1.3040982\n",
      "  1.26901118  0.92791572  2.17060394 -0.14781984 -0.32838586  0.32936046]\n",
      "epoch 252\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015457420286920797 R2: 0.9149219140313299 time: 1703097022.473184\n",
      "batch_idx: 1 loss: 0.0007722372294490949 R2: 0.9149424780764204 time: 1703097024.6378577\n",
      "batch_idx: 2 loss: 0.0015113766450089994 R2: 0.9149452120455189 time: 1703097026.7889073\n",
      "batch_idx: 3 loss: 0.0007752980116395577 R2: 0.9149231339735324 time: 1703097028.9700947\n",
      "Training [50%] Loss: 0.001151163478697433 time: 1703097028.9700947\n",
      "weight: [ 1.37600732  1.68726986  0.33629225  0.53822465  0.30936505  1.30466536\n",
      "  1.26924266  0.9291006   2.17649853 -0.14779778 -0.32849704  0.32925547]\n",
      "epoch 253\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015443477137619028 R2: 0.9149483089635441 time: 1703097031.1264374\n",
      "batch_idx: 1 loss: 0.0007716179422426852 R2: 0.9149686896788062 time: 1703097033.2561002\n",
      "batch_idx: 2 loss: 0.0015114848876922413 R2: 0.914971260756649 time: 1703097035.4350238\n",
      "batch_idx: 3 loss: 0.0007757918848189807 R2: 0.914949034545826 time: 1703097037.7022526\n",
      "Training [51%] Loss: 0.0011508106071289525 time: 1703097037.7022526\n",
      "weight: [ 1.37716613  1.68719894  0.3366198   0.53745028  0.30861979  1.30523112\n",
      "  1.26946897  0.93027794  2.18232847 -0.14777592 -0.32860701  0.32915193]\n",
      "epoch 254\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001542974122222839 R2: 0.9149741938505972 time: 1703097039.8774502\n",
      "batch_idx: 1 loss: 0.0007710041055264633 R2: 0.914994393679995 time: 1703097042.0114594\n",
      "batch_idx: 2 loss: 0.0015115962662602123 R2: 0.9149968045481621 time: 1703097044.1918912\n",
      "batch_idx: 3 loss: 0.0007762838740282642 R2: 0.914974433733876 time: 1703097046.3439913\n",
      "Training [51%] Loss: 0.0011504645920094446 time: 1703097046.3439913\n",
      "weight: [ 1.37830502  1.68712654  0.33694278  0.53667698  0.30787468  1.30579554\n",
      "  1.26969031  0.93144777  2.18809438 -0.14775427 -0.32871574  0.32904993]\n",
      "epoch 255\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001541620900670397 R2: 0.9149995793108732 time: 1703097048.477306\n",
      "batch_idx: 1 loss: 0.0007703957854583233 R2: 0.9150196006827841 time: 1703097050.6738338\n",
      "batch_idx: 2 loss: 0.001511710596244249 R2: 0.9150218539751622 time: 1703097052.8700154\n",
      "batch_idx: 3 loss: 0.0007767738747144524 R2: 0.9149993420062735 time: 1703097055.0669808\n",
      "Training [51%] Loss: 0.0011501252892718555 time: 1703097055.0669808\n",
      "weight: [ 1.37942454  1.68705268  0.33726129  0.53590475  0.30712972  1.30635867\n",
      "  1.26990689  0.93261013  2.19379688 -0.14773282 -0.32882319  0.32894955]\n",
      "epoch 256\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015402877022100583 R2: 0.9150244757486613 time: 1703097057.4575245\n",
      "batch_idx: 1 loss: 0.0007697930413423739 R2: 0.9150443210748593 time: 1703097059.671933\n",
      "batch_idx: 2 loss: 0.0015118277000900173 R2: 0.915046419378642 time: 1703097061.8068874\n",
      "batch_idx: 3 loss: 0.0007772617876239933 R2: 0.9150237696205185 time: 1703097063.98137\n",
      "Training [51%] Loss: 0.0011497925578166108 time: 1703097063.98137\n",
      "weight: [ 1.3805252   1.68697739  0.3375754   0.53513356  0.30638492  1.30692058\n",
      "  1.27011888  0.93376506  2.1994366  -0.14771158 -0.32892932  0.3288509 ]\n",
      "epoch 257\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015389741863370081 R2: 0.9150488933584174 time: 1703097066.213279\n",
      "batch_idx: 1 loss: 0.0007691959259799661 R2: 0.9150685650331228 time: 1703097068.3429134\n",
      "batch_idx: 2 loss: 0.0015119474068892983 R2: 0.9150705108897993 time: 1703097070.6454022\n",
      "batch_idx: 3 loss: 0.0007777475185924504 R2: 0.9150477266272267 time: 1703097072.7765636\n",
      "Training [51%] Loss: 0.0011494662594496807 time: 1703097072.7765636\n",
      "weight: [ 1.38160749  1.6869007   0.33788518  0.5343634   0.30564031  1.30748133\n",
      "  1.27032647  0.9349126   2.20501417 -0.14769055 -0.32903408  0.32875406]\n",
      "epoch 258\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001537680018816434 R2: 0.9150728421289214 time: 1703097074.9496768\n",
      "batch_idx: 1 loss: 0.0007686044860048425 R2: 0.9150923425279369 time: 1703097077.0879295\n",
      "batch_idx: 2 loss: 0.0015120695521243952 R2: 0.915094138434229 time: 1703097079.2237298\n",
      "batch_idx: 3 loss: 0.0007782309783432868 R2: 0.9150712228741901 time: 1703097081.4282985\n",
      "Training [52%] Loss: 0.0011491462588222397 time: 1703097081.4282985\n",
      "weight: [ 1.3826719   1.68682265  0.33819071  0.53359427  0.30489588  1.30804098\n",
      "  1.27052984  0.9360528   2.2105302  -0.14766972 -0.32913745  0.32865912]\n",
      "epoch 259\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015364048715649243 R2: 0.9150963318473314 time: 1703097083.5654202\n",
      "batch_idx: 1 loss: 0.0007680187622028144 R2: 0.9151156633272708 time: 1703097085.8315036\n",
      "batch_idx: 2 loss: 0.0015121939774246935 R2: 0.9151173117360537 time: 1703097087.9676569\n",
      "batch_idx: 3 loss: 0.0007787120822952712 R2: 0.9150942680103926 time: 1703097090.1582563\n",
      "Training [52%] Loss: 0.0011488324233719259 time: 1703097090.1582563\n",
      "weight: [ 1.38371888  1.68674325  0.33849206  0.53282614  0.30415166  1.30859958\n",
      "  1.27072916  0.93718568  2.21598533 -0.1476491  -0.32923939  0.32856617]\n",
      "epoch 260\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015351484225333638 R2: 0.9151193721031691 time: 1703097092.284224\n",
      "batch_idx: 1 loss: 0.0007674387898167201 R2: 0.9151385370007231 time: 1703097094.421687\n",
      "batch_idx: 2 loss: 0.0015123205303345208 R2: 0.9151400403219062 time: 1703097096.6185553\n",
      "batch_idx: 3 loss: 0.0007791907503782651 R2: 0.9151168714898873 time: 1703097098.7550523\n",
      "Training [52%] Loss: 0.0011485246232657174 time: 1703097098.7550523\n",
      "weight: [ 1.3847489   1.68666254  0.33878931  0.53205899  0.30340765  1.30915719\n",
      "  1.27092461  0.93831131  2.22138018 -0.14762868 -0.32933986  0.32847532]\n",
      "epoch 261\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015339103555915192 R2: 0.9151419722921886 time: 1703097101.0546117\n",
      "batch_idx: 1 loss: 0.0007668645988371916 R2: 0.9151609729234785 time: 1703097103.1875067\n",
      "batch_idx: 2 loss: 0.0015124490640918695 R2: 0.9151623335248743 time: 1703097105.3599086\n",
      "batch_idx: 3 loss: 0.0007796669068569279 R2: 0.9151390425756306 time: 1703097107.492682\n",
      "Training [52%] Loss: 0.0011482227313443772 time: 1703097107.492682\n",
      "weight: [ 1.38576237  1.68658055  0.33908253  0.53129282  0.30266386  1.30971387\n",
      "  1.27111634  0.93942971  2.22671538 -0.14760846 -0.32943883  0.32838664]\n",
      "epoch 262\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015326903604145827 R2: 0.9151641416201658 time: 1703097109.6245813\n",
      "batch_idx: 1 loss: 0.0007662962142798389 R2: 0.9151829802801451 time: 1703097111.8008375\n",
      "batch_idx: 2 loss: 0.0015125794374173558 R2: 0.9151842004882974 time: 1703097113.923925\n",
      "batch_idx: 3 loss: 0.000780140480162017 R2: 0.9151607903431798 time: 1703097116.1064727\n",
      "Training [52%] Loss: 0.0011479266230684487 time: 1703097116.1064727\n",
      "weight: [ 1.38675973  1.68649731  0.33937177  0.53052761  0.30192029  1.31026968\n",
      "  1.27130453  0.94054093  2.23199154 -0.14758844 -0.32953627  0.32830024]\n",
      "epoch 263\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001531488132371884 R2: 0.9151858891066006 time: 1703097118.3627958\n",
      "batch_idx: 1 loss: 0.0007657336564494253 R2: 0.9152045680685127 time: 1703097120.5440688\n",
      "batch_idx: 2 loss: 0.00151271151431282 R2: 0.9152056501695057 time: 1703097122.6932387\n",
      "batch_idx: 3 loss: 0.0007806114027289198 R2: 0.9151821236843543 time: 1703097124.8364847\n",
      "Training [53%] Loss: 0.0011476361764657624 time: 1703097124.8364847\n",
      "weight: [ 1.38774138  1.68641284  0.33965712  0.52976334  0.30117697  1.31082466\n",
      "  1.27148932  0.94164501  2.23720928 -0.14756863 -0.32963216  0.32821621]\n",
      "epoch 264\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001530303372417745 R2: 0.91520722358833 time: 1703097126.9989989\n",
      "batch_idx: 1 loss: 0.0007651769411916098 R2: 0.915225745103205 time: 1703097129.1403763\n",
      "batch_idx: 2 loss: 0.0015128451638691174 R2: 0.915226691343457 time: 1703097131.301621\n",
      "batch_idx: 3 loss: 0.000781079610843103 R2: 0.9152030513107687 time: 1703097133.5172255\n",
      "Training [53%] Loss: 0.0011473512720803938 time: 1703097133.5172255\n",
      "weight: [ 1.38870771  1.68632717  0.33993863  0.52899999  0.30043388  1.31137888\n",
      "  1.27167087  0.942742    2.24236922 -0.14754901 -0.32972647  0.32813463]\n",
      "epoch 265\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001529135786984733 R2: 0.9152281537230478 time: 1703097135.6917012\n",
      "batch_idx: 1 loss: 0.0007646260801327244 R2: 0.9152465200192538 time: 1703097137.8065104\n",
      "batch_idx: 2 loss: 0.0015129802600825498 R2: 0.9152473326062764 time: 1703097139.9562726\n",
      "batch_idx: 3 loss: 0.0007815450444920412 R2: 0.9152235817573088 time: 1703097142.1250453\n",
      "Training [53%] Loss: 0.0011470717929230121 time: 1703097142.1250453\n",
      "weight: [ 1.38965909  1.68624033  0.34021638  0.52823755  0.29969104  1.31193238\n",
      "  1.27184933  0.94383194  2.24747198 -0.14752959 -0.32981916  0.32805561]\n",
      "epoch 266\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015279850878791056 R2: 0.9152486879927496 time: 1703097144.2452056\n",
      "batch_idx: 1 loss: 0.0007640810809082202 R2: 0.9152669012755881 time: 1703097146.4281554\n",
      "batch_idx: 2 loss: 0.001513116681679503 R2: 0.9152675823787323 time: 1703097148.5963466\n",
      "batch_idx: 3 loss: 0.0007820076472233869 R2: 0.9152437233855203 time: 1703097150.8430493\n",
      "Training [53%] Loss: 0.001146797624422554 time: 1703097150.8430493\n",
      "weight: [ 1.39059591  1.68615234  0.34049042  0.52747601  0.29894845  1.3124852\n",
      "  1.27202484  0.94491487  2.25251817 -0.14751037 -0.32991023  0.32797922]\n",
      "epoch 267\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015268509921786259 R2: 0.9152688347071111 time: 1703097152.9790876\n",
      "batch_idx: 1 loss: 0.000763541947380173 R2: 0.9152868971584299 time: 1703097155.0996897\n",
      "batch_idx: 2 loss: 0.0015132543119488456 R2: 0.9152874489096174 time: 1703097157.2659357\n",
      "batch_idx: 3 loss: 0.0007824673660089078 R2: 0.9152634843869187 time: 1703097159.4022207\n",
      "Training [53%] Loss: 0.001146528654379138 time: 1703097159.4022207\n",
      "weight: [ 1.3915185   1.68606324  0.34076081  0.52671535  0.29820612  1.3130374\n",
      "  1.27219754  0.94599084  2.25750839 -0.14749134 -0.32999965  0.32790556]\n",
      "epoch 268\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015257332221326365 R2: 0.9152886020067553 time: 1703097161.5959523\n",
      "batch_idx: 1 loss: 0.0007630086798443473 R2: 0.9153065157846131 time: 1703097163.7238457\n",
      "batch_idx: 2 loss: 0.0015133930385817292 R2: 0.9153069402790462 time: 1703097166.0009434\n",
      "batch_idx: 3 loss: 0.0007829241511139717 R2: 0.9152828727862298 time: 1703097168.1370952\n",
      "Training [54%] Loss: 0.0011462647729181712 time: 1703097168.1370952\n",
      "weight: [ 1.39242723  1.68597305  0.34102762  0.52595554  0.29746405  1.31358903\n",
      "  1.27236757  0.94705989  2.26244324 -0.14747251 -0.33008739  0.32783471]\n",
      "epoch 269\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001524631505064307 R2: 0.9153079978664873 time: 1703097170.2813635\n",
      "batch_idx: 1 loss: 0.0007624812752273217 R2: 0.915325765104844 time: 1703097172.4402857\n",
      "batch_idx: 2 loss: 0.0015135327535183269 R2: 0.9153260644017027 time: 1703097174.5735815\n",
      "batch_idx: 3 loss: 0.0007833779559721728 R2: 0.915301896444548 time: 1703097176.7459507\n",
      "Training [54%] Loss: 0.0011460058724455321 time: 1703097176.7459507\n",
      "weight: [ 1.39332241  1.68588179  0.34129091  0.52519659  0.29672225  1.31414011\n",
      "  1.27253505  0.94812207  2.26732333 -0.14745387 -0.33017346  0.32776676]\n",
      "epoch 270\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001523545573275049 R2: 0.915327030098425 time: 1703097178.8829572\n",
      "batch_idx: 1 loss: 0.0007619597272740463 R2: 0.9153446529068662 time: 1703097181.1567128\n",
      "batch_idx: 2 loss: 0.001513673352801297 R2: 0.9153448290299908 time: 1703097183.285856\n",
      "batch_idx: 3 loss: 0.0007838287370647397 R2: 0.9153205630624462 time: 1703097185.41737\n",
      "Training [54%] Loss: 0.001145751847603783 time: 1703097185.41737\n",
      "weight: [ 1.39420438  1.68578949  0.34155073  0.52443846  0.29598071  1.3146907\n",
      "  1.27270013  0.94917742  2.27214925 -0.14743542 -0.33025783  0.32770179]\n",
      "epoch 271\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015224751639510176 R2: 0.9153457063550846 time: 1703097187.6807382\n",
      "batch_idx: 1 loss: 0.0007614440267262551 R2: 0.9153631868185673 time: 1703097190.041354\n",
      "batch_idx: 2 loss: 0.0015138147364355274 R2: 0.9153632417571285 time: 1703097192.4883091\n",
      "batch_idx: 3 loss: 0.0007842764538045614 R2: 0.9153388801829913 time: 1703097194.7647202\n",
      "Training [54%] Loss: 0.0011455025952293403 time: 1703097194.7647202\n",
      "weight: [ 1.39507344  1.68569618  0.34180714  0.52368115  0.29523944  1.31524084\n",
      "  1.27286291  0.95022598  2.2769216  -0.14741716 -0.33034049  0.32763987]\n",
      "epoch 272\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015214200190715613 R2: 0.9153640341323938 time: 1703097197.0598032\n",
      "batch_idx: 1 loss: 0.0007609341614921599 R2: 0.9153813743110331 time: 1703097199.1932921\n",
      "batch_idx: 2 loss: 0.0015139568082539532 R2: 0.91538131002018 time: 1703097201.3499985\n",
      "batch_idx: 3 loss: 0.0007847210684243988 R2: 0.9153568551947219 time: 1703097203.5174263\n",
      "Training [54%] Loss: 0.0011452580143105184 time: 1703097203.5174263\n",
      "weight: [ 1.3959299   1.68560189  0.3420602   0.52292463  0.29449844  1.31579056\n",
      "  1.27302352  0.95126781  2.28164095 -0.14739909 -0.33042144  0.32758108]\n",
      "epoch 273\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015203798853196586 R2: 0.9153820207726314 time: 1703097205.6498327\n",
      "batch_idx: 1 loss: 0.0007604301168077778 R2: 0.9153992227015035 time: 1703097207.8243995\n",
      "batch_idx: 2 loss: 0.0015140994757891174 R2: 0.9153990411030201 time: 1703097209.9569414\n",
      "batch_idx: 3 loss: 0.0007851625458690552 R2: 0.9153744953345516 time: 1703097212.1455185\n",
      "Training [55%] Loss: 0.0011450180059464023 time: 1703097212.1455185\n",
      "weight: [ 1.39677405  1.68550663  0.34230996  0.52216889  0.29375771  1.3163399\n",
      "  1.27318209  0.95230294  2.28630789 -0.14738121 -0.33050068  0.32752549]\n",
      "epoch 274\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015193545139941765 R2: 0.9153996734673424 time: 1703097214.361373\n",
      "batch_idx: 1 loss: 0.000759931875390308 R2: 0.9154167391563087 time: 1703097216.4907656\n",
      "batch_idx: 2 loss: 0.0015142426501501913 R2: 0.9154164421392463 time: 1703097218.7007265\n",
      "batch_idx: 3 loss: 0.000785600853691326 R2: 0.9153918076906159 time: 1703097220.841563\n",
      "Training [55%] Loss: 0.0011447824733065003 time: 1703097220.841563\n",
      "weight: [ 1.39760617  1.68541043  0.34255647  0.52141392  0.29301725  1.3168889\n",
      "  1.27333871  0.95333143  2.29092299 -0.14736351 -0.33057819  0.32747316]\n",
      "epoch 275\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015183436609239368 R2: 0.9154169992601512 time: 1703097223.0282192\n",
      "batch_idx: 1 loss: 0.0007594394175838207 R2: 0.9154339306937288 time: 1703097225.152536\n",
      "batch_idx: 2 loss: 0.0015143862459052648 R2: 0.915433520115022 time: 1703097227.3156962\n",
      "batch_idx: 3 loss: 0.0007860359619514254 R2: 0.9154087992050629 time: 1703097229.5796828\n",
      "Training [55%] Loss: 0.001144551321591112 time: 1703097229.5796828\n",
      "weight: [ 1.39842655  1.68531332  0.3427998   0.5206597   0.29227707  1.3174376\n",
      "  1.2734935   0.95435331  2.29548684 -0.147346   -0.33065398  0.32742416]\n",
      "epoch 276\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001517347086383495 R2: 0.9154340050495607 time: 1703097231.7137542\n",
      "batch_idx: 1 loss: 0.0007589527214977622 R2: 0.9154508041867901 time: 1703097233.8804867\n",
      "batch_idx: 2 loss: 0.0015145301809685572 R2: 0.9154502818718836 time: 1703097236.0074105\n",
      "batch_idx: 3 loss: 0.0007864678431197519 R2: 0.915425476676796 time: 1703097238.1819878\n",
      "Training [55%] Loss: 0.0011443244579923916 time: 1703097238.1819878\n",
      "weight: [ 1.39923544  1.68521533  0.34303998  0.51990622  0.29153716  1.31798602\n",
      "  1.27364657  0.95536864  2.29999999 -0.14732868 -0.33072806  0.32737855]\n",
      "epoch 277\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015163645550106124 R2: 0.9154506975916707 time: 1703097240.3250945\n",
      "batch_idx: 1 loss: 0.0007584717631384925 R2: 0.9154673663660308 time: 1703097242.5043018\n",
      "batch_idx: 2 loss: 0.0015146743764923557 R2: 0.9154667341094722 time: 1703097244.6721926\n",
      "batch_idx: 3 loss: 0.0007868964719827887 R2: 0.9154418467641477 time: 1703097246.853494\n",
      "Training [55%] Loss: 0.0011441017916560623 time: 1703097246.853494\n",
      "weight: [ 1.40003311  1.68511646  0.34327707  0.51915345  0.29079753  1.3185342\n",
      "  1.27379802  0.95637746  2.30446301 -0.14731154 -0.33080043  0.32733637]\n",
      "epoch 278\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015153958357253326 R2: 0.9154670835028623 time: 1703097249.039876\n",
      "batch_idx: 1 loss: 0.0007579965165342622 R2: 0.915483623822199 time: 1703097251.207105\n",
      "batch_idx: 2 loss: 0.001514818756763402 R2: 0.9154828833882412 time: 1703097253.375568\n",
      "batch_idx: 3 loss: 0.0007873218255520329 R2: 0.9154579159875296 time: 1703097255.51385\n",
      "Training [56%] Loss: 0.0011438832336437574 time: 1703097255.51385\n",
      "weight: [ 1.40081981  1.68501675  0.34351112  0.51840139  0.29005817  1.31908217\n",
      "  1.27394795  0.95737981  2.30887645 -0.14729459 -0.3308711   0.32729768]\n",
      "epoch 279\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015144407016505657 R2: 0.9154831692624349 time: 1703097257.69769\n",
      "batch_idx: 1 loss: 0.0007575269538539378 R2: 0.9154995830089094 time: 1703097259.8634126\n",
      "batch_idx: 2 loss: 0.0015149632491034547 R2: 0.9154987361320932 time: 1703097262.123326\n",
      "batch_idx: 3 loss: 0.0007877438829757401 R2: 0.9154736907320002 time: 1703097264.287869\n",
      "Training [56%] Loss: 0.0011436686968959247 time: 1703097264.287869\n",
      "weight: [ 1.40159577  1.68491622  0.34374217  0.51765001  0.28931908  1.31962997\n",
      "  1.27409645  0.95837574  2.31324087 -0.14727782 -0.33094008  0.32726251]\n",
      "epoch 280\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015134989300342257 R2: 0.9154989612151778 time: 1703097266.4126344\n",
      "batch_idx: 1 loss: 0.0007570630455198481 R2: 0.9155152502452435 time: 1703097268.5981736\n",
      "batch_idx: 2 loss: 0.001515107783773831 R2: 0.9155142986309797 time: 1703097270.7446556\n",
      "batch_idx: 3 loss: 0.0007881626254535024 R2: 0.9154891772498296 time: 1703097272.899478\n",
      "Training [56%] Loss: 0.0011434580961953518 time: 1703097272.899478\n",
      "weight: [ 1.40236124  1.6848149   0.34397028  0.51689931  0.28858026  1.32017762\n",
      "  1.27424362  0.95936531  2.31755681 -0.14726123 -0.33100739  0.3272309 ]\n",
      "epoch 281\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015125703021727258 R2: 0.9155144655739212 time: 1703097275.039798\n",
      "batch_idx: 1 loss: 0.0007566047603150157 R2: 0.915530631718321 time: 1703097277.2856536\n",
      "batch_idx: 2 loss: 0.0015152522938835932 R2: 0.915529577043453 time: 1703097279.466315\n",
      "batch_idx: 3 loss: 0.0007885780361534441 R2: 0.9155043816629714 time: 1703097281.6022787\n",
      "Training [56%] Loss: 0.0011432513481311947 time: 1703097281.6022787\n",
      "weight: [ 1.40311645  1.6847128   0.3441955   0.51614927  0.28784172  1.32072515\n",
      "  1.27438955  0.96034854  2.32182481 -0.14724482 -0.33107304  0.32720287]\n",
      "epoch 282\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015116546033358425 R2: 0.915529688422039 time: 1703097283.7667696\n",
      "batch_idx: 1 loss: 0.000756152065485127 R2: 0.9155457334858156 time: 1703097285.892378\n",
      "batch_idx: 2 loss: 0.0015153967153012258 R2: 0.9155445773991888 time: 1703097288.0759506\n",
      "batch_idx: 3 loss: 0.0007889901001320017 R2: 0.9155193099655368 time: 1703097290.205549\n",
      "Training [56%] Loss: 0.0011430483710635493 time: 1703097290.205549\n",
      "weight: [ 1.40386161  1.68460994  0.34441785  0.51539987  0.28710344  1.32127259\n",
      "  1.27453433  0.96132549  2.32604541 -0.14722859 -0.33113707  0.32717845]\n",
      "epoch 283\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015107516226928906 R2: 0.91554463571588 time: 1703097292.3416176\n",
      "batch_idx: 1 loss: 0.000755704926835517 R2: 0.9155605614784352 time: 1703097294.5934303\n",
      "batch_idx: 2 loss: 0.0015155409865694643 R2: 0.9155593056014355 time: 1703097296.7299097\n",
      "batch_idx: 3 loss: 0.000789398804256224 R2: 0.915533968026183 time: 1703097298.9578364\n",
      "Training [57%] Loss: 0.001142849085088524 time: 1703097298.9578364\n",
      "weight: [ 1.40459694  1.68450635  0.3446374   0.5146511   0.28636543  1.32181996\n",
      "  1.27467803  0.96229621  2.33021914 -0.14721254 -0.33119949  0.32715764]\n",
      "epoch 284\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015098611532400551 R2: 0.9155593132872172 time: 1703097301.1061723\n",
      "batch_idx: 1 loss: 0.0007552633088234816 R2: 0.9155751215023475 time: 1703097303.275609\n",
      "batch_idx: 2 loss: 0.0015156850488231425 R2: 0.9155737674294626 time: 1703097305.4127991\n",
      "batch_idx: 3 loss: 0.0007898041371285039 R2: 0.9155483615905144 time: 1703097307.537876\n",
      "Training [57%] Loss: 0.0011426534120037956 time: 1703097307.537876\n",
      "weight: [ 1.40532266  1.68440205  0.34485418  0.51390295  0.28562769  1.32236729\n",
      "  1.27482074  0.96326074  2.33434652 -0.14719667 -0.33126032  0.32714047]\n",
      "epoch 285\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.00150898299172893 R2: 0.9155737268455952 time: 1703097309.843571\n",
      "batch_idx: 1 loss: 0.0007548271746461781 R2: 0.9155894192415855 time: 1703097311.988847\n",
      "batch_idx: 2 loss: 0.0015158288457097502 R2: 0.9155879685409205 time: 1703097314.200543\n",
      "batch_idx: 3 loss: 0.0007902060890136915 R2: 0.915562496283387 time: 1703097316.3309667\n",
      "Training [57%] Loss: 0.0011424612752746373 time: 1703097316.3309667\n",
      "weight: [ 1.40603896  1.68429706  0.34506824  0.5131554   0.28489021  1.32291461\n",
      "  1.27496255  0.96421913  2.33842807 -0.14718098 -0.3313196   0.32712692]\n",
      "epoch 286\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015081169385961382 R2: 0.9155878819806984 time: 1703097318.4874866\n",
      "batch_idx: 1 loss: 0.0007543964863243689 R2: 0.9156034602604134 time: 1703097320.6517668\n",
      "batch_idx: 2 loss: 0.0015159723233125987 R2: 0.9156019144742326 time: 1703097322.7758415\n",
      "batch_idx: 3 loss: 0.0007906046517685498 R2: 0.9155763776112369 time: 1703097325.0861871\n",
      "Training [57%] Loss: 0.0011422726000004139 time: 1703097325.0861871\n",
      "weight: [ 1.40674603  1.68419139  0.34527962  0.51240844  0.28415299  1.32346193\n",
      "  1.27510351  0.96517142  2.34246431 -0.14716547 -0.33137737  0.327117  ]\n",
      "epoch 287\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015072627978939843 R2: 0.9156017841646265 time: 1703097327.2196705\n",
      "batch_idx: 1 loss: 0.0007539712047822928 R2: 0.9156172500056302 time: 1703097329.4170105\n",
      "batch_idx: 2 loss: 0.0015161154300762625 R2: 0.915615610650873 time: 1703097331.5549104\n",
      "batch_idx: 3 loss: 0.0007909998187735072 R2: 0.9155900109643167 time: 1703097333.7183466\n",
      "Training [57%] Loss: 0.0011420873128815117 time: 1703097333.7183466\n",
      "weight: [ 1.40744408  1.68408508  0.34548835  0.51166205  0.28341604  1.32400928\n",
      "  1.27524372  0.96611766  2.34645574 -0.14715014 -0.33143364  0.32711067]\n",
      "epoch 288\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015064203772221106 R2: 0.9156154387541816 time: 1703097335.855103\n",
      "batch_idx: 1 loss: 0.0007535512899238588 R2: 0.9156307938088789 time: 1703097338.0032568\n",
      "batch_idx: 2 loss: 0.001516258116734248 R2: 0.9156290623776566 time: 1703097340.2064593\n",
      "batch_idx: 3 loss: 0.000791391584866659 R2: 0.9156034016189378 time: 1703097342.4816139\n",
      "Training [58%] Loss: 0.0011419053421867192 time: 1703097342.4816139\n",
      "weight: [ 1.40813329  1.68397812  0.34569448  0.51091622  0.28267934  1.32455668\n",
      "  1.27538323  0.9670579   2.35040287 -0.14713499 -0.33148847  0.32710794]\n",
      "epoch 289\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015055894876601459 R2: 0.915628850993102 time: 1703097344.6481173\n",
      "batch_idx: 1 loss: 0.0007531367007053832 R2: 0.9156440968888843 time: 1703097346.7939746\n",
      "batch_idx: 2 loss: 0.001516400336238667 R2: 0.9156422748489998 time: 1703097348.9712696\n",
      "batch_idx: 3 loss: 0.0007917799462800306 R2: 0.9156165547396663 time: 1703097351.1129997\n",
      "Training [58%] Loss: 0.0011417266177210566 time: 1703097351.1129997\n",
      "weight: [ 1.40881384  1.68387056  0.34589806  0.51017095  0.2819429   1.32510415\n",
      "  1.27552212  0.96799218  2.3543062  -0.14712002 -0.33154188  0.32710875]\n",
      "epoch 290\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015047699437012569 R2: 0.9156420260142598 time: 1703097353.2444708\n",
      "batch_idx: 1 loss: 0.0007527273952051297 R2: 0.91565716435368 time: 1703097355.4132931\n",
      "batch_idx: 2 loss: 0.001516542043691694 R2: 0.915655253149102 time: 1703097357.66593\n",
      "batch_idx: 3 loss: 0.0007921649005780598 R2: 0.9156294753814873 time: 1703097359.8527484\n",
      "Training [58%] Loss: 0.0011415510707940351 time: 1703097359.8527484\n",
      "weight: [ 1.4094859   1.6837624   0.3460991   0.50942621  0.28120671  1.32565172\n",
      "  1.27566046  0.96892054  2.35816621 -0.14710523 -0.33159392  0.32711309]\n",
      "epoch 291\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001503961563186662 R2: 0.915654968841826 time: 1703097361.992329\n",
      "batch_idx: 1 loss: 0.0007523233306898204 R2: 0.9156700012027843 time: 1703097364.1498873\n",
      "batch_idx: 2 loss: 0.0015166831962788215 R2: 0.915668002254138 time: 1703097366.2787554\n",
      "batch_idx: 3 loss: 0.0007925464465982368 R2: 0.9156421684919283 time: 1703097368.4100113\n",
      "Training [58%] Loss: 0.0011413786341883852 time: 1703097368.4100113\n",
      "weight: [ 1.41014964  1.68365365  0.34629767  0.50868199  0.28047078  1.32619939\n",
      "  1.2757983   0.96984305  2.3619834  -0.14709061 -0.33164463  0.3271209 ]\n",
      "epoch 292\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015031641672410125 R2: 0.9156676843934136 time: 1703097370.586162\n",
      "batch_idx: 1 loss: 0.0007519244636782716 R2: 0.9156826123293624 time: 1703097372.788724\n",
      "batch_idx: 2 loss: 0.0015168237532035577 R2: 0.9156805270344048 time: 1703097374.998535\n",
      "batch_idx: 3 loss: 0.0007929245843940272 R2: 0.9156546389131727 time: 1703097377.12411\n",
      "Training [58%] Loss: 0.0011412092421292173 time: 1703097377.12411\n",
      "weight: [ 1.41080523  1.68354435  0.34649378  0.50793829  0.2797351   1.32674719\n",
      "  1.27593571  0.97075973  2.36575824 -0.14707618 -0.33169405  0.32713213]\n",
      "epoch 293\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015023775802087051 R2: 0.9156801774821858 time: 1703097379.3145542\n",
      "batch_idx: 1 loss: 0.0007515307500023879 R2: 0.9156950025223359 time: 1703097381.4650764\n",
      "batch_idx: 2 loss: 0.001516963675623635 R2: 0.9156928322564101 time: 1703097383.5890439\n",
      "batch_idx: 3 loss: 0.0007932993151799105 R2: 0.9156668913841214 time: 1703097385.759385\n",
      "Training [59%] Loss: 0.0011410428302536598 time: 1703097385.759385\n",
      "weight: [ 1.41145283  1.6834345   0.34668748  0.50719508  0.27899966  1.32729514\n",
      "  1.27607274  0.97167064  2.36949122 -0.14706192 -0.33174224  0.32714673]\n",
      "epoch 294\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015016016295910655 R2: 0.9156924528189071 time: 1703097387.8906593\n",
      "batch_idx: 1 loss: 0.0007511421448656833 R2: 0.9157071764684745 time: 1703097390.1448274\n",
      "batch_idx: 2 loss: 0.0015171029265885132 R2: 0.915704922584965 time: 1703097392.267992\n",
      "batch_idx: 3 loss: 0.0007936706412786293 R2: 0.9156789305424459 time: 1703097394.4320288\n",
      "Training [59%] Loss: 0.0011408793355809728 time: 1703097394.4320288\n",
      "weight: [ 1.4120926   1.68332413  0.3468788   0.50645237  0.27826447  1.32784324\n",
      "  1.27620946  0.97257582  2.3731828  -0.14704784 -0.33178923  0.32716464]\n",
      "epoch 295\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015008361459844235 R2: 0.9157045150140132 time: 1703097396.5857837\n",
      "batch_idx: 1 loss: 0.0007507586028993963 R2: 0.915719138754454 time: 1703097398.7074473\n",
      "batch_idx: 2 loss: 0.001517241470978079 R2: 0.9157168025852311 time: 1703097400.8797455\n",
      "batch_idx: 3 loss: 0.0007940385660705998 R2: 0.9156907609265815 time: 1703097403.021638\n",
      "Training [59%] Loss: 0.0011407186964831246 time: 1703097403.021638\n",
      "weight: [ 1.41272469  1.68321324  0.34706777  0.50571014  0.27752953  1.32839153\n",
      "  1.27634591  0.97347531  2.37683345 -0.14703395 -0.33183507  0.32718578]\n",
      "epoch 296\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0015000809630190907 R2: 0.9157163685795966 time: 1703097405.2847896\n",
      "batch_idx: 1 loss: 0.0007503800782164772 R2: 0.9157308938688594 time: 1703097407.448839\n",
      "batch_idx: 2 loss: 0.0015173792754425144 R2: 0.9157284767247094 time: 1703097409.6168997\n",
      "batch_idx: 3 loss: 0.0007944030939454692 R2: 0.9157023869777333 time: 1703097411.747936\n",
      "Training [59%] Loss: 0.0011405608526558878 time: 1703097411.747936\n",
      "weight: [ 1.41334924  1.68310186  0.34725444  0.50496837  0.27679482  1.32894001\n",
      "  1.27648216  0.97436918  2.38044363 -0.14702023 -0.33187981  0.32721007]\n",
      "epoch 297\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001499335917299197 R2: 0.9157280179314082 time: 1703097413.8828905\n",
      "batch_idx: 1 loss: 0.0007500065244634407 R2: 0.9157424462041966 time: 1703097416.0710988\n",
      "batch_idx: 2 loss: 0.0015175163083431312 R2: 0.9157399493752443 time: 1703097418.2316673\n",
      "batch_idx: 3 loss: 0.0007947642302557863 R2: 0.9157138130418045 time: 1703097420.4427085\n",
      "Training [59%] Loss: 0.0011404057450903888 time: 1703097420.4427085\n",
      "weight: [ 1.4139664   1.68299     0.34743882  0.50422706  0.27606036  1.32948869\n",
      "  1.27661824  0.97525745  2.3840138  -0.14700669 -0.33192351  0.32723745]\n",
      "epoch 298\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014986008483434848 R2: 0.9157394673907728 time: 1703097422.678467\n",
      "batch_idx: 1 loss: 0.0007496378948702696 R2: 0.9157538000588359 time: 1703097424.8703516\n",
      "batch_idx: 2 loss: 0.0015176525396943575 R2: 0.9157512248149484 time: 1703097427.0245476\n",
      "batch_idx: 3 loss: 0.000795121981272788 R2: 0.9157250433713383 time: 1703097429.1672318\n",
      "Training [60%] Loss: 0.001140253316045225 time: 1703097429.1672318\n",
      "weight: [ 1.41457631  1.68287767  0.34762096  0.50348619  0.27532613  1.3300376\n",
      "  1.27675421  0.97614017  2.38754441 -0.14699334 -0.3319662   0.32726783]\n",
      "epoch 299\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014978755985269204 R2: 0.9157507211865326 time: 1703097431.3662584\n",
      "batch_idx: 1 loss: 0.0007492741422984838 R2: 0.9157649596389332 time: 1703097433.4990673\n",
      "batch_idx: 2 loss: 0.0015177879411065413 R2: 0.9157623072301299 time: 1703097435.6790645\n",
      "batch_idx: 3 loss: 0.0007954763541442141 R2: 0.9157360821274001 time: 1703097437.9126387\n",
      "Training [60%] Loss: 0.0011401035090190398 time: 1703097437.9126387\n",
      "weight: [ 1.4151791   1.68276489  0.34780089  0.50274576  0.27459213  1.33058674\n",
      "  1.27689012  0.97701739  2.39103589 -0.14698016 -0.33200794  0.3273011 ]\n",
      "epoch 300\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014971600130233545 R2: 0.915761783456921 time: 1703097440.1246796\n",
      "batch_idx: 1 loss: 0.0007489152192873638 R2: 0.9157759290603307 time: 1703097442.2650936\n",
      "batch_idx: 2 loss: 0.0015179224857298182 R2: 0.9157732007171642 time: 1703097444.3885453\n",
      "batch_idx: 3 loss: 0.0007958273568541306 R2: 0.9157469333814477 time: 1703097446.5851045\n",
      "Training [60%] Loss: 0.0011399562687236667 time: 1703097446.5851045\n",
      "weight: [ 1.41577492  1.68265168  0.34797863  0.50200576  0.27385837  1.33113613\n",
      "  1.27702601  0.97788916  2.3944887  -0.14696717 -0.33204878  0.32733719]\n",
      "epoch 301\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014964539397490223 R2: 0.9157726582514073 time: 1703097448.7198684\n",
      "batch_idx: 1 loss: 0.000748561078098501 R2: 0.9157867123504116 time: 1703097450.9056807\n",
      "batch_idx: 2 loss: 0.001518056148198886 R2: 0.9157839092843482 time: 1703097453.1653295\n",
      "batch_idx: 3 loss: 0.0007961749981847127 R2: 0.915757601117164 time: 1703097455.3470714\n",
      "Training [60%] Loss: 0.0011398115410577805 time: 1703097455.3470714\n",
      "weight: [ 1.41636388  1.68253804  0.34815422  0.50126617  0.27312483  1.33168578\n",
      "  1.27716192  0.97875552  2.39790326 -0.14695435 -0.33208877  0.327376  ]\n",
      "epoch 302\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014957572293070296 R2: 0.9157833495325305 time: 1703097457.487226\n",
      "batch_idx: 1 loss: 0.0007482116707586832 R2: 0.9157973134499289 time: 1703097459.6198304\n",
      "batch_idx: 2 loss: 0.0015181889045787682 R2: 0.9157944368537064 time: 1703097461.7968636\n",
      "batch_idx: 3 loss: 0.0007965192876798926 R2: 0.9157680892322542 time: 1703097463.9304092\n",
      "Training [60%] Loss: 0.0011396692730810934 time: 1703097463.9304092\n",
      "weight: [ 1.41694611  1.682424    0.34832769  0.50052699  0.27239152  1.3322357\n",
      "  1.2772979   0.97961651  2.40128002 -0.14694172 -0.33212795  0.32741742]\n",
      "epoch 303\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014950697349329133 R2: 0.9157938611776698 time: 1703097466.1120636\n",
      "batch_idx: 1 loss: 0.0007478669491011194 R2: 0.9158077362147882 time: 1703097468.2427466\n",
      "batch_idx: 2 loss: 0.001518320732311573 R2: 0.9158047872627805 time: 1703097470.545968\n",
      "batch_idx: 3 loss: 0.0007968602356108291 R2: 0.915778401540232 time: 1703097472.681514\n",
      "Training [61%] Loss: 0.0011395294129891086 time: 1703097472.681514\n",
      "weight: [ 1.41752174  1.68230956  0.34849906  0.4997882   0.27165844  1.3327859\n",
      "  1.27743398  0.98047218  2.40461938 -0.14692927 -0.33216638  0.32746135]\n",
      "epoch 304\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014943913124410998 R2: 0.9158041969808245 time: 1703097474.7961428\n",
      "batch_idx: 1 loss: 0.0007475268648051258 R2: 0.9158179844178262 time: 1703097476.9860523\n",
      "batch_idx: 2 loss: 0.0015184516101642716 R2: 0.9158149642663794 time: 1703097479.1204405\n",
      "batch_idx: 3 loss: 0.0007971978529430941 R2: 0.9157885417721495 time: 1703097481.3151782\n",
      "Training [61%] Loss: 0.0011393919100883979 time: 1703097481.3151782\n",
      "weight: [ 1.41809088  1.68219474  0.34866837  0.49904981  0.27092558  1.33333639\n",
      "  1.27757021  0.98132258  2.40792179 -0.14691701 -0.3322041   0.32750769]\n",
      "epoch 305\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014937218201724815 R2: 0.9158143606543174 time: 1703097483.4402707\n",
      "batch_idx: 1 loss: 0.0007471913694342153 R2: 0.9158280617505168 time: 1703097485.7123303\n",
      "batch_idx: 2 loss: 0.0015185815181775972 R2: 0.9158249715382901 time: 1703097487.8560362\n",
      "batch_idx: 3 loss: 0.0007975321513055264 R2: 0.9157985135783153 time: 1703097489.9916115\n",
      "Training [61%] Loss: 0.0011392567147724551 time: 1703097489.9916115\n",
      "weight: [ 1.41865365  1.68207955  0.34883564  0.49831179  0.27019294  1.33388719\n",
      "  1.27770661  0.98216774  2.41118764 -0.14690492 -0.33224115  0.32755634]\n",
      "epoch 306\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014930611189430203 R2: 0.9158243558304948 time: 1703097492.1734285\n",
      "batch_idx: 1 loss: 0.0007468604144726558 R2: 0.9158379718246878 time: 1703097494.293081\n",
      "batch_idx: 2 loss: 0.0015187104376160512 R2: 0.9158348126729635 time: 1703097496.4756494\n",
      "batch_idx: 3 loss: 0.0007978631429606343 R2: 0.9158083205299776 time: 1703097498.6223273\n",
      "Training [61%] Loss: 0.0011391237784980903 time: 1703097498.6223273\n",
      "weight: [ 1.41921017  1.68196401  0.3490009   0.49757415  0.26946052  1.3344383\n",
      "  1.27784322  0.98300772  2.41441735 -0.14689302 -0.33227759  0.32760719]\n",
      "epoch 307\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014924090719934445 R2: 0.9158341860633927 time: 1703097500.9056966\n",
      "batch_idx: 1 loss: 0.0007465339513605052 R2: 0.9158477181741761 time: 1703097503.0563998\n",
      "batch_idx: 2 loss: 0.0015188383509191035 R2: 0.9158444911871714 time: 1703097505.1861389\n",
      "batch_idx: 3 loss: 0.0007981908407764405 R2: 0.9158179661209747 time: 1703097507.379461\n",
      "Training [61%] Loss: 0.0011389930537623735 time: 1703097507.379461\n",
      "weight: [ 1.41976053  1.68184812  0.34916417  0.49683687  0.26872832  1.33498972\n",
      "  1.27798008  0.98384255  2.41761133 -0.1468813  -0.33231346  0.32766013]\n",
      "epoch 308\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014917655449400388 R2: 0.9158438548303562 time: 1703097509.5314443\n",
      "batch_idx: 1 loss: 0.0007462119315271038 R2: 0.9158573042564402 time: 1703097511.7118077\n",
      "batch_idx: 2 loss: 0.0015189652416536601 R2: 0.9158540105216136 time: 1703097513.846913\n",
      "batch_idx: 3 loss: 0.0007985152581997404 R2: 0.9158274537693469 time: 1703097516.0323133\n",
      "Training [62%] Loss: 0.0011388644940801358 time: 1703097516.0323133\n",
      "weight: [ 1.42030486  1.68173189  0.34932549  0.49609995  0.26799633  1.33554148\n",
      "  1.27811722  0.98467227  2.42076998 -0.14686976 -0.33234879  0.32771506]\n",
      "epoch 309\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014911304057265758 R2: 0.9158533655336368 time: 1703097518.2768822\n",
      "batch_idx: 1 loss: 0.0007458943064230631 R2: 0.9158667334541886 time: 1703097520.4153636\n",
      "batch_idx: 2 loss: 0.0015190910944678244 R2: 0.9158633740425219 time: 1703097522.6025953\n",
      "batch_idx: 3 loss: 0.0007988364092305757 R2: 0.9158367868189291 time: 1703097524.720249\n",
      "Training [62%] Loss: 0.0011387380539620098 time: 1703097524.720249\n",
      "weight: [ 1.42084325  1.68161535  0.34948487  0.49536337  0.26726455  1.33609358\n",
      "  1.27825467  0.98549695  2.4238937  -0.1468584  -0.33238363  0.32777188]\n",
      "epoch 310\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014905035245773445 R2: 0.9158627215019722 time: 1703097526.9033234\n",
      "batch_idx: 1 loss: 0.0007455810275506993 R2: 0.9158760090769189 time: 1703097529.0381966\n",
      "batch_idx: 2 loss: 0.0015192158950460668 R2: 0.9158725850431988 time: 1703097531.2265832\n",
      "batch_idx: 3 loss: 0.0007991543083979274 R2: 0.9158459685409197 time: 1703097533.4392517\n",
      "Training [62%] Loss: 0.0011386136888930096 time: 1703097533.4392517\n",
      "weight: [ 1.42137581  1.6814985   0.34964235  0.49462713  0.26653298  1.33664602\n",
      "  1.27839245  0.9863166   2.42698287 -0.14684722 -0.33241803  0.32783048]\n",
      "epoch 311\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014898847739513443 R2: 0.9158719259920979 time: 1703097535.5745564\n",
      "batch_idx: 1 loss: 0.0007452720464930192 R2: 0.9158851343624643 time: 1703097537.7565897\n",
      "batch_idx: 2 loss: 0.0015193396300657767 R2: 0.9158816467455525 time: 1703097539.906695\n",
      "batch_idx: 3 loss: 0.0007994689707364402 R2: 0.9158550021353815 time: 1703097542.0893054\n",
      "Training [62%] Loss: 0.0011384913553116453 time: 1703097542.0893054\n",
      "weight: [ 1.42190264  1.68138135  0.34979794  0.49389122  0.26580162  1.33719882\n",
      "  1.2785306   0.98713129  2.4300379  -0.14683623 -0.33245201  0.32789076]\n",
      "epoch 312\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014892740284976334 R2: 0.9158809821902689 time: 1703097544.2304635\n",
      "batch_idx: 1 loss: 0.0007449673149411008 R2: 0.9158941124784976 time: 1703097546.41281\n",
      "batch_idx: 2 loss: 0.0015194622871553315 R2: 0.9158905623015837 time: 1703097548.5789268\n",
      "batch_idx: 3 loss: 0.000799780411764159 R2: 0.915863890732766 time: 1703097550.7918713\n",
      "Training [62%] Loss: 0.0011383710105895561 time: 1703097550.7918713\n",
      "weight: [ 1.42242383  1.68126391  0.34995168  0.49315564  0.26507047  1.33775197\n",
      "  1.27866914  0.98794104  2.43305916 -0.14682542 -0.33248562  0.32795262]\n",
      "epoch 313\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014886711650118239 R2: 0.9158898932137289 time: 1703097552.9589329\n",
      "batch_idx: 1 loss: 0.0007446667847200315 R2: 0.9159029465239937 time: 1703097555.092178\n",
      "batch_idx: 2 loss: 0.001519583854853685 R2: 0.9158993347948486 time: 1703097557.2588468\n",
      "batch_idx: 3 loss: 0.0008000886474611368 R2: 0.9158726373953657 time: 1703097559.3944597\n",
      "Training [63%] Loss: 0.0011382526130116694 time: 1703097559.3944597\n",
      "weight: [ 1.42293948  1.6811462   0.35010359  0.49242038  0.26433951  1.33830549\n",
      "  1.27880809  0.98874591  2.43604703 -0.14681479 -0.33251888  0.32801596]\n",
      "epoch 314\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014880760623936806 R2: 0.9158986621121412 time: 1703097561.6132026\n",
      "batch_idx: 1 loss: 0.0007443704078133223 R2: 0.9159116395306766 time: 1703097564.0863128\n",
      "batch_idx: 2 loss: 0.0015197043225714936 R2: 0.9159079672418976 time: 1703097566.3358393\n",
      "batch_idx: 3 loss: 0.0008003936942488395 R2: 0.9158812451187405 time: 1703097568.5456014\n",
      "Training [63%] Loss: 0.001138136121756834 time: 1703097568.5456014\n",
      "weight: [ 1.42344969  1.68102821  0.35025368  0.49168542  0.26360876  1.33885939\n",
      "  1.27894749  0.98954594  2.43900188 -0.14680434 -0.33255183  0.32808069]\n",
      "epoch 315\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001487488601606 R2: 0.9159072918690235 time: 1703097570.7085538\n",
      "batch_idx: 1 loss: 0.0007440781363857609 R2: 0.9159201944644287 time: 1703097572.8644578\n",
      "batch_idx: 2 loss: 0.001519823680553875 R2: 0.9159164625936709 time: 1703097574.9977186\n",
      "batch_idx: 3 loss: 0.0008006955689702725 R2: 0.915889716833149 time: 1703097577.1887352\n",
      "Training [63%] Loss: 0.0011380214968789771 time: 1703097577.1887352\n",
      "weight: [ 1.42395453  1.68090998  0.35040199  0.49095077  0.26287821  1.33941367\n",
      "  1.27908734  0.99034116  2.44192409 -0.14679407 -0.33258451  0.32814672]\n",
      "epoch 316\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014869086656344606 R2: 0.9159157854031152 time: 1703097579.3316188\n",
      "batch_idx: 1 loss: 0.0007437899228049294 R2: 0.9159286142266737 time: 1703097581.5500922\n",
      "batch_idx: 2 loss: 0.001519941919844726 R2: 0.9159248237368741 time: 1703097583.7192738\n",
      "batch_idx: 3 loss: 0.0008009942888707768 R2: 0.9158980554049012 time: 1703097585.8558097\n",
      "Training [63%] Loss: 0.001137908699288723 time: 1703097585.8558097\n",
      "weight: [ 1.4244541   1.68079149  0.35054853  0.49021642  0.26214785  1.33996833\n",
      "  1.27922768  0.99113162  2.44481403 -0.14678398 -0.33261693  0.32821396]\n",
      "epoch 317\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014863361394486766 R2: 0.9159241455697419 time: 1703097588.025093\n",
      "batch_idx: 1 loss: 0.0007435057196611013 R2: 0.915936901655732 time: 1703097590.1768754\n",
      "batch_idx: 2 loss: 0.0015200590322526583 R2: 0.9159330534953286 time: 1703097592.3313868\n",
      "batch_idx: 3 loss: 0.0008012898715793505 R2: 0.9159062636377243 time: 1703097594.4694026\n",
      "Training [63%] Loss: 0.0011377976907354466 time: 1703097594.4694026\n",
      "weight: [ 1.42494849  1.68067277  0.35069333  0.48948236  0.26141769  1.34052338\n",
      "  1.27936852  0.99191737  2.44767206 -0.14677406 -0.33264914  0.32828232]\n",
      "epoch 318\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014857709099643853 R2: 0.9159323751621427 time: 1703097596.6320918\n",
      "batch_idx: 1 loss: 0.0007432254797858027 R2: 0.9159450595281331 time: 1703097598.8832555\n",
      "batch_idx: 2 loss: 0.0015201750103185688 R2: 0.9159411546312825 time: 1703097601.023456\n",
      "batch_idx: 3 loss: 0.0008015823350905362 R2: 0.9159143442740764 time: 1703097603.193597\n",
      "Training [64%] Loss: 0.0011376884337898233 time: 1703097603.193597\n",
      "weight: [ 1.42543779  1.68055382  0.35083641  0.48874858  0.26068772  1.34107883\n",
      "  1.27950988  0.99269843  2.45049853 -0.14676433 -0.33268114  0.32835173]\n",
      "epoch 319\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014852128660065866 R2: 0.9159404769127664 time: 1703097605.3333707\n",
      "batch_idx: 1 loss: 0.0007429491562689948 R2: 0.91595309055993 time: 1703097607.5381315\n",
      "batch_idx: 2 loss: 0.001520289847284723 R2: 0.915949129846703 time: 1703097609.6760437\n",
      "batch_idx: 3 loss: 0.0008018716977467465 R2: 0.9159222999964383 time: 1703097611.812754\n",
      "Training [64%] Loss: 0.0011375808918267626 time: 1703097611.812754\n",
      "weight: [ 1.42592206  1.68043466  0.35097779  0.48801507  0.25995794  1.34163469\n",
      "  1.27965178  0.99347485  2.45329379 -0.14675477 -0.33271298  0.3284221 ]\n",
      "epoch 320\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014846618982738424 R2: 0.9159484534945566 time: 1703097614.0790925\n",
      "batch_idx: 1 loss: 0.0007426767024747817 R2: 0.9159609974079561 time: 1703097616.2024407\n",
      "batch_idx: 2 loss: 0.0015204035370654713 R2: 0.9159569817845515 time: 1703097618.3835645\n",
      "batch_idx: 3 loss: 0.000802157978220993 R2: 0.9159301334285839 time: 1703097620.6003172\n",
      "Training [64%] Loss: 0.0011374750290087722 time: 1703097620.6003172\n",
      "weight: [ 1.4264014   1.68031528  0.35111749  0.48728184  0.25922835  1.34219095\n",
      "  1.27979424  0.99424668  2.45605821 -0.14674539 -0.33274466  0.32849336]\n",
      "epoch 321\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014841178993035523 R2: 0.9159563075221937 time: 1703097622.7818675\n",
      "batch_idx: 1 loss: 0.0007424080720559281 R2: 0.9159687826710783 time: 1703097624.9018435\n",
      "batch_idx: 2 loss: 0.001520516074219415 R2: 0.9159647130300076 time: 1703097627.0464807\n",
      "batch_idx: 3 loss: 0.0008024411955000381 R2: 0.9159378471368231 time: 1703097629.343387\n",
      "Training [64%] Loss: 0.0011373708102697334 time: 1703097629.343387\n",
      "weight: [ 1.42687588  1.6801957   0.35125554  0.48654888  0.25849894  1.34274762\n",
      "  1.27993727  0.99501395  2.45879213 -0.14673619 -0.3327762   0.32856545]\n",
      "epoch 322\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001483580763438125 R2: 0.9159640415533172 time: 1703097631.5233276\n",
      "batch_idx: 1 loss: 0.0007421432189670619 R2: 0.9159764488914076 time: 1703097633.706842\n",
      "batch_idx: 2 loss: 0.0015206274539230743 R2: 0.915972326111698 time: 1703097635.837837\n",
      "batch_idx: 3 loss: 0.0008027213688678588 R2: 0.91594544363121 time: 1703097638.017547\n",
      "Training [64%] Loss: 0.00113726820129903 time: 1703097638.017547\n",
      "weight: [ 1.42734557  1.68007594  0.35139194  0.48581617  0.25776972  1.3433047\n",
      "  1.28008089  0.9957767   2.46149588 -0.14672715 -0.33280764  0.32863829]\n",
      "epoch 323\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014830503867922181 R2: 0.9159716580897264 time: 1703097640.1494694\n",
      "batch_idx: 1 loss: 0.0007418820974766039 R2: 0.9159839985554978 time: 1703097642.2806606\n",
      "batch_idx: 2 loss: 0.001520737671945957 R2: 0.9159798235028657 time: 1703097644.4655514\n",
      "batch_idx: 3 loss: 0.0008029985178894441 R2: 0.9159529253667339 time: 1703097646.7254899\n",
      "Training [65%] Loss: 0.0011371671685260556 time: 1703097646.7254899\n",
      "weight: [ 1.42781057  1.67995599  0.35152673  0.48508372  0.25704068  1.34386221\n",
      "  1.28022511  0.99653497  2.46416981 -0.1467183  -0.33283898  0.32871182]\n",
      "epoch 324\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014825266672207568 R2: 0.9159791595785632 time: 1703097648.9917133\n",
      "batch_idx: 1 loss: 0.000741624662177647 R2: 0.9159914340955154 time: 1703097651.1398492\n",
      "batch_idx: 2 loss: 0.001520846724626959 R2: 0.9159872076225593 time: 1703097653.6069171\n",
      "batch_idx: 3 loss: 0.000803272662394915 R2: 0.9159602947444936 time: 1703097656.1175056\n",
      "Training [65%] Loss: 0.0011370676791050693 time: 1703097656.1175056\n",
      "weight: [ 1.42827092  1.67983586  0.35165993  0.48435152  0.25631182  1.34442014\n",
      "  1.28036994  0.99728881  2.46681425 -0.14670961 -0.33287024  0.32878598]\n",
      "epoch 325\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014820095042878665 R2: 0.9159865484134488 time: 1703097658.3157856\n",
      "batch_idx: 1 loss: 0.0007413708679976786 R2: 0.9159987578903783 time: 1703097660.5899136\n",
      "batch_idx: 2 loss: 0.0015209546088520431 R2: 0.9159944808367644 time: 1703097662.8178287\n",
      "batch_idx: 3 loss: 0.0008035438224639269 R2: 0.9159675541128252 time: 1703097665.1090894\n",
      "Training [65%] Loss: 0.0011369697009003788 time: 1703097665.1090894\n",
      "weight: [ 1.42872672  1.67971557  0.35179154  0.48361956  0.25558314  1.34497849\n",
      "  1.2805154   0.99803824  2.46942953 -0.14670109 -0.33290143  0.32886072]\n",
      "epoch 326\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014814987992365699 R2: 0.9159938269356239 time: 1703097667.3321884\n",
      "batch_idx: 1 loss: 0.0007411206702072856 R2: 0.9160059722668926 time: 1703097669.5342808\n",
      "batch_idx: 2 loss: 0.001521061322033129 R2: 0.9160016454595181 time: 1703097671.6998734\n",
      "batch_idx: 3 loss: 0.0008038120184103742 R2: 0.9159747057684339 time: 1703097673.8179753\n",
      "Training [65%] Loss: 0.0011368732024718397 time: 1703097673.8179753\n",
      "weight: [ 1.42917803  1.67959512  0.3519216   0.48288784  0.25485464  1.34553728\n",
      "  1.2806615   0.99878331  2.47201599 -0.14669274 -0.33293257  0.32893599]\n",
      "epoch 327\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014809944549592786 R2: 0.9160009974350489 time: 1703097676.0075839\n",
      "batch_idx: 1 loss: 0.0007408740244279079 R2: 0.9160130795008369 time: 1703097678.2552795\n",
      "batch_idx: 2 loss: 0.001521166862088071 R2: 0.9160087037540224 time: 1703097680.4577951\n",
      "batch_idx: 3 loss: 0.000804077270767389 R2: 0.9159817519574794 time: 1703097682.6691306\n",
      "Training [65%] Loss: 0.0011367781530606615 time: 1703097682.6691306\n",
      "weight: [ 1.42962491  1.67947452  0.35205011  0.48215635  0.25412631  1.3460965\n",
      "  1.28080825  0.99952406  2.47457393 -0.14668456 -0.33296368  0.32901172]\n",
      "epoch 328\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014804963759689753 R2: 0.9160080621514954 time: 1703097684.8670075\n",
      "batch_idx: 1 loss: 0.000740630886638675 R2: 0.9160200818180521 time: 1703097687.0152774\n",
      "batch_idx: 2 loss: 0.0015212712274216959 R2: 0.9160156579336987 time: 1703097689.142685\n",
      "batch_idx: 3 loss: 0.0008043396002726308 R2: 0.91598869487666 time: 1703097691.319878\n",
      "Training [66%] Loss: 0.0011366845225754942 time: 1703097691.319878\n",
      "weight: [ 1.43006743  1.67935377  0.3521771   0.48142509  0.25339815  1.34665616\n",
      "  1.28095565  1.00026052  2.47710369 -0.14667655 -0.33299475  0.32908789]\n",
      "epoch 329\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014800044683711065 R2: 0.9160150232755969 time: 1703097693.5469496\n",
      "batch_idx: 1 loss: 0.0007403912131824663 R2: 0.9160269813954912 time: 1703097695.7403805\n",
      "batch_idx: 2 loss: 0.0015213744169077696 R2: 0.9160225101632683 time: 1703097697.8891523\n",
      "batch_idx: 3 loss: 0.0008045990278538852 R2: 0.9159955366742591 time: 1703097700.0920732\n",
      "Training [66%] Loss: 0.001136592281578807 time: 1703097700.0920732\n",
      "weight: [ 1.43050567  1.67923289  0.35230259  0.48069406  0.25267017  1.34721625\n",
      "  1.28110372  1.00099273  2.47960557 -0.1466687  -0.3330258   0.32916444]\n",
      "epoch 330\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001479518639836038 R2: 0.9160218829499062 time: 1703097702.2388997\n",
      "batch_idx: 1 loss: 0.0007401549607711482 R2: 0.9160337803622692 time: 1703097704.3729815\n",
      "batch_idx: 2 loss: 0.0015214764298718806 R2: 0.9160292625597666 time: 1703097706.5687344\n",
      "batch_idx: 3 loss: 0.000804855574614962 R2: 0.9160022794511772 time: 1703097708.7673025\n",
      "Training [66%] Loss: 0.0011365014012735073 time: 1703097708.7673025\n",
      "weight: [ 1.43093968  1.67911188  0.35242659  0.47996324  0.25194235  1.34777679\n",
      "  1.28125247  1.00172074  2.4820799  -0.14666102 -0.33305684  0.32924134]\n",
      "epoch 331\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014790387995721537 R2: 0.9160286432699107 time: 1703097711.0311646\n",
      "batch_idx: 1 loss: 0.000739922086490166 R2: 0.9160404808006554 time: 1703097713.1602151\n",
      "batch_idx: 2 loss: 0.0015215772660750518 R2: 0.9160359171935738 time: 1703097715.3595479\n",
      "batch_idx: 3 loss: 0.0008051092618219316 R2: 0.9160089252619439 time: 1703097717.4915164\n",
      "Training [66%] Loss: 0.0011364118534898258 time: 1703097717.4915164\n",
      "weight: [ 1.43136953  1.67899075  0.35254912  0.47923264  0.2512147   1.34833778\n",
      "  1.2814019   1.00244457  2.48452697 -0.1466535  -0.33308787  0.32931854]\n",
      "epoch 332\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014785648582995483 R2: 0.916035306285026 time: 1703097719.6281507\n",
      "batch_idx: 1 loss: 0.0007396925478024907 R2: 0.9160470847470968 time: 1703097721.8392036\n",
      "batch_idx: 2 loss: 0.001521676925698114 R2: 0.9160424760893958 time: 1703097724.0048418\n",
      "batch_idx: 3 loss: 0.0008053601108896941 R2: 0.9160154761157177 time: 1703097726.3764179\n",
      "Training [66%] Loss: 0.0011363236106724619 time: 1703097726.3764179\n",
      "weight: [ 1.43179528  1.6788695   0.35267019  0.47850224  0.25048721  1.34889921\n",
      "  1.28155202  1.00316426  2.48694711 -0.14664613 -0.33311891  0.32939602]\n",
      "epoch 333\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014780967282240823 R2: 0.9160418739996017 time: 1703097728.541587\n",
      "batch_idx: 1 loss: 0.0007394663025520107 R2: 0.9160535941931786 time: 1703097730.7621717\n",
      "batch_idx: 2 loss: 0.0015217754093267161 R2: 0.9160489412272538 time: 1703097732.917411\n",
      "batch_idx: 3 loss: 0.0008056081433688792 R2: 0.9160219339772478 time: 1703097735.052122\n",
      "Training [67%] Loss: 0.0011362366458679222 time: 1703097735.052122\n",
      "weight: [ 1.43221698  1.67874814  0.35278984  0.47777206  0.24975989  1.34946109\n",
      "  1.28170283  1.00387985  2.48934059 -0.14663893 -0.33314996  0.32947374]\n",
      "epoch 334\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014776343230121 R2: 0.9160483483738538 time: 1703097737.2745903\n",
      "batch_idx: 1 loss: 0.0007392433089663796 R2: 0.9160600110865855 time: 1703097739.415799\n",
      "batch_idx: 2 loss: 0.0015218727179368823 R2: 0.9160553145434325 time: 1703097741.753854\n",
      "batch_idx: 3 loss: 0.0008058533809331163 R2: 0.9160283007678262 time: 1703097743.8880804\n",
      "Training [67%] Loss: 0.0011361509327121196 time: 1703097743.8880804\n",
      "weight: [ 1.4326347   1.67862668  0.35290806  0.47704207  0.24903273  1.35002343\n",
      "  1.28185435  1.00459138  2.49170774 -0.14663189 -0.33318102  0.32955166]\n",
      "epoch 335\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014771775577654995 R2: 0.9160547313248404 time: 1703097746.0590692\n",
      "batch_idx: 1 loss: 0.0007390235256594476 R2: 0.916066337332046 time: 1703097748.2175488\n",
      "batch_idx: 2 loss: 0.0015219688528811154 R2: 0.9160615979314176 time: 1703097750.3790414\n",
      "batch_idx: 3 loss: 0.0008060958453666928 R2: 0.9160345783662361 time: 1703097752.5593882\n",
      "Training [67%] Loss: 0.001136066445418189 time: 1703097752.5593882\n",
      "weight: [ 1.4330485   1.67850513  0.35302488  0.47631228  0.24830573  1.35058621\n",
      "  1.28200657  1.00529889  2.49404883 -0.146625   -0.33321211  0.32962977]\n",
      "epoch 336\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014767263489972055 R2: 0.9160610247273698 time: 1703097754.715068\n",
      "batch_idx: 1 loss: 0.0007388069116333019 R2: 0.9160725747922477 time: 1703097757.0529373\n",
      "batch_idx: 2 loss: 0.001522063815874914 R2: 0.916067793242831 time: 1703097759.219238\n",
      "batch_idx: 3 loss: 0.0008063355585525487 R2: 0.9160407686096524 time: 1703097761.4366503\n",
      "Training [67%] Loss: 0.0011359831587644926 time: 1703097761.4366503\n",
      "weight: [ 1.43345843  1.67838348  0.35314031  0.47558268  0.24757889  1.35114946\n",
      "  1.28215951  1.00600239  2.49636417 -0.14661826 -0.33324322  0.32970802]\n",
      "epoch 337\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001476280614607157 R2: 0.9160672304149162 time: 1703097763.5991347\n",
      "batch_idx: 1 loss: 0.0007385934262798522 R2: 0.9160787252887473 time: 1703097765.7734709\n",
      "batch_idx: 2 loss: 0.0015221576089837583 R2: 0.9160739022883085 time: 1703097767.9596894\n",
      "batch_idx: 3 loss: 0.0008065725424606847 R2: 0.9160468732945523 time: 1703097770.1726913\n",
      "Training [67%] Loss: 0.0011359010480828631 time: 1703097770.1726913\n",
      "weight: [ 1.43386455  1.67826176  0.35325437  0.47485327  0.2468522   1.35171316\n",
      "  1.28231315  1.00670195  2.49865404 -0.14661168 -0.33327436  0.32978641]\n",
      "epoch 338\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014758402738585545 R2: 0.9160733501805062 time: 1703097772.3559527\n",
      "batch_idx: 1 loss: 0.0007383830293822401 R2: 0.9160847906028516 time: 1703097774.599571\n",
      "batch_idx: 2 loss: 0.0015222502346103504 R2: 0.9160799268384114 time: 1703097776.820737\n",
      "batch_idx: 3 loss: 0.0008068068191370013 R2: 0.9160528941775882 time: 1703097778.9762914\n",
      "Training [68%] Loss: 0.0011358200892470367 time: 1703097778.9762914\n",
      "weight: [ 1.43426691  1.67813995  0.35336708  0.47412405  0.24612567  1.35227732\n",
      "  1.28246752  1.00739757  2.50091872 -0.14660525 -0.33330553  0.3298649 ]\n",
      "epoch 339\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014754052473545266 R2: 0.9160793857775944 time: 1703097781.1412296\n",
      "batch_idx: 1 loss: 0.0007381756811158462 R2: 0.9160907724764857 time: 1703097783.3237436\n",
      "batch_idx: 2 loss: 0.0015223416954823058 R2: 0.9160858686244729 time: 1703097785.4739692\n",
      "batch_idx: 3 loss: 0.0008070384106924228 R2: 0.916058832976456 time: 1703097787.7741313\n",
      "Training [68%] Loss: 0.0011357402586612754 time: 1703097787.7741313\n",
      "weight: [ 1.43466556  1.67801808  0.35347844  0.47339501  0.24539929  1.35284194\n",
      "  1.28262261  1.00808932  2.50315851 -0.14659896 -0.33333674  0.32994347]\n",
      "epoch 340\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001474975457015176 R2: 0.9160853389209282 time: 1703097790.2877023\n",
      "batch_idx: 1 loss: 0.000737971342049094 R2: 0.9160966726130569 time: 1703097792.7230875\n",
      "batch_idx: 2 loss: 0.0015224319946400082 R2: 0.9160917293394654 time: 1703097795.050105\n",
      "batch_idx: 3 loss: 0.0008072673392925887 R2: 0.9160646913707403 time: 1703097797.1952138\n",
      "Training [68%] Loss: 0.0011356615332492168 time: 1703097797.1952138\n",
      "weight: [ 1.43506056  1.67789613  0.35358848  0.47266614  0.24467307  1.35340702\n",
      "  1.28277842  1.0087772   2.50537367 -0.14659283 -0.33336798  0.3300221 ]\n",
      "epoch 341\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001474550826054899 R2: 0.9160912112873676 time: 1703097799.3631408\n",
      "batch_idx: 1 loss: 0.0007377699731440647 R2: 0.916102492678269 time: 1703097801.5381594\n",
      "batch_idx: 2 loss: 0.001522521135424822 R2: 0.916097510638821 time: 1703097803.7401326\n",
      "batch_idx: 3 loss: 0.0008074936271477791 R2: 0.9160704710027415 time: 1703097806.044619\n",
      "Training [68%] Loss: 0.0011355838904428912 time: 1703097806.044619\n",
      "weight: [ 1.43545196  1.67777413  0.3536972   0.47193746  0.243947    1.35397256\n",
      "  1.28293495  1.00946127  2.50756448 -0.14658684 -0.33339927  0.33010077]\n",
      "epoch 342\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014741312789600517 R2: 0.9160970045167337 time: 1703097808.2761962\n",
      "batch_idx: 1 loss: 0.0007375715357568287 R2: 0.9161082343009637 time: 1703097810.4588342\n",
      "batch_idx: 2 loss: 0.0015226091214675184 R2: 0.9161032141412606 time: 1703097812.606719\n",
      "batch_idx: 3 loss: 0.0008077172965033302 R2: 0.9160761734782963 time: 1703097814.8184478\n",
      "Training [68%] Loss: 0.0011355073081719322 time: 1703097814.8184478\n",
      "weight: [ 1.4358398   1.67765207  0.35380463  0.47120894  0.24322107  1.35453857\n",
      "  1.28309221  1.01014156  2.50973122 -0.146581   -0.3334306   0.33017946]\n",
      "epoch 343\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001473716741467021 R2: 0.9161027202125872 time: 1703097816.9838524\n",
      "batch_idx: 1 loss: 0.000737375991637675 R2: 0.9161138990739058 time: 1703097819.1667304\n",
      "batch_idx: 2 loss: 0.0015226959566769067 R2: 0.9161088414295891 time: 1703097821.45711\n",
      "batch_idx: 3 loss: 0.0008079383696304471 R2: 0.9160818003675647 time: 1703097824.0864408\n",
      "Training [69%] Loss: 0.0011354317648530124 time: 1703097824.0864408\n",
      "weight: [ 1.43622414  1.67752996  0.35391078  0.47048059  0.2424953   1.35510504\n",
      "  1.2832502   1.01081809  2.51187416 -0.14657529 -0.33346197  0.33025816]\n",
      "epoch 344\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001473307140540452 R2: 0.9161083599430462 time: 1703097826.7141912\n",
      "batch_idx: 1 loss: 0.0007371833029311482 R2: 0.9161194885545771 time: 1703097829.288217\n",
      "batch_idx: 2 loss: 0.0015227816452287183 R2: 0.9161143940514747 time: 1703097831.9648297\n",
      "batch_idx: 3 loss: 0.0008081568688173344 R2: 0.9160873532058249 time: 1703097834.5442524\n",
      "Training [69%] Loss: 0.0011353572393794132 time: 1703097834.5442524\n",
      "weight: [ 1.43660503  1.67740781  0.35401566  0.46975241  0.24176967  1.35567198\n",
      "  1.28340891  1.0114909   2.51399356 -0.14656973 -0.33349339  0.33033685]\n",
      "epoch 345\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014729024043519466 R2: 0.9161139252415407 time: 1703097837.127277\n",
      "batch_idx: 1 loss: 0.0007369934321758929 R2: 0.9161250042659519 time: 1703097839.692176\n",
      "batch_idx: 2 loss: 0.0015228661915547143 R2: 0.9161198735202305 time: 1703097842.3764067\n",
      "batch_idx: 3 loss: 0.0008083728163607648 R2: 0.9160928334942241 time: 1703097845.0744023\n",
      "Training [69%] Loss: 0.0011352837111108297 time: 1703097845.0744023\n",
      "weight: [ 1.43698251  1.67728561  0.35411928  0.46902439  0.24104418  1.35623938\n",
      "  1.28356835  1.01216003  2.51608968 -0.14656431 -0.33352486  0.33041551]\n",
      "epoch 346\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014725024622590007 R2: 0.9161194176075803 time: 1703097847.6637573\n",
      "batch_idx: 1 loss: 0.000736806342304418 R2: 0.9161304476972478 time: 1703097850.3678944\n",
      "batch_idx: 2 loss: 0.0015229496003319695 R2: 0.9161252813155668 time: 1703097853.014604\n",
      "batch_idx: 3 loss: 0.0008085862345580104 R2: 0.9160982427005407 time: 1703097855.6660473\n",
      "Training [69%] Loss: 0.0011352111598633497 time: 1703097855.6660473\n",
      "weight: [ 1.43735662  1.67716338  0.35422166  0.46829653  0.24031884  1.35680725\n",
      "  1.28372852  1.0128255   2.51816279 -0.14655903 -0.33355638  0.33049412]\n",
      "epoch 347\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014721072447842233 R2: 0.9161248385074996 time: 1703097858.240485\n",
      "batch_idx: 1 loss: 0.000736621996642683 R2: 0.9161358203046606 time: 1703097860.915161\n",
      "batch_idx: 2 loss: 0.0015230318764723944 R2: 0.9161306188843238 time: 1703097863.576399\n",
      "batch_idx: 3 loss: 0.0008087971456990625 R2: 0.9161035822599087 time: 1703097866.118849\n",
      "Training [69%] Loss: 0.0011351395658995907 time: 1703097866.118849\n",
      "weight: [ 1.43772742  1.67704112  0.35432281  0.46756882  0.23959365  1.35737559\n",
      "  1.28388941  1.01348735  2.52021315 -0.14655388 -0.33358795  0.33057268]\n",
      "epoch 348\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014717166835949659 R2: 0.916130189375181 time: 1703097868.83283\n",
      "batch_idx: 1 loss: 0.0007364403589095468 R2: 0.9161411235121036 time: 1703097871.602081\n",
      "batch_idx: 2 loss: 0.0015231130251124757 R2: 0.9161358876411911 time: 1703097874.2475882\n",
      "batch_idx: 3 loss: 0.0008090055720592785 R2: 0.9161088535755526 time: 1703097876.8845043\n",
      "Training [70%] Loss: 0.0011350689099190667 time: 1703097876.8845043\n",
      "weight: [ 1.43809494  1.67691883  0.35442275  0.46684127  0.23886859  1.3579444\n",
      "  1.28405103  1.01414562  2.522241   -0.14654887 -0.33361956  0.33065117]\n",
      "epoch 349\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014713307114831698 R2: 0.9161354716127701 time: 1703097879.5990589\n",
      "batch_idx: 1 loss: 0.0007362613932161138 R2: 0.9161463587118991 time: 1703097882.2044039\n",
      "batch_idx: 2 loss: 0.0015231930516032357 R2: 0.9161410889694391 time: 1703097884.8239205\n",
      "batch_idx: 3 loss: 0.0008092115358923045 R2: 0.9161140580194796 time: 1703097887.492502\n",
      "Training [70%] Loss: 0.0011349991730487058 time: 1703097887.492502\n",
      "weight: [ 1.43845924  1.67679652  0.35452148  0.46611387  0.23814368  1.35851367\n",
      "  1.28421337  1.01480032  2.5242466  -0.14654399 -0.33365123  0.33072957]\n",
      "epoch 350\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014709492623455642 R2: 0.9161406865913764 time: 1703097890.113641\n",
      "batch_idx: 1 loss: 0.000736085064064929 R2: 0.9161515272654996 time: 1703097892.7484298\n",
      "batch_idx: 2 loss: 0.0015232719615003924 R2: 0.916146224221578 time: 1703097895.3874543\n",
      "batch_idx: 3 loss: 0.0008094150594233285 R2: 0.916119196933167 time: 1703097898.0857348\n",
      "Training [70%] Loss: 0.0011349303368335536 time: 1703097898.0857348\n",
      "weight: [ 1.43882035  1.6766742   0.35461902  0.46538662  0.23741891  1.35908341\n",
      "  1.28437643  1.0154515   2.5262302  -0.14653925 -0.33368296  0.33080788]\n",
      "epoch 351\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014705722711642027 R2: 0.9161458356517599 time: 1703097900.7062638\n",
      "batch_idx: 1 loss: 0.0007359113363490338 R2: 0.916156630504149 time: 1703097903.3896778\n",
      "batch_idx: 2 loss: 0.0015233497605548142 R2: 0.9161512947200746 time: 1703097906.0467973\n",
      "batch_idx: 3 loss: 0.0008096161648426176 R2: 0.9161242716282569 time: 1703097908.6778898\n",
      "Training [70%] Loss: 0.001134862383227667 time: 1703097908.6778898\n",
      "weight: [ 1.43917831  1.67655186  0.35471539  0.46465952  0.23669427  1.35965362\n",
      "  1.28454021  1.01609919  2.52819205 -0.14653463 -0.33371473  0.33088609]\n",
      "epoch 352\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014701996739872926 R2: 0.9161509201049902 time: 1703097911.3145394\n",
      "batch_idx: 1 loss: 0.0007357401753509568 R2: 0.9161616697295564 time: 1703097913.8779745\n",
      "batch_idx: 2 loss: 0.0015234264547031224 R2: 0.916156301757989 time: 1703097916.5251431\n",
      "batch_idx: 3 loss: 0.0008098148742993708 R2: 0.9161292833871976 time: 1703097919.1928596\n",
      "Training [70%] Loss: 0.0011347952945851857 time: 1703097919.1928596\n",
      "weight: [ 1.43953317  1.67642951  0.35481059  0.46393256  0.23596977  1.36022431\n",
      "  1.28470471  1.01674341  2.53013239 -0.14653014 -0.33374656  0.33096417]\n",
      "epoch 353\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014698314079103047 R2: 0.916155941233116 time: 1703097921.8135896\n",
      "batch_idx: 1 loss: 0.0007355715467414883 R2: 0.916166646214549 time: 1703097924.448522\n",
      "batch_idx: 2 loss: 0.0015235020500586256 R2: 0.9161612465996443 time: 1703097927.0840256\n",
      "batch_idx: 3 loss: 0.0008100112098958204 R2: 0.9161342334639077 time: 1703097929.7218285\n",
      "Training [71%] Loss: 0.0011347290536515597 time: 1703097929.7218285\n",
      "weight: [ 1.43988496  1.67630715  0.35490464  0.46320574  0.23524541  1.36079545\n",
      "  1.28486992  1.0173842   2.53205146 -0.14652578 -0.33377844  0.33104213]\n",
      "epoch 354\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014694674110574961 R2: 0.9161609002898089 time: 1703097932.3283813\n",
      "batch_idx: 1 loss: 0.0007354054165784145 R2: 0.9161715612037173 time: 1703097935.0421774\n",
      "batch_idx: 2 loss: 0.001523576552902427 R2: 0.9161661304812588 time: 1703097937.7102346\n",
      "batch_idx: 3 loss: 0.0008102051936815999 R2: 0.9161391230844057 time: 1703097940.3938391\n",
      "Training [71%] Loss: 0.0011346636435549842 time: 1703097940.3938391\n",
      "weight: [ 1.44023373  1.67618479  0.35499754  0.46247906  0.23452119  1.36136707\n",
      "  1.28503585  1.01802158  2.53394949 -0.14652154 -0.33381037  0.33111995]\n",
      "epoch 355\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014691076225636468 R2: 0.9161657985009695 time: 1703097943.121898\n",
      "batch_idx: 1 loss: 0.0007352417513050577 R2: 0.9161764159140388 time: 1703097945.8635561\n",
      "batch_idx: 2 loss: 0.0015236499696748452 R2: 0.9161709546115681 time: 1703097948.6150067\n",
      "batch_idx: 3 loss: 0.0008103968476483624 R2: 0.9161439534474219 time: 1703097951.13319\n",
      "Training [71%] Loss: 0.001134599047797978 time: 1703097951.13319\n",
      "weight: [ 1.44057952  1.67606244  0.35508931  0.46175252  0.23379709  1.36193916\n",
      "  1.28520248  1.01865559  2.53582673 -0.14651743 -0.33384236  0.33119762]\n",
      "epoch 356\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014687519825561626 R2: 0.9161706370653727 time: 1703097953.7948184\n",
      "batch_idx: 1 loss: 0.0007350805177487707 R2: 0.9161812115354889 time: 1703097956.3148944\n",
      "batch_idx: 2 loss: 0.0015237223069670153 R2: 0.9161757201724386 time: 1703097958.7546294\n",
      "batch_idx: 3 loss: 0.0008105861937246362 R2: 0.9161487257250241 time: 1703097961.3346283\n",
      "Training [71%] Loss: 0.0011345352502491462 time: 1703097961.3346283\n",
      "weight: [ 1.44092236  1.67594009  0.35517997  0.46102612  0.23307314  1.36251172\n",
      "  1.28536982  1.01928626  2.53768341 -0.14651344 -0.33387439  0.33127514]\n",
      "epoch 357\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001468400432137518 R2: 0.9161754171552573 time: 1703097963.8537755\n",
      "batch_idx: 1 loss: 0.0007349216831192199 R2: 0.9161859492316464 time: 1703097966.4601314\n",
      "batch_idx: 2 loss: 0.0015237935715128217 R2: 0.91618042831947 time: 1703097968.8899658\n",
      "batch_idx: 3 loss: 0.0008107732537708875 R2: 0.9161534410632015 time: 1703097971.3841944\n",
      "Training [71%] Loss: 0.0011344722351351118 time: 1703097971.3841944\n",
      "weight: [ 1.44126229  1.67581775  0.35526952  0.46029984  0.23234931  1.36308474\n",
      "  1.28553785  1.01991361  2.53951975 -0.14650957 -0.33390649  0.33135249]\n",
      "epoch 358\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014680529133679382 R2: 0.9161801399169052 time: 1703097973.8342614\n",
      "batch_idx: 1 loss: 0.0007347652150066455 R2: 0.9161906301402724 time: 1703097976.3299415\n",
      "batch_idx: 2 loss: 0.0015238637701810091 R2: 0.9161850801825769 time: 1703097978.8877108\n",
      "batch_idx: 3 loss: 0.0008109580495748432 R2: 0.9161581005824486 time: 1703097981.546665\n",
      "Training [72%] Loss: 0.001134409987032609 time: 1703097981.546665\n",
      "weight: [ 1.44159934  1.67569542  0.35535797  0.4595737   0.23162562  1.36365823\n",
      "  1.28570658  1.02053768  2.54133598 -0.14650581 -0.33393863  0.33142967]\n",
      "epoch 359\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001467709369248444 R2: 0.9161848064712486 time: 1703097984.190514\n",
      "batch_idx: 1 loss: 0.0007346110813799549 R2: 0.916195255373901 time: 1703097986.6797898\n",
      "batch_idx: 2 loss: 0.0015239329099675787 R2: 0.916189676866565 time: 1703097989.2797577\n",
      "batch_idx: 3 loss: 0.0008111406028469576 R2: 0.9161627053783313 time: 1703097991.865061\n",
      "Training [72%] Loss: 0.0011343484908607339 time: 1703097991.865061\n",
      "weight: [ 1.44193356  1.67557311  0.35544533  0.45884769  0.23090206  1.36423219\n",
      "  1.28587601  1.0211585   2.54313234 -0.14650218 -0.33397082  0.33150668]\n",
      "epoch 360\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001467369743704196 R2: 0.9161894179143953 time: 1703097994.5660079\n",
      "batch_idx: 1 loss: 0.0007344592505846984 R2: 0.9161998260203875 time: 1703097997.2145984\n",
      "batch_idx: 2 loss: 0.001524000997988446 R2: 0.9161942194516872 time: 1703097999.6951265\n",
      "batch_idx: 3 loss: 0.0008113209352161168 R2: 0.9161672565220631 time: 1703098002.2707648\n",
      "Training [72%] Loss: 0.0011342877318733643 time: 1703098002.2707648\n",
      "weight: [ 1.44226498  1.67545081  0.35553162  0.45812181  0.23017863  1.36480662\n",
      "  1.28604612  1.02177608  2.54490903 -0.14649866 -0.33400307  0.33158349]\n",
      "epoch 361\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014670339815681295 R2: 0.9161939753182166 time: 1703098004.858159\n",
      "batch_idx: 1 loss: 0.000734309691340991 R2: 0.916204343143464 time: 1703098007.4268253\n",
      "batch_idx: 2 loss: 0.001524068041472291 R2: 0.9161987089942013 time: 1703098009.8815718\n",
      "batch_idx: 3 loss: 0.00081149906822556 R2: 0.9161717550610241 time: 1703098012.4386816\n",
      "Training [72%] Loss: 0.0011342276956517428 time: 1703098012.4386816\n",
      "weight: [ 1.44259363  1.67532854  0.35561685  0.45739605  0.22945533  1.36538152\n",
      "  1.28621692  1.02239047  2.54666629 -0.14649526 -0.33403537  0.33166012]\n",
      "epoch 362\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014667020285648593 R2: 0.9161984797308745 time: 1703098015.0123317\n",
      "batch_idx: 1 loss: 0.0007341623727412923 R2: 0.9162088077832907 time: 1703098017.599894\n",
      "batch_idx: 2 loss: 0.0015241340477536946 R2: 0.916203146526897 time: 1703098020.4272888\n",
      "batch_idx: 3 loss: 0.0008116750233288959 R2: 0.9161762020193199 time: 1703098023.1401324\n",
      "Training [72%] Loss: 0.0011341683680971856 time: 1703098023.1401324\n",
      "weight: [ 1.44291954  1.6752063   0.35570103  0.45667042  0.22873217  1.36595688\n",
      "  1.28638839  1.02300169  2.54840433 -0.14649197 -0.33406772  0.33173655]\n",
      "epoch 363\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014663738312949636 R2: 0.9162029321773477 time: 1703098025.68336\n",
      "batch_idx: 1 loss: 0.000734017264248123 R2: 0.9162132209569605 time: 1703098028.1938837\n",
      "batch_idx: 2 loss: 0.0015241990242664666 R2: 0.9162075330596385 time: 1703098030.8353133\n",
      "batch_idx: 3 loss: 0.0008118488218864071 R2: 0.9161805983982847 time: 1703098033.2792547\n",
      "Training [73%] Loss: 0.0011341097354239903 time: 1703098033.2792547\n",
      "weight: [ 1.44324276  1.67508408  0.35578416  0.45594491  0.22800912  1.36653271\n",
      "  1.28656054  1.02360976  2.55012336 -0.14648879 -0.33410012  0.33181277]\n",
      "epoch 364\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014660493372194328 R2: 0.916207333659956 time: 1703098035.7098742\n",
      "batch_idx: 1 loss: 0.000733874335691661 R2: 0.9162175836590324 time: 1703098038.3015382\n",
      "batch_idx: 2 loss: 0.0015242629785372388 R2: 0.9162118695798538 time: 1703098040.895814\n",
      "batch_idx: 3 loss: 0.0008120204851614231 R2: 0.9161849451770084 time: 1703098043.5091794\n",
      "Training [73%] Loss: 0.001134051784152439 time: 1703098043.5091794\n",
      "weight: [ 1.44356331  1.67496189  0.35586625  0.45521953  0.22728621  1.367109\n",
      "  1.28673336  1.02421472  2.5518236  -0.14648572 -0.33413257  0.33188878]\n",
      "epoch 365\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001465728494644516 R2: 0.9162116851588691 time: 1703098046.1410193\n",
      "batch_idx: 1 loss: 0.0007337335572672955 R2: 0.9162218968620348 time: 1703098048.6896462\n",
      "batch_idx: 2 loss: 0.0015243259181792347 R2: 0.9162161570530619 time: 1703098051.2792332\n",
      "batch_idx: 3 loss: 0.0008121900343169575 R2: 0.9161892433128275 time: 1703098053.8978736\n",
      "Training [73%] Loss: 0.001133994501102001 time: 1703098053.8978736\n",
      "weight: [ 1.44388123  1.67483974  0.35594733  0.45449426  0.22656343  1.36768576\n",
      "  1.28690684  1.02481659  2.55350525 -0.14648276 -0.33416506  0.33196458]\n",
      "epoch 366\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014654112527067596 R2: 0.916215987632588 time: 1703098056.680486\n",
      "batch_idx: 1 loss: 0.0007335948995330885 R2: 0.916226161516955 time: 1703098059.3675213\n",
      "batch_idx: 2 loss: 0.0015243878508862667 R2: 0.916220396423354 time: 1703098062.0524333\n",
      "batch_idx: 3 loss: 0.0008123574904124245 R2: 0.9161934937418105 time: 1703098064.7102382\n",
      "Training [73%] Loss: 0.0011339378733846348 time: 1703098064.7102382\n",
      "weight: [ 1.44419655  1.67471763  0.35602739  0.45376911  0.22584077  1.36826298\n",
      "  1.28708097  1.02541539  2.55516853 -0.14647991 -0.33419761  0.33204017]\n",
      "epoch 367\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014650975613583834 R2: 0.9162202420184544 time: 1703098067.2852151\n",
      "batch_idx: 1 loss: 0.0007334583334071556 R2: 0.9162303785537287 time: 1703098069.9915721\n",
      "batch_idx: 2 loss: 0.001524448784426965 R2: 0.9162245886138718 time: 1703098072.5420892\n",
      "batch_idx: 3 loss: 0.0008125228744005625 R2: 0.9161976973792545 time: 1703098075.1975226\n",
      "Training [73%] Loss: 0.0011338818883982666 time: 1703098075.1975226\n",
      "weight: [ 1.44450929  1.67459555  0.35610644  0.45304408  0.22511824  1.36884066\n",
      "  1.28725576  1.02601117  2.55681364 -0.14647717 -0.3342302   0.33211552]\n",
      "epoch 368\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001464787371352877 R2: 0.9162244492331076 time: 1703098077.7993317\n",
      "batch_idx: 1 loss: 0.0007333238301650128 R2: 0.9162345488817023 time: 1703098080.5562294\n",
      "batch_idx: 2 loss: 0.0015245087266391416 R2: 0.916228734527294 time: 1703098083.1347709\n",
      "batch_idx: 3 loss: 0.0008126862071245083 R2: 0.9162018551201306 time: 1703098085.7988253\n",
      "Training [74%] Loss: 0.001133826533820385 time: 1703098085.7988253\n",
      "weight: [ 1.4448195   1.67447352  0.3561845   0.45231917  0.22439583  1.36941881\n",
      "  1.28743119  1.02660393  2.55844079 -0.14647453 -0.33426284  0.33219066]\n",
      "epoch 369\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001464480634230859 R2: 0.9162286101729519 time: 1703098088.5176203\n",
      "batch_idx: 1 loss: 0.0007331913614368388 R2: 0.9162386733901056 time: 1703098091.1160815\n",
      "batch_idx: 2 loss: 0.00152456768542441 R2: 0.916232835046278 time: 1703098093.7576838\n",
      "batch_idx: 3 loss: 0.0008128475093149874 R2: 0.9162059678395605 time: 1703098096.3019621\n",
      "Training [74%] Loss: 0.0011337717976017737 time: 1703098096.3019621\n",
      "weight: [ 1.44512719  1.67435153  0.35626158  0.45159437  0.22367355  1.36999742\n",
      "  1.28760727  1.02719371  2.56005017 -0.14647199 -0.33429553  0.33226556]\n",
      "epoch 370\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014641773023062422 R2: 0.9162327257146204 time: 1703098098.9836931\n",
      "batch_idx: 1 loss: 0.000733060899204702 R2: 0.9162427529485055 time: 1703098101.5848646\n",
      "batch_idx: 2 loss: 0.0015246256687429622 R2: 0.9162368910339266 time: 1703098104.2572663\n",
      "batch_idx: 3 loss: 0.0008130068015876834 R2: 0.9162100363932539 time: 1703098106.877991\n",
      "Training [74%] Loss: 0.0011337176679603973 time: 1703098106.877991\n",
      "weight: [ 1.44543241  1.67422959  0.35633768  0.45086969  0.2229514   1.37057649\n",
      "  1.28778398  1.02778054  2.56164199 -0.14646955 -0.33432826  0.33234023]\n",
      "epoch 371\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014638773286526068 R2: 0.9162367967154014 time: 1703098109.4208188\n",
      "batch_idx: 1 loss: 0.0007329324157997258 R2: 0.9162467884072415 time: 1703098112.134641\n",
      "batch_idx: 2 loss: 0.0015246826846084956 R2: 0.9162409033342189 time: 1703098114.750982\n",
      "batch_idx: 3 loss: 0.000813164104440749 R2: 0.9162140616179549 time: 1703098117.4401174\n",
      "Training [74%] Loss: 0.0011336641333753942 time: 1703098117.4401174\n",
      "weight: [ 1.44573517  1.6741077   0.35641282  0.45014512  0.22222937  1.37115602\n",
      "  1.28796131  1.02836444  2.56321643 -0.14646722 -0.33436104  0.33241466]\n",
      "epoch 372\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014635806670898182 R2: 0.9162408240136933 time: 1703098120.0916295\n",
      "batch_idx: 1 loss: 0.0007328058838992404 R2: 0.9162507805978632 time: 1703098122.7751937\n",
      "batch_idx: 2 loss: 0.001524738741083371 R2: 0.9162448727724511 time: 1703098125.4147859\n",
      "batch_idx: 3 loss: 0.0008133194382524243 R2: 0.9162180443318558 time: 1703098128.170109\n",
      "Training [74%] Loss: 0.0011336111825812135 time: 1703098128.170109\n",
      "weight: [ 1.44603552  1.67398586  0.35648699  0.44942066  0.22150746  1.37173601\n",
      "  1.28813927  1.02894543  2.5647737  -0.14646498 -0.33439386  0.33248885]\n",
      "epoch 373\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001463287272170917 R2: 0.9162448084294195 time: 1703098130.9707446\n",
      "batch_idx: 1 loss: 0.0007326812765238537 R2: 0.916254730333549 time: 1703098133.6028113\n",
      "batch_idx: 2 loss: 0.00152479384627389 R2: 0.9162488001556438 time: 1703098136.2871604\n",
      "batch_idx: 3 loss: 0.0008134728232788114 R2: 0.916221985335033 time: 1703098138.8976123\n",
      "Training [75%] Loss: 0.0011335588045618679 time: 1703098138.8976123\n",
      "weight: [ 1.44633347  1.67386407  0.35656022  0.44869631  0.22078568  1.37231646\n",
      "  1.28831784  1.02952354  2.56631399 -0.14646284 -0.33442672  0.3325628 ]\n",
      "epoch 374\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014629970991692383 R2: 0.9162487507644398 time: 1703098141.5598292\n",
      "batch_idx: 1 loss: 0.00073255856703452 R2: 0.916258638409532 time: 1703098144.2900786\n",
      "batch_idx: 2 loss: 0.001524848008325764 R2: 0.9162526862729752 time: 1703098147.0513077\n",
      "batch_idx: 3 loss: 0.0008136242796517772 R2: 0.916225885409849 time: 1703098149.7972896\n",
      "Training [75%] Loss: 0.0011335069885453249 time: 1703098149.7972896\n",
      "weight: [ 1.44662905  1.67374234  0.35663251  0.44797207  0.22006402  1.37289737\n",
      "  1.28849702  1.0300988   2.56783749 -0.1464608  -0.33445963  0.33263651]\n",
      "epoch 375\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001462710104065748 R2: 0.9162526518029722 time: 1703098152.4507651\n",
      "batch_idx: 1 loss: 0.0007324377291295679 R2: 0.9162625056034933 time: 1703098155.0694306\n",
      "batch_idx: 2 loss: 0.0015249012354197234 R2: 0.9162565318961663 time: 1703098157.753189\n",
      "batch_idx: 3 loss: 0.0008137738273769662 R2: 0.9162297453213519 time: 1703098160.411648\n",
      "Training [75%] Loss: 0.0011334557239980013 time: 1703098160.411648\n",
      "weight: [ 1.4469223   1.67362066  0.35670387  0.44724794  0.21934249  1.37347873\n",
      "  1.2886768   1.03067122  2.56934438 -0.14645886 -0.33449257  0.33270997]\n",
      "epoch 376\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014624262435366405 R2: 0.9162565123119745 time: 1703098163.0948741\n",
      "batch_idx: 1 loss: 0.0007323187368416985 R2: 0.9162663326759596 time: 1703098165.6670036\n",
      "batch_idx: 2 loss: 0.0015249535357672876 R2: 0.9162603377798806 time: 1703098168.4512074\n",
      "batch_idx: 3 loss: 0.0008139214863319602 R2: 0.9162335658176726 time: 1703098171.0945933\n",
      "Training [75%] Loss: 0.0011334050006193968 time: 1703098171.0945933\n",
      "weight: [ 1.44721323  1.67349905  0.3567743   0.44652392  0.21862108  1.37406054\n",
      "  1.28885718  1.03124084  2.57083486 -0.146457   -0.33452556  0.33278318]\n",
      "epoch 377\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014621454749411161 R2: 0.9162603330415365 time: 1703098173.7419722\n",
      "batch_idx: 1 loss: 0.0007322015645349629 R2: 0.9162701203706961 time: 1703098176.3949327\n",
      "batch_idx: 2 loss: 0.0015250049176066859 R2: 0.9162641046621193 time: 1703098179.010877\n",
      "batch_idx: 3 loss: 0.0008140672762645167 R2: 0.9162373476304012 time: 1703098182.0363722\n",
      "Training [75%] Loss: 0.0011333548083368203 time: 1703098182.0363722\n",
      "weight: [ 1.44750188  1.6733775   0.35684382  0.4458      0.21789979  1.37464282\n",
      "  1.28903815  1.03180768  2.5723091  -0.14645525 -0.33455859  0.33285614]\n",
      "epoch 378\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014618677563094757 R2: 0.9162641147252787 time: 1703098184.7824657\n",
      "batch_idx: 1 loss: 0.00073208618690171 R2: 0.9162738694150837 time: 1703098187.4238117\n",
      "batch_idx: 2 loss: 0.0015250553891989278 R2: 0.9162678332645898 time: 1703098190.0230799\n",
      "batch_idx: 3 loss: 0.0008142112167909505 R2: 0.9162410914749681 time: 1703098192.6801035\n",
      "Training [76%] Loss: 0.001133305137300266 time: 1703098192.6801035\n",
      "weight: [ 1.44778827  1.67325602  0.35691244  0.44507619  0.21717862  1.37522554\n",
      "  1.28921969  1.03237176  2.57376729 -0.14645358 -0.33459166  0.33292885]\n",
      "epoch 379\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014615930463312843 R2: 0.9162678580806928 time: 1703098195.2111027\n",
      "batch_idx: 1 loss: 0.0007319725789595322 R2: 0.9162775805204862 time: 1703098197.779669\n",
      "batch_idx: 2 loss: 0.001525104958823994 R2: 0.9162715242930709 time: 1703098200.4153132\n",
      "batch_idx: 3 loss: 0.0008143533273946253 R2: 0.9162447980510059 time: 1703098202.9659626\n",
      "Training [76%] Loss: 0.0011332559778773588 time: 1703098202.9659626\n",
      "weight: [ 1.44807243  1.6731346   0.35698016  0.44435249  0.21645758  1.37580872\n",
      "  1.28940182  1.03293311  2.5752096  -0.146452   -0.33462476  0.33300131]\n",
      "epoch 380\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001461321304343904 R2: 0.9162715638095319 time: 1703098205.5906532\n",
      "batch_idx: 1 loss: 0.0007318607160481734 R2: 0.9162812543826199 time: 1703098208.1381376\n",
      "batch_idx: 2 loss: 0.0015251536347771954 R2: 0.9162751784377855 time: 1703098210.8540146\n",
      "batch_idx: 3 loss: 0.0008144936274245109 R2: 0.9162484680427093 time: 1703098213.4601367\n",
      "Training [76%] Loss: 0.001133207320648446 time: 1703098213.4601367\n",
      "weight: [ 1.44835438  1.67301324  0.35704698  0.44362889  0.21573665  1.37639234\n",
      "  1.28958451  1.03349174  2.57663622 -0.14645051 -0.3346579   0.33307352]\n",
      "epoch 381\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014610524903211574 R2: 0.9162752325981588 time: 1703098216.1373343\n",
      "batch_idx: 1 loss: 0.000731750573826422 R2: 0.916284891681897 time: 1703098218.8287947\n",
      "batch_idx: 2 loss: 0.0015252014253656564 R2: 0.9162787963737399 time: 1703098221.3814898\n",
      "batch_idx: 3 loss: 0.0008146321360939021 R2: 0.916252102119182 time: 1703098224.0012085\n",
      "Training [76%] Loss: 0.0011331591564017844 time: 1703098224.0012085\n",
      "weight: [ 1.44863414  1.67289196  0.35711293  0.4429054   0.21501585  1.37697642\n",
      "  1.28976776  1.03404769  2.57804732 -0.14644911 -0.33469108  0.33314547]\n",
      "epoch 382\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001460786564862193 R2: 0.9162788651178826 time: 1703098226.6800904\n",
      "batch_idx: 1 loss: 0.0007316421282690378 R2: 0.9162884930837775 time: 1703098229.3768995\n",
      "batch_idx: 2 loss: 0.0015252483389049047 R2: 0.9162823787610822 time: 1703098232.0583532\n",
      "batch_idx: 3 loss: 0.0008147688724791851 R2: 0.9162557009347896 time: 1703098234.6306598\n",
      "Training [76%] Loss: 0.0011331114761288302 time: 1703098234.6306598\n",
      "weight: [ 1.44891175  1.67277075  0.357178    0.442182    0.21429517  1.37756095\n",
      "  1.28995157  1.03460098  2.57944308 -0.1464478  -0.3347243   0.33321716]\n",
      "epoch 383\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014605234891805837 R2: 0.9162824620253126 time: 1703098237.368077\n",
      "batch_idx: 1 loss: 0.0007315353556635815 R2: 0.9162920592391129 time: 1703098239.9378014\n",
      "batch_idx: 2 loss: 0.0015252943837156235 R2: 0.9162859262454314 time: 1703098242.6375625\n",
      "batch_idx: 3 loss: 0.0008149038555187175 R2: 0.9162592651294817 time: 1703098245.2578058\n",
      "Training [77%] Loss: 0.0011330642710196265 time: 1703098245.2578058\n",
      "weight: [ 1.44918722  1.67264961  0.35724221  0.44145872  0.21357461  1.37814592\n",
      "  1.29013592  1.03515162  2.58082366 -0.14644657 -0.33475754  0.3332886 ]\n",
      "epoch 384\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014602632250936387 R2: 0.9162860239626853 time: 1703098247.9024587\n",
      "batch_idx: 1 loss: 0.0007314302326073279 R2: 0.9162955907844716 time: 1703098250.5469923\n",
      "batch_idx: 2 loss: 0.0015253395681205147 R2: 0.9162894394582054 time: 1703098253.288355\n",
      "batch_idx: 3 loss: 0.0008150371040117974 R2: 0.9162627953291235 time: 1703098255.8842018\n",
      "Training [77%] Loss: 0.0011330175324583197 time: 1703098255.8842018\n",
      "weight: [ 1.44946059  1.67252855  0.35730555  0.44073553  0.21285418  1.37873134\n",
      "  1.29032082  1.03569964  2.58218923 -0.14644543 -0.33479083  0.33335978]\n",
      "epoch 385\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014600057350118638 R2: 0.9162895515581935 time: 1703098258.5656726\n",
      "batch_idx: 1 loss: 0.0007313267360041301 R2: 0.9162990883424568 time: 1703098261.2490292\n",
      "batch_idx: 2 loss: 0.0015253839004412573 R2: 0.9162929190169485 time: 1703098263.8366907\n",
      "batch_idx: 3 loss: 0.0008151686366177358 R2: 0.9162662921458203 time: 1703098266.6218038\n",
      "Training [77%] Loss: 0.0011329712520187468 time: 1703098266.6218038\n",
      "weight: [ 1.44973186  1.67240756  0.35736805  0.44001244  0.21213386  1.37931721\n",
      "  1.29050624  1.03624507  2.58353998 -0.14644437 -0.33482415  0.33343071]\n",
      "epoch 386\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014597509819286674 R2: 0.9162930454262922 time: 1703098269.345599\n",
      "batch_idx: 1 loss: 0.0007312248430612429 R2: 0.9163025525220393 time: 1703098272.045034\n",
      "batch_idx: 2 loss: 0.0015254273889956477 R2: 0.9162963655256439 time: 1703098274.7737315\n",
      "batch_idx: 3 loss: 0.0008152984718549581 R2: 0.9162697561782223 time: 1703098277.3809438\n",
      "Training [77%] Loss: 0.0011329254214601289 time: 1703098277.3809438\n",
      "weight: [ 1.45000107  1.67228665  0.3574297   0.43928946  0.21141366  1.37990352\n",
      "  1.29069219  1.03678792  2.58487605 -0.14644339 -0.3348575   0.33350137]\n",
      "epoch 387\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014594989294102296 R2: 0.9162965061680227 time: 1703098280.1567893\n",
      "batch_idx: 1 loss: 0.0007311245312862378 R2: 0.916305983918846 time: 1703098282.9783971\n",
      "batch_idx: 2 loss: 0.0015254700420947644 R2: 0.916299779575019 time: 1703098285.8387024\n",
      "batch_idx: 3 loss: 0.0008154266281002997 R2: 0.9162731880118337 time: 1703098288.4599712\n",
      "Training [77%] Loss: 0.0011328800327228827 time: 1703098288.4599712\n",
      "weight: [ 1.45026824  1.67216582  0.35749052  0.43856658  0.21069359  1.38049027\n",
      "  1.29087866  1.03732822  2.58619762 -0.14644249 -0.33489088  0.33357178]\n",
      "epoch 388\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014592495415855324 R2: 0.9162999343713082 time: 1703098291.0321808\n",
      "batch_idx: 1 loss: 0.0007310257784838307 R2: 0.9163093831154769 time: 1703098293.7150037\n",
      "batch_idx: 2 loss: 0.0015255118680403293 R2: 0.9163031617428542 time: 1703098296.2476883\n",
      "batch_idx: 3 loss: 0.0008155531235882207 R2: 0.9162765882193182 time: 1703098299.0046158\n",
      "Training [78%] Loss: 0.0011328350779244783 time: 1703098299.0046158\n",
      "weight: [ 1.45053339  1.67204507  0.35755051  0.43784379  0.20997363  1.38107746\n",
      "  1.29106565  1.03786598  2.58750484 -0.14644168 -0.33492429  0.33364194]\n",
      "epoch 389\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014590027831366584 R2: 0.9163033306112519 time: 1703098301.704017\n",
      "batch_idx: 1 loss: 0.0007309285627527305 R2: 0.9163127506818054 time: 1703098304.4049733\n",
      "batch_idx: 2 loss: 0.0015255528751221149 R2: 0.9163065125942669 time: 1703098307.1089473\n",
      "batch_idx: 3 loss: 0.000815677976410255 R2: 0.9162799573607773 time: 1703098309.7862666\n",
      "Training [78%] Loss: 0.0011327905493554397 time: 1703098309.7862666\n",
      "weight: [ 1.45079655  1.67192441  0.35760968  0.43712111  0.2092538   1.38166509\n",
      "  1.29125313  1.03840124  2.58879789 -0.14644094 -0.33495773  0.33371183]\n",
      "epoch 390\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014587586192891715 R2: 0.9163066954504352 time: 1703098312.5186894\n",
      "batch_idx: 1 loss: 0.000730832862482547 R2: 0.9163160871752452 time: 1703098315.168315\n",
      "batch_idx: 2 loss: 0.0015255930716154753 R2: 0.9163098326820084 time: 1703098317.8839524\n",
      "batch_idx: 3 loss: 0.0008158012045144305 R2: 0.9162832959840456 time: 1703098320.5052912\n",
      "Training [78%] Loss: 0.001132746439475406 time: 1703098320.5052912\n",
      "weight: [ 1.45105773  1.67180382  0.35766803  0.43639852  0.20853408  1.38225317\n",
      "  1.29144111  1.03893401  2.59007691 -0.14644028 -0.33499121  0.33378147]\n",
      "epoch 391\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014585170158027308 R2: 0.9163100294391867 time: 1703098323.1878037\n",
      "batch_idx: 1 loss: 0.0007307386563506186 R2: 0.9163193931410468 time: 1703098325.8397763\n",
      "batch_idx: 2 loss: 0.0015256324657789999 R2: 0.9163131225467381 time: 1703098328.383834\n",
      "batch_idx: 3 loss: 0.0008159228257047832 R2: 0.9162866046249712 time: 1703098331.0711489\n",
      "Training [78%] Loss: 0.0011327027409092831 time: 1703098331.0711489\n",
      "weight: [ 1.45131695  1.67168333  0.35772558  0.43567604  0.20781449  1.38284168\n",
      "  1.29162958  1.03946431  2.59134206 -0.14643969 -0.33502471  0.33385085]\n",
      "epoch 392\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014582779389618794 R2: 0.9163133331158662 time: 1703098333.7951472\n",
      "batch_idx: 1 loss: 0.0007306459233189079 R2: 0.9163226691125835 time: 1703098336.5221097\n",
      "batch_idx: 2 loss: 0.0015256710658522184 R2: 0.9163163827172978 time: 1703098339.0680487\n",
      "batch_idx: 3 loss: 0.0008160428576409978 R2: 0.9162898838076735 time: 1703098341.8376393\n",
      "Training [78%] Loss: 0.001132659446443501 time: 1703098341.8376393\n",
      "weight: [ 1.45157424  1.67156291  0.35778232  0.43495365  0.20709501  1.38343062\n",
      "  1.29181854  1.03999216  2.59259351 -0.14643919 -0.33505824  0.33391998]\n",
      "epoch 393\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014580413555669509 R2: 0.9163166070071453 time: 1703098344.4835267\n",
      "batch_idx: 1 loss: 0.0007305546426308815 R2: 0.9163259156116 time: 1703098347.1756887\n",
      "batch_idx: 2 loss: 0.0015257088800534639 R2: 0.9163196137109904 time: 1703098349.9429824\n",
      "batch_idx: 3 loss: 0.0008161613178379894 R2: 0.9162931340448303 time: 1703098352.6730597\n",
      "Training [79%] Loss: 0.0011326165490223214 time: 1703098352.6730597\n",
      "weight: [ 1.45182962  1.67144259  0.35783827  0.43423136  0.20637566  1.38402\n",
      "  1.29200796  1.04051759  2.5938314  -0.14643875 -0.33509179  0.33398885]\n",
      "epoch 394\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014578072329252444 R2: 0.9163198516282508 time: 1703098355.3746274\n",
      "batch_idx: 1 loss: 0.0007304647938083889 R2: 0.9163291331484882 time: 1703098358.0400996\n",
      "batch_idx: 2 loss: 0.0015257459165777493 R2: 0.9163228160338267 time: 1703098360.6467032\n",
      "batch_idx: 3 loss: 0.0008162782236656889 R2: 0.9162963558379242 time: 1703098363.2504647\n",
      "Training [79%] Loss: 0.0011325740417442679 time: 1703098363.2504647\n",
      "weight: [ 1.45208311  1.67132235  0.35789344  0.43350916  0.20565642  1.38460982\n",
      "  1.29219786  1.0410406   2.59505588 -0.14643839 -0.33512538  0.33405747]\n",
      "epoch 395\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001457575538842255 R2: 0.9163230674832473 time: 1703098365.955309\n",
      "batch_idx: 1 loss: 0.0007303763566485731 R2: 0.916332322222544 time: 1703098368.6337402\n",
      "batch_idx: 2 loss: 0.001525782183594804 R2: 0.9163259901807994 time: 1703098371.322474\n",
      "batch_idx: 3 loss: 0.0008163935923487982 R2: 0.9162995496775027 time: 1703098374.0980442\n",
      "Training [79%] Loss: 0.0011325319178586075 time: 1703098374.0980442\n",
      "weight: [ 1.45233472  1.67120221  0.35794782  0.43278707  0.20493731  1.38520007\n",
      "  1.29238821  1.04156124  2.59626711 -0.1464381  -0.33515899  0.33412583]\n",
      "epoch 396\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014573462416130997 R2: 0.9163262550652714 time: 1703098376.7583542\n",
      "batch_idx: 1 loss: 0.0007302893112207926 R2: 0.9163354833222142 time: 1703098379.4468925\n",
      "batch_idx: 2 loss: 0.0015258176892471427 R2: 0.9163291366361136 time: 1703098381.9877756\n",
      "batch_idx: 3 loss: 0.0008165074409666214 R2: 0.9163027160434186 time: 1703098384.7137551\n",
      "Training [79%] Loss: 0.001132490170761914 time: 1703098384.7137551\n",
      "weight: [ 1.45258449  1.67108215  0.35800143  0.43206507  0.20421832  1.38579074\n",
      "  1.29257902  1.0420795   2.59746524 -0.14643789 -0.33519263  0.33419394]\n",
      "epoch 397\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001457119310014179 R2: 0.9163294148567888 time: 1703098387.4310477\n",
      "batch_idx: 1 loss: 0.0007302036378635146 R2: 0.9163386169253501 time: 1703098390.2638173\n",
      "batch_idx: 2 loss: 0.0015258524416482777 R2: 0.9163322558734504 time: 1703098393.0984807\n",
      "batch_idx: 3 loss: 0.0008166197864529764 R2: 0.9163058554050899 time: 1703098395.8471675\n",
      "Training [79%] Loss: 0.001132448793994737 time: 1703098395.8471675\n",
      "weight: [ 1.45283242  1.67096219  0.35805428  0.43134316  0.20349944  1.38638185\n",
      "  1.29277027  1.04259542  2.5986504  -0.14643774 -0.33522629  0.33426179]\n",
      "epoch 398\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001456894713294862 R2: 0.9163325473298303 time: 1703098398.6074245\n",
      "batch_idx: 1 loss: 0.0007301193171812786 R2: 0.9163417234994421 time: 1703098401.1557784\n",
      "batch_idx: 2 loss: 0.001525886448880951 R2: 0.9163353483561941 time: 1703098403.769352\n",
      "batch_idx: 3 loss: 0.0008167306455961513 R2: 0.9163089682217181 time: 1703098406.382457\n",
      "Training [80%] Loss: 0.0011324077812383108 time: 1703098406.382457\n",
      "weight: [ 1.45307854  1.67084232  0.35810636  0.43062136  0.20278068  1.38697339\n",
      "  1.29296196  1.04310901  2.59982275 -0.14643766 -0.33525997  0.33432939]\n",
      "epoch 399\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014566724211694123 R2: 0.9163356529462388 time: 1703098409.0736713\n",
      "batch_idx: 1 loss: 0.0007300363300416275 R2: 0.9163448035018528 time: 1703098411.8154361\n",
      "batch_idx: 2 loss: 0.0015259197189954788 R2: 0.9163384145376721 time: 1703098414.460167\n",
      "batch_idx: 3 loss: 0.0008168400350388908 R2: 0.9163120549425322 time: 1703098417.1563084\n",
      "Training [80%] Loss: 0.0011323671263113524 time: 1703098417.1563084\n",
      "weight: [ 1.45332286  1.67072255  0.35815769  0.42989965  0.20206205  1.38756535\n",
      "  1.29315407  1.0436203   2.60098244 -0.14643765 -0.33529368  0.33439675]\n",
      "epoch 400\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014564524038090608 R2: 0.9163387321578821 time: 1703098419.871217\n",
      "batch_idx: 1 loss: 0.0007299546575720963 R2: 0.9163478573800591 time: 1703098422.6484032\n",
      "batch_idx: 2 loss: 0.0015259522600081605 R2: 0.9163414548613819 time: 1703098425.3477674\n",
      "batch_idx: 3 loss: 0.0008169479712784583 R2: 0.9163151160070131 time: 1703098427.952587\n",
      "Training [80%] Loss: 0.001132326823166944 time: 1703098427.952587\n",
      "weight: [ 1.45356541  1.67060287  0.35820828  0.42917803  0.20134353  1.38815774\n",
      "  1.29334662  1.04412929  2.60212959 -0.14643771 -0.33532741  0.33446385]\n",
      "epoch 401\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014562346318341985 R2: 0.9163417854068936 time: 1703098430.6727586\n",
      "batch_idx: 1 loss: 0.0007298742811571696 R2: 0.9163508855718574 time: 1703098433.3366508\n",
      "batch_idx: 2 loss: 0.0015259840798997838 R2: 0.9163444697612133 time: 1703098435.9548528\n",
      "batch_idx: 3 loss: 0.0008170544706667379 R2: 0.9163181518451132 time: 1703098438.6506455\n",
      "Training [80%] Loss: 0.0011322868658894725 time: 1703098438.6506455\n",
      "weight: [ 1.45380621  1.67048329  0.35825812  0.42845651  0.20062514  1.38875056\n",
      "  1.29353958  1.04463601  2.60326436 -0.14643784 -0.33536117  0.3345307 ]\n",
      "epoch 402\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014560190763066964 R2: 0.9163448131258877 time: 1703098441.2449548\n",
      "batch_idx: 1 loss: 0.0007297951824352803 R2: 0.9163538885055968 time: 1703098443.9276397\n",
      "batch_idx: 2 loss: 0.0015260151866141687 R2: 0.9163474596616649 time: 1703098446.7268538\n",
      "batch_idx: 3 loss: 0.0008171595494103906 R2: 0.9163211628774721 time: 1703098449.4839067\n",
      "Training [80%] Loss: 0.0011322472486916342 time: 1703098449.4839067\n",
      "weight: [ 1.45404527  1.6703638   0.35830723  0.42773508  0.19990686  1.38934379\n",
      "  1.29373294  1.04514049  2.60438688 -0.14643803 -0.33539494  0.33459731]\n",
      "epoch 403\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014558057087224138 R2: 0.9163478157381716 time: 1703098452.0559983\n",
      "batch_idx: 1 loss: 0.0007297173432958524 R2: 0.9163568666003951 time: 1703098454.693274\n",
      "batch_idx: 2 loss: 0.0015260455880567933 R2: 0.9163504249780626 time: 1703098457.2192776\n",
      "batch_idx: 3 loss: 0.0008172632235710298 R2: 0.9163241495156361 time: 1703098459.918618\n",
      "Training [81%] Loss: 0.0011322079659115225 time: 1703098459.918618\n",
      "weight: [ 1.4542826   1.67024442  0.3583556   0.42701375  0.1991887   1.38993745\n",
      "  1.29392671  1.04564272  2.60549729 -0.14643829 -0.33542874  0.33466367]\n",
      "epoch 404\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001455594501003832 R2: 0.9163507936579635 time: 1703098462.6921346\n",
      "batch_idx: 1 loss: 0.0007296407458762844 R2: 0.9163598202663351 time: 1703098465.4418569\n",
      "batch_idx: 2 loss: 0.0015260752920935245 R2: 0.9163533661167615 time: 1703098468.094253\n",
      "batch_idx: 3 loss: 0.0008173655090654736 R2: 0.9163271121622498 time: 1703098470.7320127\n",
      "Training [81%] Loss: 0.0011321690120097785 time: 1703098470.7320127\n",
      "weight: [ 1.45451824  1.67012513  0.35840326  0.42629251  0.19847066  1.39053153\n",
      "  1.29412087  1.04614275  2.60659572 -0.14643861 -0.33546256  0.33472978]\n",
      "epoch 405\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.00145538542549279 R2: 0.9163537472905908 time: 1703098473.4615407\n",
      "batch_idx: 1 loss: 0.0007295653725590469 R2: 0.9163627499046786 time: 1703098475.9882312\n",
      "batch_idx: 2 loss: 0.0015261043065493288 R2: 0.9163562834753612 time: 1703098478.5010943\n",
      "batch_idx: 3 loss: 0.0008174664216660177 R2: 0.9163300512112752 time: 1703098481.2539954\n",
      "Training [81%] Loss: 0.0011321303815667958 time: 1703098481.2539954\n",
      "weight: [ 1.45475219  1.67000594  0.35845019  0.42557137  0.19775274  1.39112603\n",
      "  1.29431542  1.04664057  2.60768231 -0.14643899 -0.3354964   0.33479565]\n",
      "epoch 406\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014551784549433904 R2: 0.9163566770326937 time: 1703098483.943781\n",
      "batch_idx: 1 loss: 0.0007294912059687354 R2: 0.9163656559080714 time: 1703098486.64681\n",
      "batch_idx: 2 loss: 0.00152613263920714 R2: 0.9163591774428866 time: 1703098489.233021\n",
      "batch_idx: 3 loss: 0.0008175659770007374 R2: 0.916332967048176 time: 1703098491.9174545\n",
      "Training [81%] Loss: 0.0011320920692800007 time: 1703098491.9174545\n",
      "weight: [ 1.45498447  1.66988686  0.35849642  0.42485032  0.19703495  1.39172095\n",
      "  1.29451034  1.04713622  2.6087572  -0.14643943 -0.33553026  0.33486128]\n",
      "epoch 407\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014549735625150415 R2: 0.9163595832724194 time: 1703098494.5853088\n",
      "batch_idx: 1 loss: 0.0007294182289691382 R2: 0.9163685386607213 time: 1703098497.2685456\n",
      "batch_idx: 2 loss: 0.0015261602978067419 R2: 0.916362048400002 time: 1703098499.967385\n",
      "batch_idx: 3 loss: 0.0008176641905538667 R2: 0.916335860050116 time: 1703098502.539835\n",
      "Training [81%] Loss: 0.0011320540699611972 time: 1703098502.539835\n",
      "weight: [ 1.45521511  1.66976788  0.35854194  0.42412936  0.19631727  1.39231628\n",
      "  1.29470564  1.04762971  2.6098205  -0.14643994 -0.33556413  0.33492667]\n",
      "epoch 408\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014547707217655798 R2: 0.9163624663896236 time: 1703098505.2246299\n",
      "batch_idx: 1 loss: 0.0007293464246603884 R2: 0.9163713985386094 time: 1703098507.8769586\n",
      "batch_idx: 2 loss: 0.001526187290043674 R2: 0.9163648967191783 time: 1703098510.637967\n",
      "batch_idx: 3 loss: 0.0008177610776661728 R2: 0.916338730586148 time: 1703098513.2633307\n",
      "Training [82%] Loss: 0.0011320163785339538 time: 1703098513.2633307\n",
      "weight: [ 1.45544411  1.669649    0.35858677  0.4234085   0.19559971  1.39291203\n",
      "  1.29490131  1.04812105  2.61087237 -0.1464405  -0.33559803  0.33499181]\n",
      "epoch 409\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014545699066445714 R2: 0.9163653267560445 time: 1703098515.8287742\n",
      "batch_idx: 1 loss: 0.0007292757763760533 R2: 0.9163742359096577 time: 1703098518.4975278\n",
      "batch_idx: 2 loss: 0.0015262136235682986 R2: 0.9163677227649034 time: 1703098521.1089554\n",
      "batch_idx: 3 loss: 0.0008178566535353715 R2: 0.9163415790173989 time: 1703098523.8364918\n",
      "Training [82%] Loss: 0.0011319789900310737 time: 1703098523.8364918\n",
      "weight: [ 1.45567149  1.66953022  0.35863089  0.42268773  0.19488227  1.39350819\n",
      "  1.29509733  1.04861027  2.61191291 -0.14644113 -0.33563194  0.33505672]\n",
      "epoch 410\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001454371091486703 R2: 0.9163681647354893 time: 1703098526.475295\n",
      "batch_idx: 1 loss: 0.0007292062676803036 R2: 0.9163770511339291 time: 1703098529.1555\n",
      "batch_idx: 2 loss: 0.0015262393059848013 R2: 0.9163705268938477 time: 1703098531.793325\n",
      "batch_idx: 3 loss: 0.0008179509332166237 R2: 0.916344405697244 time: 1703098534.443285\n",
      "Training [82%] Loss: 0.0011319418995921079 time: 1703098534.443285\n",
      "weight: [ 1.45589727  1.66941155  0.35867434  0.42196705  0.19416495  1.39410476\n",
      "  1.2952937   1.04909737  2.61294226 -0.14644181 -0.33566587  0.33512139]\n",
      "epoch 411\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014541742510053232 R2: 0.9163709806840276 time: 1703098537.175073\n",
      "batch_idx: 1 loss: 0.0007291378823651063 R2: 0.9163798445637863 time: 1703098539.8404245\n",
      "batch_idx: 2 loss: 0.0015262643448503434 R2: 0.9163733094550409 time: 1703098542.605217\n",
      "batch_idx: 3 loss: 0.0008180439316229415 R2: 0.9163472109714818 time: 1703098545.2394798\n",
      "Training [82%] Loss: 0.0011319051024609285 time: 1703098545.2394798\n",
      "weight: [ 1.45612147  1.66929298  0.35871709  0.42124647  0.19344774  1.39470174\n",
      "  1.29549042  1.04958239  2.61396055 -0.14644255 -0.33569982  0.33518582]\n",
      "epoch 412\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001453979360286062 R2: 0.9163737749501403 time: 1703098547.9986439\n",
      "batch_idx: 1 loss: 0.0007290706044473902 R2: 0.916382616544084 time: 1703098550.6088874\n",
      "batch_idx: 2 loss: 0.0015262887476742083 R2: 0.9163760707900567 time: 1703098553.2538595\n",
      "batch_idx: 3 loss: 0.0008181356635258817 R2: 0.9163499951785214 time: 1703098555.912695\n",
      "Training [82%] Loss: 0.0011318685939833855 time: 1703098555.912695\n",
      "weight: [ 1.45634411  1.66917453  0.35875918  0.42052598  0.19273066  1.39529914\n",
      "  1.29568747  1.05006532  2.61496789 -0.14644335 -0.33573379  0.33525002]\n",
      "epoch 413\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014537863947806213 R2: 0.9163765478749168 time: 1703098558.5838015\n",
      "batch_idx: 1 loss: 0.0007290044181662359 R2: 0.9163853674123315 time: 1703098561.3780081\n",
      "batch_idx: 2 loss: 0.0015263125219170117 R2: 0.9163788112331673 time: 1703098564.008793\n",
      "batch_idx: 3 loss: 0.000818226143555709 R2: 0.9163527586495285 time: 1703098566.6457753\n",
      "Training [83%] Loss: 0.0011318323696048944 time: 1703098566.6457753\n",
      "weight: [ 1.45656519  1.66905617  0.35880059  0.41980558  0.1920137   1.39589694\n",
      "  1.29588485  1.0505462   2.61596442 -0.14644421 -0.33576777  0.33531399]\n",
      "epoch 414\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014535953303005992 R2: 0.9163792997921971 time: 1703098569.2626395\n",
      "batch_idx: 1 loss: 0.0007289393079803073 R2: 0.9163880974988515 time: 1703098571.8908567\n",
      "batch_idx: 2 loss: 0.0015263356749899957 R2: 0.9163815311115165 time: 1703098574.6014116\n",
      "batch_idx: 3 loss: 0.0008183153862028805 R2: 0.9163555017086 time: 1703098577.3178723\n",
      "Training [83%] Loss: 0.0011317964248684456 time: 1703098577.3178723\n",
      "weight: [ 1.45678473  1.66893793  0.35884133  0.41908528  0.19129686  1.39649515\n",
      "  1.29608256  1.05102504  2.61695025 -0.14644512 -0.33580177  0.33537773]\n",
      "epoch 415\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014534061430116377 R2: 0.9163820310287603 time: 1703098580.068199\n",
      "batch_idx: 1 loss: 0.0007288752585645289 R2: 0.9163908071269542 time: 1703098582.671691\n",
      "batch_idx: 2 loss: 0.0015263582142542626 R2: 0.9163842307452935 time: 1703098585.3846607\n",
      "batch_idx: 3 loss: 0.000818403405816392 R2: 0.9163582246729322 time: 1703098588.0198915\n",
      "Training [83%] Loss: 0.0011317607554117053 time: 1703098588.0198915\n",
      "weight: [ 1.45700276  1.66881979  0.35888142  0.41836506  0.19058014  1.39709376\n",
      "  1.29628058  1.05150185  2.6179255  -0.14644608 -0.33583578  0.33544123]\n",
      "epoch 416\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014532188094270031 R2: 0.9163847419044654 time: 1703098590.734887\n",
      "batch_idx: 1 loss: 0.0007288122548089447 R2: 0.9163934966130821 time: 1703098593.4650583\n",
      "batch_idx: 2 loss: 0.0015263801470203335 R2: 0.916386910447828 time: 1703098596.0287614\n",
      "batch_idx: 3 loss: 0.0008184902166102652 R2: 0.9163609278529508 time: 1703098598.7213857\n",
      "Training [83%] Loss: 0.0011317253569666367 time: 1703098598.7213857\n",
      "weight: [ 1.45721928  1.66870177  0.35892085  0.41764494  0.18986353  1.39769279\n",
      "  1.2964789   1.05197665  2.6188903  -0.1464471  -0.33586981  0.33550451]\n",
      "epoch 417\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014530333064033062 R2: 0.9163874327324081 time: 1703098601.4576619\n",
      "batch_idx: 1 loss: 0.0007287502818118223 R2: 0.9163961662670121 time: 1703098604.1826038\n",
      "batch_idx: 2 loss: 0.0015264014805469383 R2: 0.9163895705259282 time: 1703098606.8585877\n",
      "batch_idx: 3 loss: 0.0008185758326483431 R2: 0.9163636115525137 time: 1703098609.4401774\n",
      "Training [83%] Loss: 0.0011316902253526025 time: 1703098609.4401774\n",
      "weight: [ 1.4574343   1.66858385  0.35895963  0.41692491  0.18914705  1.3982922\n",
      "  1.29667753  1.05244945  2.61984475 -0.14644817 -0.33590385  0.33556756]\n",
      "epoch 418\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001452849611130411 R2: 0.9163901038190888 time: 1703098612.1860738\n",
      "batch_idx: 1 loss: 0.0007286893248885795 R2: 0.9163988163918237 time: 1703098614.798347\n",
      "batch_idx: 2 loss: 0.001526422222042414 R2: 0.9163922112795374 time: 1703098617.491029\n",
      "batch_idx: 3 loss: 0.0008186602678894571 R2: 0.9163662760689574 time: 1703098620.0488307\n",
      "Training [84%] Loss: 0.0011316553564877153 time: 1703098620.0488307\n",
      "weight: [ 1.45764786  1.66846605  0.35899777  0.41620497  0.18843068  1.39889206\n",
      "  1.29687646  1.05292027  2.62078898 -0.14644929 -0.33593791  0.33563039]\n",
      "epoch 419\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014526677011389105 R2: 0.9163927554645476 time: 1703098622.6684828\n",
      "batch_idx: 1 loss: 0.0007286293695361501 R2: 0.9164014472846558 time: 1703098625.3071969\n",
      "batch_idx: 2 loss: 0.0015264423786577191 R2: 0.9163948330032664 time: 1703098627.9420516\n",
      "batch_idx: 3 loss: 0.0008187435360590469 R2: 0.9163689216935132 time: 1703098630.6160493\n",
      "Training [84%] Loss: 0.0011316207463479568 time: 1703098630.6160493\n",
      "weight: [ 1.45785995  1.66834835  0.35903527  0.41548513  0.18771444  1.39949218\n",
      "  1.29707567  1.05338914  2.6217231  -0.14645046 -0.33597198  0.33569299]\n",
      "epoch 420\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014524875542549858 R2: 0.9163953879624612 time: 1703098633.3656545\n",
      "batch_idx: 1 loss: 0.0007285704015270743 R2: 0.9164040592349225 time: 1703098636.0146108\n",
      "batch_idx: 2 loss: 0.0015264619575076487 R2: 0.9163974359824042 time: 1703098638.6290526\n",
      "batch_idx: 3 loss: 0.0008188256510362526 R2: 0.9163715487105909 time: 1703098641.3828828\n",
      "Training [84%] Loss: 0.0011315863910814903 time: 1703098641.3828828\n",
      "weight: [ 1.45807059  1.66823077  0.35907213  0.41476537  0.18699831  1.40009312\n",
      "  1.29727516  1.05385605  2.62264723 -0.14645169 -0.33600607  0.33575537]\n",
      "epoch 421\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014523091487184507 R2: 0.9163980016005929 time: 1703098643.99633\n",
      "batch_idx: 1 loss: 0.0007285124066150532 R2: 0.9164066525305825 time: 1703098646.7106142\n",
      "batch_idx: 2 loss: 0.0015264809655966321 R2: 0.9164000205058696 time: 1703098649.3306835\n",
      "batch_idx: 3 loss: 0.0008189066256614754 R2: 0.9163741574008035 time: 1703098651.9834266\n",
      "Training [84%] Loss: 0.0011315522866479028 time: 1703098651.9834266\n",
      "weight: [ 1.45827981  1.6681133   0.35910837  0.41404571  0.1862823   1.40069311\n",
      "  1.29747492  1.05432103  2.62356146 -0.14645296 -0.33604016  0.33581753]\n",
      "epoch 422\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001452132462782582 R2: 0.9164005966596088 time: 1703098654.776654\n",
      "batch_idx: 1 loss: 0.0007284553714384522 R2: 0.916409227438398 time: 1703098657.2573922\n",
      "batch_idx: 2 loss: 0.0015264994100738377 R2: 0.9164025868254647 time: 1703098659.875413\n",
      "batch_idx: 3 loss: 0.0008189864755247607 R2: 0.9163767480309998 time: 1703098662.468241\n",
      "Training [84%] Loss: 0.0011315184299549082 time: 1703098662.468241\n",
      "weight: [ 1.4584876   1.66799594  0.35914398  0.41332613  0.18556643  1.40129798\n",
      "  1.29767496  1.05478409  2.62446593 -0.14645428 -0.33607427  0.33587948]\n",
      "epoch 423\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001451957475997636 R2: 0.9164031734184735 time: 1703098665.005325\n",
      "batch_idx: 1 loss: 0.0007283992806199903 R2: 0.9164117842702805 time: 1703098667.6623158\n",
      "batch_idx: 2 loss: 0.001526517297333623 R2: 0.9164051352903743 time: 1703098670.316378\n",
      "batch_idx: 3 loss: 0.000819065206486231 R2: 0.9163793208884672 time: 1703098673.0130832\n",
      "Training [85%] Loss: 0.00113148481510937 time: 1703098673.0130832\n",
      "weight: [ 1.45869399  1.6678787   0.35917897  0.41260666  0.18485061  1.40188798\n",
      "  1.29787525  1.05524526  2.62536073 -0.14645566 -0.3361084   0.3359412 ]\n",
      "epoch 424\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014517841648009756 R2: 0.9164057321335536 time: 1703098675.5566914\n",
      "batch_idx: 1 loss: 0.0007283441262311646 R2: 0.9164143231565178 time: 1703098678.0830245\n",
      "batch_idx: 2 loss: 0.0015265346362913676 R2: 0.9164076658942962 time: 1703098680.6575632\n",
      "batch_idx: 3 loss: 0.0008191428556944032 R2: 0.9163818761381618 time: 1703098683.2427802\n",
      "Training [85%] Loss: 0.0011314514457544777 time: 1703098683.2427802\n",
      "weight: [ 1.45889899  1.66776158  0.35921335  0.41188724  0.18413513  1.40253177\n",
      "  1.2980758   1.05570453  2.62624598 -0.14645707 -0.33614253  0.33600271]\n",
      "epoch 425\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014516125168039424 R2: 0.9164082731176807 time: 1703098685.9152253\n",
      "batch_idx: 1 loss: 0.0007282898751277249 R2: 0.9164168448431834 time: 1703098688.5639424\n",
      "batch_idx: 2 loss: 0.001526551426806333 R2: 0.9164101798634394 time: 1703098691.356242\n",
      "batch_idx: 3 loss: 0.0008192193357541233 R2: 0.9163844140794974 time: 1703098693.9608977\n",
      "Training [85%] Loss: 0.001131418288623031 time: 1703098693.9608977\n",
      "weight: [ 1.45910262  1.66764456  0.35924712  0.41116805  0.18341899  1.40298422\n",
      "  1.2982766   1.05616193  2.62712178 -0.14645854 -0.33617668  0.33606401]\n",
      "epoch 426\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014514424681562159 R2: 0.9164107963747817 time: 1703098696.6127782\n",
      "batch_idx: 1 loss: 0.0007282365709194382 R2: 0.9164193477900128 time: 1703098699.2027216\n",
      "batch_idx: 2 loss: 0.0015265677103764841 R2: 0.9164126741920169 time: 1703098701.8394167\n",
      "batch_idx: 3 loss: 0.0008192947937178648 R2: 0.9163869303492744 time: 1703098704.3773613\n",
      "Training [85%] Loss: 0.0011313853857925008 time: 1703098704.3773613\n",
      "weight: [ 1.45930488  1.66752766  0.35928029  0.41044844  0.18270585  1.40414543\n",
      "  1.29847764  1.05661747  2.62798824 -0.14646005 -0.33621084  0.3361251 ]\n",
      "epoch 427\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001451273964741554 R2: 0.9164133023472377 time: 1703098707.025399\n",
      "batch_idx: 1 loss: 0.0007281838139939054 R2: 0.9164218386394086 time: 1703098709.717501\n",
      "batch_idx: 2 loss: 0.001526583474332269 R2: 0.9164151645195571 time: 1703098712.3198524\n",
      "batch_idx: 3 loss: 0.0008193655265703898 R2: 0.9163893801293593 time: 1703098714.9602606\n",
      "Training [85%] Loss: 0.0011313516949095295 time: 1703098714.9602606\n",
      "weight: [ 1.45950572  1.66741082  0.3593128   0.40973088  0.18198213  1.40266083\n",
      "  1.29867896  1.05707122  2.62884541 -0.14646158 -0.33624504  0.33618601]\n",
      "epoch 428\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014511040759628312 R2: 0.9164157777137897 time: 1703098717.5354407\n",
      "batch_idx: 1 loss: 0.0007281304173521661 R2: 0.916424288979575 time: 1703098720.160575\n",
      "batch_idx: 2 loss: 0.001526600504024177 R2: 0.9164176417228435 time: 1703098722.7742069\n",
      "batch_idx: 3 loss: 0.0008194023168221949 R2: 0.9163910927334008 time: 1703098725.3632727\n",
      "Training [86%] Loss: 0.0011313093285403423 time: 1703098725.3632727\n",
      "weight: [ 1.4597043   1.66729323  0.35934384  0.40900698  0.1813002   1.41100937\n",
      "  1.29888134  1.05752395  2.62969274 -0.14646275 -0.33627963  0.33624731]\n",
      "epoch 429\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001450919540229155 R2: 0.9164181588087233 time: 1703098727.9531405\n",
      "batch_idx: 1 loss: 0.0007281130334358821 R2: 0.9164267731290241 time: 1703098730.5254698\n",
      "batch_idx: 2 loss: 0.0015266006163226218 R2: 0.9164200118017571 time: 1703098733.2621856\n",
      "batch_idx: 3 loss: 0.0008194636772400118 R2: 0.9163933929608561 time: 1703098735.8825345\n",
      "Training [86%] Loss: 0.0011312742168069177 time: 1703098735.8825345\n",
      "weight: [ 1.4598981   1.66717249  0.35937105  0.40830628  0.18050822  1.39821094\n",
      "  1.29908698  1.05797786  2.6305285  -0.14646272 -0.33631537  0.33631093]\n",
      "epoch 430\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001450736569453355 R2: 0.9164205659892893 time: 1703098738.449808\n",
      "batch_idx: 1 loss: 0.0007280890734632452 R2: 0.9164292028719958 time: 1703098741.2128954\n",
      "batch_idx: 2 loss: 0.0015266007264539464 R2: 0.9164221118186774 time: 1703098743.7917335\n",
      "batch_idx: 3 loss: 0.0008196248852868982 R2: 0.9163974642701842 time: 1703098746.5083425\n",
      "Training [86%] Loss: 0.0011312628136643612 time: 1703098746.5083425\n",
      "weight: [ 1.46009426  1.66705525  0.35940105  0.40757915  0.17984578  1.4054165\n",
      "  1.2992896   1.0584269   2.63135856 -0.14646514 -0.33634872  0.33637216]\n",
      "epoch 431\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014505959190970702 R2: 0.9164230576469651 time: 1703098749.161266\n",
      "batch_idx: 1 loss: 0.000727953590931911 R2: 0.9164315863977051 time: 1703098751.8358045\n",
      "batch_idx: 2 loss: 0.0015266739854862144 R2: 0.9164251945817969 time: 1703098754.4752407\n",
      "batch_idx: 3 loss: 0.0008195763057863523 R2: 0.9163979948970663 time: 1703098757.1097007\n",
      "Training [86%] Loss: 0.001131199950325387 time: 1703098757.1097007\n",
      "weight: [ 1.46030178  1.66694996  0.35944228  0.4068417   0.17916995  1.4123924\n",
      "  1.29948125  1.05886327  2.6321898  -0.1464731  -0.33637693  0.33642387]\n",
      "epoch 432\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014505315088476829 R2: 0.9164258200297597 time: 1703098759.854561\n",
      "batch_idx: 1 loss: 0.0007279183388269549 R2: 0.9164339736457382 time: 1703098762.555632\n",
      "batch_idx: 2 loss: 0.0015266505358150387 R2: 0.9164271181556375 time: 1703098765.1943934\n",
      "batch_idx: 3 loss: 0.000819766237470162 R2: 0.9164023303272776 time: 1703098767.7802954\n",
      "Training [86%] Loss: 0.0011312166552399595 time: 1703098767.7802954\n",
      "weight: [ 1.46048383  1.66682223  0.35946043  0.4061382   0.17844091  1.40970343\n",
      "  1.29969454  1.05931857  2.63299251 -0.14646899 -0.33641689  0.33649104]\n",
      "epoch 433\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014502162484845095 R2: 0.9164276304666984 time: 1703098770.402006\n",
      "batch_idx: 1 loss: 0.0007279271724933728 R2: 0.9164363991802714 time: 1703098773.022446\n",
      "batch_idx: 2 loss: 0.0015266652116858294 R2: 0.9164295861752748 time: 1703098775.5643454\n",
      "batch_idx: 3 loss: 0.0008197781850437553 R2: 0.916403953364674 time: 1703098778.215774\n",
      "Training [87%] Loss: 0.0011311467044268669 time: 1703098778.215774\n",
      "weight: [ 1.46068368  1.66671244  0.35949577  0.40541991  0.17768714  1.40624885\n",
      "  1.2998911   1.05975573  2.63380208 -0.14647377 -0.33644839  0.3365452 ]\n",
      "epoch 434\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014501835025736488 R2: 0.9164305699622034 time: 1703098780.821071\n",
      "batch_idx: 1 loss: 0.0007277983629630753 R2: 0.9164387986697585 time: 1703098783.4897947\n",
      "batch_idx: 2 loss: 0.0015266999916988089 R2: 0.9164322697949625 time: 1703098786.1095207\n",
      "batch_idx: 3 loss: 0.0008198365634271272 R2: 0.9164062035143619 time: 1703098788.8077743\n",
      "Training [87%] Loss: 0.001131129605165665 time: 1703098788.8077743\n",
      "weight: [ 1.46087745  1.66659828  0.35952608  0.4046992   0.17697521  1.40703609\n",
      "  1.30009215  1.06019526  2.63459903 -0.14647604 -0.33648236  0.33660268]\n",
      "epoch 435\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014499861450622113 R2: 0.9164327699515319 time: 1703098791.4444811\n",
      "batch_idx: 1 loss: 0.0007278100402916674 R2: 0.9164411344349285 time: 1703098794.1443293\n",
      "batch_idx: 2 loss: 0.0015266829421038438 R2: 0.9164342239871756 time: 1703098796.801215\n",
      "batch_idx: 3 loss: 0.000819953736413713 R2: 0.91640928279836 time: 1703098799.4484231\n",
      "Training [87%] Loss: 0.0011311082159678589 time: 1703098799.4484231\n",
      "weight: [ 1.46106306  1.66647781  0.3595494   0.40398042  0.17629252  1.41052663\n",
      "  1.30029948  1.06063895  2.63538192 -0.14647492 -0.33651959  0.33666517]\n",
      "epoch 436\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014497979251249502 R2: 0.9164349663090106 time: 1703098802.1311524\n",
      "batch_idx: 1 loss: 0.0007277560145090357 R2: 0.9164434567223007 time: 1703098804.6733851\n",
      "batch_idx: 2 loss: 0.0015267090210944244 R2: 0.9164367576469725 time: 1703098807.3114998\n",
      "batch_idx: 3 loss: 0.0008199768924944085 R2: 0.9164109962610494 time: 1703098809.979789\n",
      "Training [87%] Loss: 0.0011310599633057045 time: 1703098809.979789\n",
      "weight: [ 1.46125832  1.66636763  0.3595823   0.40325528  0.17558404  1.4121409\n",
      "  1.30049733  1.06107157  2.63616531 -0.14647911 -0.33655171  0.3367208 ]\n",
      "epoch 437\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001449708555686656 R2: 0.9164375707902634 time: 1703098812.6310234\n",
      "batch_idx: 1 loss: 0.0007276856275730748 R2: 0.9164457913124563 time: 1703098815.2677498\n",
      "batch_idx: 2 loss: 0.0015267205386767563 R2: 0.9164391177391581 time: 1703098817.889252\n",
      "batch_idx: 3 loss: 0.0008200573814376909 R2: 0.916413589406672 time: 1703098820.5089242\n",
      "Training [87%] Loss: 0.0011310430258435445 time: 1703098820.5089242\n",
      "weight: [ 1.46144515  1.66625085  0.35960794  0.40254001  0.17486097  1.41131097\n",
      "  1.30070175  1.06150865  2.63693455 -0.1464799  -0.33658711  0.3367815 ]\n",
      "epoch 438\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014495133356429366 R2: 0.9164397403359509 time: 1703098823.0656743\n",
      "batch_idx: 1 loss: 0.0007276757730619386 R2: 0.9164481138698252 time: 1703098825.703601\n",
      "batch_idx: 2 loss: 0.0015267171345449432 R2: 0.916441282162277 time: 1703098828.3249366\n",
      "batch_idx: 3 loss: 0.0008201325053811984 R2: 0.9164160844249667 time: 1703098831.0614302\n",
      "Training [88%] Loss: 0.0011310096871577542 time: 1703098831.0614302\n",
      "weight: [ 1.46163061  1.66613406  0.3596329   0.40182466  0.1741398   1.41067527\n",
      "  1.30090644  1.06194415  2.63769553 -0.14648078 -0.33662244  0.3368423 ]\n",
      "epoch 439\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014493622550157167 R2: 0.9164420620148472 time: 1703098833.6758533\n",
      "batch_idx: 1 loss: 0.0007276217760122286 R2: 0.9164504166601816 time: 1703098836.2844145\n",
      "batch_idx: 2 loss: 0.0015267352312182613 R2: 0.9164436864494597 time: 1703098838.946511\n",
      "batch_idx: 3 loss: 0.0008201780731887981 R2: 0.9164181519829524 time: 1703098841.5992897\n",
      "Training [88%] Loss: 0.0011309743338587513 time: 1703098841.5992897\n",
      "weight: [ 1.46181909  1.66602133  0.35966124  0.40110331  0.17342931  1.4117117\n",
      "  1.30110755  1.06237435  2.63845191 -0.14648392 -0.33665562  0.33689999]\n",
      "epoch 440\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014492372958442835 R2: 0.9164444564691225 time: 1703098844.3111706\n",
      "batch_idx: 1 loss: 0.0007275716259680773 R2: 0.9164526937485672 time: 1703098846.90336\n",
      "batch_idx: 2 loss: 0.0015267444473068428 R2: 0.9164459492493272 time: 1703098849.5341103\n",
      "batch_idx: 3 loss: 0.0008202484611040761 R2: 0.9164205678863764 time: 1703098852.1610785\n",
      "Training [88%] Loss: 0.0011309504575558198 time: 1703098852.1610785\n",
      "weight: [ 1.46200283  1.66590542  0.35968575  0.40038424  0.17272684  1.41332903\n",
      "  1.30131195  1.0628059   2.63919743 -0.14648551 -0.33669036  0.33695955]\n",
      "epoch 441\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014490698975769733 R2: 0.9164466535110307 time: 1703098854.74149\n",
      "batch_idx: 1 loss: 0.0007275442049359196 R2: 0.9164549557722322 time: 1703098857.356082\n",
      "batch_idx: 2 loss: 0.0015267489677294186 R2: 0.9164481508526784 time: 1703098859.8958585\n",
      "batch_idx: 3 loss: 0.0008203128783123778 R2: 0.9164229096211468 time: 1703098862.5961936\n",
      "Training [88%] Loss: 0.0011309189871386723 time: 1703098862.5961936\n",
      "weight: [ 1.46218586  1.66579007  0.35971015  0.39966624  0.17201588  1.41398306\n",
      "  1.30151607  1.06323539  2.63993546 -0.14648743 -0.33672484  0.33701822]\n",
      "epoch 442\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014489267029815824 R2: 0.9164489400501686 time: 1703098865.1700296\n",
      "batch_idx: 1 loss: 0.000727496151774696 R2: 0.9164572159670346 time: 1703098867.759926\n",
      "batch_idx: 2 loss: 0.0015267623312686203 R2: 0.916450466639071 time: 1703098870.4477134\n",
      "batch_idx: 3 loss: 0.0008203637290497119 R2: 0.9164250672144488 time: 1703098873.0803242\n",
      "Training [88%] Loss: 0.0011308872287686526 time: 1703098873.0803242\n",
      "weight: [ 1.46236934  1.66567635  0.35973553  0.39894796  0.17129768  1.41394158\n",
      "  1.3017189   1.06366185  2.64066703 -0.14649021 -0.33675856  0.33707526]\n",
      "epoch 443\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014487914552354538 R2: 0.9164512442862442 time: 1703098875.769505\n",
      "batch_idx: 1 loss: 0.000727454301018437 R2: 0.9164594624207668 time: 1703098878.4097092\n",
      "batch_idx: 2 loss: 0.0015267692794281669 R2: 0.9164526887580221 time: 1703098881.0505495\n",
      "batch_idx: 3 loss: 0.0008204282034092791 R2: 0.9164274121713019 time: 1703098883.734905\n",
      "Training [89%] Loss: 0.001130860809772834 time: 1703098883.734905\n",
      "weight: [ 1.4625497   1.66556093  0.35975856  0.39823084  0.17058469  1.41430481\n",
      "  1.3019236   1.06408835  2.64138928 -0.14649208 -0.3367932   0.33713322]\n",
      "epoch 444\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014486358035397562 R2: 0.9164534408510635 time: 1703098886.3774557\n",
      "batch_idx: 1 loss: 0.0007274224835177964 R2: 0.9164616889097287 time: 1703098889.0541108\n",
      "batch_idx: 2 loss: 0.0015267753081658077 R2: 0.9164548831643634 time: 1703098891.6591864\n",
      "batch_idx: 3 loss: 0.0008204870986887741 R2: 0.9164296713461132 time: 1703098894.4075265\n",
      "Training [89%] Loss: 0.0011308301734780336 time: 1703098894.4075265\n",
      "weight: [ 1.46272966  1.66544633  0.35978176  0.39751234  0.16987602  1.41527157\n",
      "  1.30212777  1.06451258  2.64210454 -0.14649436 -0.33682751  0.33719044]\n",
      "epoch 445\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014484981036419617 R2: 0.9164556881524394 time: 1703098897.072928\n",
      "batch_idx: 1 loss: 0.0007273781362042476 R2: 0.9164639019250338 time: 1703098899.7727365\n",
      "batch_idx: 2 loss: 0.0015267859287791117 R2: 0.9164571299703163 time: 1703098902.361915\n",
      "batch_idx: 3 loss: 0.0008205395748562804 R2: 0.9164318373534724 time: 1703098904.997634\n",
      "Training [89%] Loss: 0.0011308004358704003 time: 1703098904.997634\n",
      "weight: [ 1.46290919  1.66533251  0.35980509  0.3967935   0.16916584  1.41612788\n",
      "  1.30233144  1.0649346   2.64281282 -0.14649699 -0.33686151  0.33724707]\n",
      "epoch 446\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.00144836127811021 R2: 0.9164579225055103 time: 1703098907.6661806\n",
      "batch_idx: 1 loss: 0.0007273405986051989 R2: 0.9164661038670673 time: 1703098910.2884521\n",
      "batch_idx: 2 loss: 0.0015267915409997025 R2: 0.9164593108693563 time: 1703098912.8926387\n",
      "batch_idx: 3 loss: 0.0008205999974688339 R2: 0.9164341200694267 time: 1703098915.4345443\n",
      "Training [89%] Loss: 0.0011307733537959863 time: 1703098915.4345443\n",
      "weight: [ 1.4630864   1.6652177   0.3598268   0.39607634  0.16845323  1.41654638\n",
      "  1.30253628  1.06535604  2.64351268 -0.14649905 -0.3368961   0.33730447]\n",
      "epoch 447\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014482143050865354 R2: 0.9164601005082604 time: 1703098918.0716546\n",
      "batch_idx: 1 loss: 0.0007273072861576928 R2: 0.9164682944723582 time: 1703098920.6329484\n",
      "batch_idx: 2 loss: 0.00152679722988844 R2: 0.91646148797262 time: 1703098923.1401331\n",
      "batch_idx: 3 loss: 0.0008206549175993968 R2: 0.9164363224713984 time: 1703098925.536627\n",
      "Training [89%] Loss: 0.0011307434346830164 time: 1703098925.536627\n",
      "weight: [ 1.46326311  1.66510359  0.35984857  0.39535877  0.16773997  1.41694653\n",
      "  1.3027407   1.06577536  2.64420568 -0.14650141 -0.33693042  0.33736141]\n",
      "epoch 448\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014480792280535226 R2: 0.9164623086665611 time: 1703098927.7055807\n",
      "batch_idx: 1 loss: 0.0007272676477675881 R2: 0.9164704718334683 time: 1703098929.9029431\n",
      "batch_idx: 2 loss: 0.00152680472887934 R2: 0.916463678535641 time: 1703098932.0675282\n",
      "batch_idx: 3 loss: 0.0008207079048162321 R2: 0.9164384931876505 time: 1703098934.267676\n",
      "Training [90%] Loss: 0.0011307148773791707 time: 1703098934.267676\n",
      "weight: [ 1.46343891  1.66498979  0.35987002  0.39464074  0.16702892  1.41763867\n",
      "  1.30294505  1.06619294  2.64489155 -0.1465039  -0.33696466  0.33741815]\n",
      "epoch 449\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014479428613395659 R2: 0.9164644914689862 time: 1703098936.4129088\n",
      "batch_idx: 1 loss: 0.0007272328639924388 R2: 0.9164726343974745 time: 1703098938.5818043\n",
      "batch_idx: 2 loss: 0.0015268094654949265 R2: 0.9164658250603439 time: 1703098940.8559196\n",
      "batch_idx: 3 loss: 0.0008207642053003536 R2: 0.9164407092037774 time: 1703098943.1326213\n",
      "Training [90%] Loss: 0.0011306873490318213 time: 1703098943.1326213\n",
      "weight: [ 1.46361307  1.6648756   0.35989044  0.39392318  0.1663189   1.4183931\n",
      "  1.30314999  1.06660942  2.64556977 -0.14650614 -0.33699917  0.33747513]\n",
      "epoch 450\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014478035123145386 R2: 0.9164666444690882 time: 1703098945.4713793\n",
      "batch_idx: 1 loss: 0.00072719918254101 R2: 0.916474785149971 time: 1703098947.7614207\n",
      "batch_idx: 2 loss: 0.0015268146000195027 R2: 0.9164679715340187 time: 1703098950.0055957\n",
      "batch_idx: 3 loss: 0.0008208166239069561 R2: 0.9164428711237645 time: 1703098952.2182207\n",
      "Training [90%] Loss: 0.0011306584796955017 time: 1703098952.2182207\n",
      "weight: [ 1.46378653  1.6647619   0.35991073  0.39320563  0.16560709  1.41895762\n",
      "  1.30335468  1.06702402  2.64624119 -0.14650861 -0.3370335   0.33753167]\n",
      "epoch 451\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001447670868844769 R2: 0.9164688090242802 time: 1703098954.4157069\n",
      "batch_idx: 1 loss: 0.0007271630769459633 R2: 0.9164769251526378 time: 1703098956.6749778\n",
      "batch_idx: 2 loss: 0.0015268200349471102 R2: 0.9164701156506763 time: 1703098958.9036636\n",
      "batch_idx: 3 loss: 0.0008208684206978438 R2: 0.9164450240576812 time: 1703098961.147416\n",
      "Training [90%] Loss: 0.0011306306003589215 time: 1703098961.147416\n",
      "weight: [ 1.46395888  1.66464829  0.35993049  0.39248828  0.1648949   1.41946136\n",
      "  1.3035595   1.0674371   2.64690553 -0.14651109 -0.33706785  0.33758801]\n",
      "epoch 452\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001447536823658193 R2: 0.9164709507683707 time: 1703098963.329689\n",
      "batch_idx: 1 loss: 0.0007271302831624683 R2: 0.9164790522194591 time: 1703098965.5267305\n",
      "batch_idx: 2 loss: 0.001526823868351681 R2: 0.9164722305412123 time: 1703098967.6749318\n",
      "batch_idx: 3 loss: 0.0008209213074337263 R2: 0.9164471906218047 time: 1703098969.8705423\n",
      "Training [90%] Loss: 0.0011306030706515172 time: 1703098969.8705423\n",
      "weight: [ 1.46412993  1.66453458  0.35994953  0.39177106  0.16418395  1.42008948\n",
      "  1.30376462  1.06784887  2.64756272 -0.1465135  -0.33710232  0.3376443 ]\n",
      "epoch 453\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014474028018566431 R2: 0.9164730746057742 time: 1703098972.0998995\n",
      "batch_idx: 1 loss: 0.0007270975089930361 R2: 0.9164811665781596 time: 1703098974.3924484\n",
      "batch_idx: 2 loss: 0.0015268281304669077 R2: 0.9164743428364421 time: 1703098976.6179593\n",
      "batch_idx: 3 loss: 0.0008209715032268473 R2: 0.9164493178933182 time: 1703098978.924452\n",
      "Training [91%] Loss: 0.0011305749861358586 time: 1703098978.924452\n",
      "weight: [ 1.46430017  1.66442123  0.35996831  0.39105366  0.16347319  1.42076546\n",
      "  1.3039696   1.06825889  2.64821324 -0.14651607 -0.33713667  0.33770021]\n",
      "epoch 454\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014472729484176349 R2: 0.916475198508118 time: 1703098981.3008263\n",
      "batch_idx: 1 loss: 0.0007270639665235823 R2: 0.9164832694538548 time: 1703098983.5150597\n",
      "batch_idx: 2 loss: 0.0015268321874151842 R2: 0.9164764459195412 time: 1703098985.738471\n",
      "batch_idx: 3 loss: 0.000821021457862289 R2: 0.9164514419375174 time: 1703098988.1764936\n",
      "Training [91%] Loss: 0.0011305476400546726 time: 1703098988.1764936\n",
      "weight: [ 1.46446928  1.66430793  0.35998653  0.39033653  0.16276194  1.42135824\n",
      "  1.30417473  1.06866748  2.64885688 -0.14651863 -0.33717106  0.33775599]\n",
      "epoch 455\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014471422202282339 R2: 0.9164773028724932 time: 1703098990.6571374\n",
      "batch_idx: 1 loss: 0.0007270326833355224 R2: 0.9164853608371656 time: 1703098993.1760275\n",
      "batch_idx: 2 loss: 0.001526835213534993 R2: 0.9164785290707929 time: 1703098995.5513077\n",
      "batch_idx: 3 loss: 0.0008210714036204049 R2: 0.916453565437403 time: 1703098997.8529692\n",
      "Training [91%] Loss: 0.0011305203801797886 time: 1703098997.8529692\n",
      "weight: [ 1.46463725  1.66419465  0.36000417  0.38961961  0.16205063  1.42192106\n",
      "  1.30438003  1.06907466  2.6494937  -0.14652118 -0.33720551  0.33781168]\n",
      "epoch 456\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014470124890790387 R2: 0.9164793949663921 time: 1703099000.0929625\n",
      "batch_idx: 1 loss: 0.0007270013578371902 R2: 0.9164874405303941 time: 1703099002.2709763\n",
      "batch_idx: 2 loss: 0.0015268384035288093 R2: 0.9164806068761117 time: 1703099004.5228736\n",
      "batch_idx: 3 loss: 0.0008211195474059363 R2: 0.9164556620554029 time: 1703099007.080376\n",
      "Training [91%] Loss: 0.0011304929494627435 time: 1703099007.080376\n",
      "weight: [ 1.46480434  1.66408164  0.36002147  0.38890259  0.1613397   1.42254239\n",
      "  1.30458527  1.06948023  2.65012399 -0.14652383 -0.33723988  0.33786711]\n",
      "epoch 457\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014468853802839438 R2: 0.9164814811108123 time: 1703099009.387276\n",
      "batch_idx: 1 loss: 0.000726970061444105 R2: 0.916489508509707 time: 1703099011.5703619\n",
      "batch_idx: 2 loss: 0.0015268412360835103 R2: 0.916482672924013 time: 1703099013.7972908\n",
      "batch_idx: 3 loss: 0.0008211674364904731 R2: 0.9164577540775791 time: 1703099015.9906917\n",
      "Training [91%] Loss: 0.0011304660285755082 time: 1703099015.9906917\n",
      "weight: [ 1.46497035  1.66396869  0.36003823  0.38818569  0.160629    1.42318\n",
      "  1.30479063  1.06988438  2.65074764 -0.14652647 -0.33727429  0.33792244]\n",
      "epoch 458\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014467581660110274 R2: 0.9164835507907739 time: 1703099018.1632402\n",
      "batch_idx: 1 loss: 0.0007269402082874767 R2: 0.9164915651531643 time: 1703099020.38777\n",
      "batch_idx: 2 loss: 0.0015268434070048885 R2: 0.9164847240866767 time: 1703099022.6161768\n",
      "batch_idx: 3 loss: 0.0008212148377091868 R2: 0.916459838702522 time: 1703099024.8598685\n",
      "Training [92%] Loss: 0.0011304391547531448 time: 1703099024.8598685\n",
      "weight: [ 1.46513531  1.66385583  0.36005448  0.38746896  0.15991812  1.42377693\n",
      "  1.30499608  1.07028709  2.65136476 -0.14652913 -0.33730873  0.33797763]\n",
      "epoch 459\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014466322155113074 R2: 0.916485610066464 time: 1703099027.0255003\n",
      "batch_idx: 1 loss: 0.0007269104765385572 R2: 0.9164936108198336 time: 1703099029.2405748\n",
      "batch_idx: 2 loss: 0.001526845508560081 R2: 0.9164867678908528 time: 1703099031.4026134\n",
      "batch_idx: 3 loss: 0.0008212609899585825 R2: 0.9164619047731565 time: 1703099033.5855024\n",
      "Training [92%] Loss: 0.001130412297642132 time: 1703099033.5855024\n",
      "weight: [ 1.46529935  1.66374317  0.36007034  0.38675226  0.15920721  1.42436969\n",
      "  1.30520152  1.07068828  2.6519755  -0.14653185 -0.33734313  0.33803262]\n",
      "epoch 460\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014465079715106554 R2: 0.9164876606079405 time: 1703099035.7858984\n",
      "batch_idx: 1 loss: 0.0007268811663047992 R2: 0.9164956452945081 time: 1703099038.025936\n",
      "batch_idx: 2 loss: 0.0015268471993778333 R2: 0.9164887995831366 time: 1703099040.2547107\n",
      "batch_idx: 3 loss: 0.000821306784831584 R2: 0.9164639645905611 time: 1703099042.4537365\n",
      "Training [92%] Loss: 0.001130385780506218 time: 1703099042.4537365\n",
      "weight: [ 1.46546236  1.6636306   0.36008569  0.38603565  0.15849662  1.4249915\n",
      "  1.30540704  1.07108806  2.65257985 -0.14653458 -0.33737757  0.33808748]\n",
      "epoch 461\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014463841368359811 R2: 0.9164896971682184 time: 1703099044.6496656\n",
      "batch_idx: 1 loss: 0.0007268528077412777 R2: 0.9164976686472951 time: 1703099046.7829201\n",
      "batch_idx: 2 loss: 0.0015268484430142426 R2: 0.9164908189847425 time: 1703099048.9486752\n",
      "batch_idx: 3 loss: 0.0008213519036412 R2: 0.9164660139450961 time: 1703099051.2244194\n",
      "Training [92%] Loss: 0.0011303593228081752 time: 1703099051.2244194\n",
      "weight: [ 1.46562438  1.66351816  0.36010058  0.38531914  0.15778608  1.4256107\n",
      "  1.30561261  1.0714864   2.65317791 -0.14653732 -0.33741201  0.33814218]\n",
      "epoch 462\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001446261617174516 R2: 0.9164917238360968 time: 1703099053.5795684\n",
      "batch_idx: 1 loss: 0.0007268246883926205 R2: 0.9164996812855529 time: 1703099055.8101568\n",
      "batch_idx: 2 loss: 0.0015268495032366497 R2: 0.916492829752157 time: 1703099057.9714317\n",
      "batch_idx: 3 loss: 0.0008213960845468528 R2: 0.916468049290032 time: 1703099060.16858\n",
      "Training [92%] Loss: 0.0011303329733376598 time: 1703099060.16858\n",
      "weight: [ 1.46578547  1.66340587  0.36011504  0.38460271  0.15707547  1.42621176\n",
      "  1.30581818  1.07188329  2.65376977 -0.14654012 -0.33744644  0.3381967 ]\n",
      "epoch 463\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014461403477915246 R2: 0.9164937405596681 time: 1703099062.3359478\n",
      "batch_idx: 1 loss: 0.0007267971308063081 R2: 0.9165016832228001 time: 1703099064.4847317\n",
      "batch_idx: 2 loss: 0.0015268501600828027 R2: 0.916494828867299 time: 1703099066.6662607\n",
      "batch_idx: 3 loss: 0.0008214398066390728 R2: 0.9164700771596361 time: 1703099068.9094708\n",
      "Training [93%] Loss: 0.001130306861329927 time: 1703099068.9094708\n",
      "weight: [ 1.46594556  1.66329368  0.36012903  0.38388638  0.15636501  1.42682059\n",
      "  1.30602382  1.07227879  2.65435546 -0.14654292 -0.3374809   0.33825109]\n",
      "epoch 464\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014460197630806393 R2: 0.9164957450392791 time: 1703099071.1296241\n",
      "batch_idx: 1 loss: 0.0007267702716565789 R2: 0.916503674446275 time: 1703099073.2879248\n",
      "batch_idx: 2 loss: 0.0015268504706808866 R2: 0.9164968170319396 time: 1703099075.4540706\n",
      "batch_idx: 3 loss: 0.0008214828092398511 R2: 0.9164720938540812 time: 1703099077.6030407\n",
      "Training [93%] Loss: 0.001130280828664489 time: 1703099077.6030407\n",
      "weight: [ 1.46610472  1.66318164  0.36014257  0.38317012  0.1556547   1.42744003\n",
      "  1.30622948  1.07267286  2.65493508 -0.14654575 -0.33751535  0.33830532]\n",
      "epoch 465\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014459004301685036 R2: 0.9164977397593598 time: 1703099079.7683256\n",
      "batch_idx: 1 loss: 0.0007267437335967591 R2: 0.9165056551916756 time: 1703099081.9811656\n",
      "batch_idx: 2 loss: 0.0015268505478586328 R2: 0.9164987959003158 time: 1703099084.211136\n",
      "batch_idx: 3 loss: 0.0008215250386753822 R2: 0.9164740987894564 time: 1703099086.5321233\n",
      "Training [93%] Loss: 0.0011302549375748195 time: 1703099086.5321233\n",
      "weight: [ 1.46626294  1.66306973  0.36015568  0.38245394  0.1549444   1.42805132\n",
      "  1.30643514  1.07306553  2.6555087  -0.14654861 -0.33754981  0.33835939]\n",
      "epoch 466\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001445782154075237 R2: 0.9164997241930356 time: 1703099088.6823108\n",
      "batch_idx: 1 loss: 0.0007267177681105974 R2: 0.9165076255821761 time: 1703099090.9016528\n",
      "batch_idx: 2 loss: 0.0015268502557000624 R2: 0.9165007637509686 time: 1703099093.0612128\n",
      "batch_idx: 3 loss: 0.000821566741537026 R2: 0.9164760953911705 time: 1703099095.2755024\n",
      "Training [93%] Loss: 0.0011302292298557308 time: 1703099095.2755024\n",
      "weight: [ 1.46642022  1.66295795  0.36016833  0.38173786  0.15423417  1.42865951\n",
      "  1.30664084  1.07345683  2.65607636 -0.14655149 -0.33758427  0.33841333]\n",
      "epoch 467\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014456646997474004 R2: 0.9165016974931193 time: 1703099097.5187404\n",
      "batch_idx: 1 loss: 0.000726692375101033 R2: 0.9165095856611176 time: 1703099099.6828623\n",
      "batch_idx: 2 loss: 0.0015268496592009831 R2: 0.9165027213719613 time: 1703099102.02857\n",
      "batch_idx: 3 loss: 0.0008216077429565429 R2: 0.9164780810877836 time: 1703099104.2193816\n",
      "Training [93%] Loss: 0.00113020361925149 time: 1703099104.2193816\n",
      "weight: [ 1.46657657  1.6628463   0.36018056  0.38102186  0.15352406  1.42927542\n",
      "  1.30684655  1.07384673  2.65663815 -0.1465544  -0.33761874  0.33846712]\n",
      "epoch 468\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014455483955647008 R2: 0.9165036610969348 time: 1703099106.4783337\n",
      "batch_idx: 1 loss: 0.0007266673613601267 R2: 0.916511535557561 time: 1703099108.644129\n",
      "batch_idx: 2 loss: 0.001526848804352159 R2: 0.9165046694543546 time: 1703099110.8424585\n",
      "batch_idx: 3 loss: 0.0008216480620843181 R2: 0.9164800562124296 time: 1703099113.0692768\n",
      "Training [94%] Loss: 0.0011301781558403262 time: 1703099113.0692768\n",
      "weight: [ 1.46673202  1.66273479  0.36019236  0.38030594  0.15281403  1.42989071\n",
      "  1.30705226  1.07423526  2.65719413 -0.14655734 -0.33765321  0.33852075]\n",
      "epoch 469\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014454330651957185 R2: 0.9165056144987697 time: 1703099115.283046\n",
      "batch_idx: 1 loss: 0.0007266428887847787 R2: 0.9165134753987492 time: 1703099117.6357515\n",
      "batch_idx: 2 loss: 0.0015268476151928485 R2: 0.9165066071116141 time: 1703099119.8008826\n",
      "batch_idx: 3 loss: 0.0008216878195181363 R2: 0.9164820224778426 time: 1703099122.0207162\n",
      "Training [94%] Loss: 0.0011301528471728706 time: 1703099122.0207162\n",
      "weight: [ 1.46688655  1.6626234   0.36020371  0.37959011  0.15210406  1.43050202\n",
      "  1.30725798  1.07462243  2.65774436 -0.14656029 -0.33768769  0.33857424]\n",
      "epoch 470\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014453186187130648 R2: 0.916507557490546 time: 1703099124.275104\n",
      "batch_idx: 1 loss: 0.0007266189221303067 R2: 0.9165154052722941 time: 1703099126.4698367\n",
      "batch_idx: 2 loss: 0.0015268461405399988 R2: 0.9165085349708212 time: 1703099128.6515243\n",
      "batch_idx: 3 loss: 0.0008217269126946201 R2: 0.9164839783594717 time: 1703099130.8868902\n",
      "Training [94%] Loss: 0.0011301276485194977 time: 1703099130.8868902\n",
      "weight: [ 1.46704019  1.66251215  0.36021465  0.37887435  0.15139419  1.43111682\n",
      "  1.3074637   1.07500824  2.65828892 -0.14656327 -0.33772217  0.33862758]\n",
      "epoch 471\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014452052277687537 R2: 0.9165094908745319 time: 1703099133.2345645\n",
      "batch_idx: 1 loss: 0.0007265953681764648 R2: 0.916517325272898 time: 1703099135.4474406\n",
      "batch_idx: 2 loss: 0.0015268443975267705 R2: 0.9165104533007641 time: 1703099138.0377023\n",
      "batch_idx: 3 loss: 0.000821765376955263 R2: 0.9164859243567289 time: 1703099140.688405\n",
      "Training [94%] Loss: 0.001130102592606813 time: 1703099140.688405\n",
      "weight: [ 1.46719294  1.66240104  0.36022517  0.37815868  0.15068441  1.4317332\n",
      "  1.3076694   1.0753927   2.65882786 -0.14656628 -0.33775666  0.33868078]\n",
      "epoch 472\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014450927680562638 R2: 0.9165114143114497 time: 1703099143.262954\n",
      "batch_idx: 1 loss: 0.000726572317716722 R2: 0.9165192355106481 time: 1703099145.930154\n",
      "batch_idx: 2 loss: 0.0015268423489587658 R2: 0.9165123617025916 time: 1703099148.5406961\n",
      "batch_idx: 3 loss: 0.0008218032680519946 R2: 0.9164878612727337 time: 1703099151.2500587\n",
      "Training [94%] Loss: 0.0011300776756959367 time: 1703099151.2500587\n",
      "weight: [ 1.4673448   1.66229006  0.36023526  0.37744309  0.1499747   1.4323472\n",
      "  1.3078751   1.07577583  2.65936125 -0.1465693  -0.33779115  0.33873384]\n",
      "epoch 473\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001444981210675521 R2: 0.9165133278425074 time: 1703099153.8546803\n",
      "batch_idx: 1 loss: 0.0007265497339266028 R2: 0.9165211360849665 time: 1703099156.634054\n",
      "batch_idx: 2 loss: 0.0015268400271868405 R2: 0.9165142606146917 time: 1703099159.350124\n",
      "batch_idx: 3 loss: 0.0008218405325526323 R2: 0.9164897883091483 time: 1703099161.9365172\n",
      "Training [95%] Loss: 0.001130052876085399 time: 1703099161.9365172\n",
      "weight: [ 1.46749579  1.66217922  0.36024495  0.37672758  0.14926509  1.43296265\n",
      "  1.30808078  1.07615763  2.65988915 -0.14657235 -0.33782565  0.33878676]\n",
      "epoch 474\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014448706350468576 R2: 0.9165152319065285 time: 1703099164.6250176\n",
      "batch_idx: 1 loss: 0.0007265275745580458 R2: 0.916523027082879 time: 1703099167.2093554\n",
      "batch_idx: 2 loss: 0.0015268374379390974 R2: 0.9165161501427063 time: 1703099169.877462\n",
      "batch_idx: 3 loss: 0.0008218772024460999 R2: 0.9164917059006253 time: 1703099172.4680717\n",
      "Training [95%] Loss: 0.0011300282124975252 time: 1703099172.4680717\n",
      "weight: [ 1.46764592  1.66206852  0.36025422  0.37601215  0.14855556  1.43357965\n",
      "  1.30828644  1.0765381   2.66041162 -0.14657542 -0.33786015  0.33883954]\n",
      "epoch 475\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014447609619706193 R2: 0.9165171263255665 time: 1703099175.0441303\n",
      "batch_idx: 1 loss: 0.0007265058847806236 R2: 0.9165249086024747 time: 1703099177.630271\n",
      "batch_idx: 2 loss: 0.0015268345659480112 R2: 0.9165180301493481 time: 1703099180.250224\n",
      "batch_idx: 3 loss: 0.0008219133026673082 R2: 0.9164936144060218 time: 1703099182.9956324\n",
      "Training [95%] Loss: 0.0011300036788416405 time: 1703099182.9956324\n",
      "weight: [ 1.46779519  1.66195795  0.36026308  0.3752968   0.14784611  1.4341955\n",
      "  1.30849208  1.07691727  2.66092872 -0.14657851 -0.33789465  0.33889219]\n",
      "epoch 476\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014446521847841882 R2: 0.9165190112190847 time: 1703099185.6069427\n",
      "batch_idx: 1 loss: 0.0007264846353267645 R2: 0.9165267807399381 time: 1703099188.261633\n",
      "batch_idx: 2 loss: 0.0015268314316897278 R2: 0.9165199009257347 time: 1703099190.9297156\n",
      "batch_idx: 3 loss: 0.0008219488093055682 R2: 0.9164955134590176 time: 1703099193.5663419\n",
      "Training [95%] Loss: 0.0011299792652765621 time: 1703099193.5663419\n",
      "weight: [ 1.46794362  1.66184752  0.36027154  0.37458153  0.14713675  1.43481219\n",
      "  1.30869769  1.07729513  2.66144052 -0.14658163 -0.33792916  0.33894469]\n",
      "epoch 477\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014445443339901967 R2: 0.916520886831449 time: 1703099196.1218147\n",
      "batch_idx: 1 loss: 0.0007264638076296913 R2: 0.9165286435798675 time: 1703099198.7669358\n",
      "batch_idx: 2 loss: 0.0015268280372086435 R2: 0.9165217625279162 time: 1703099201.5230427\n",
      "batch_idx: 3 loss: 0.0008219837466475182 R2: 0.9164974033837028 time: 1703099204.1423533\n",
      "Training [95%] Loss: 0.0011299549813690124 time: 1703099204.1423533\n",
      "weight: [ 1.4680912   1.66173723  0.3602796   0.37386634  0.14642749  1.43542997\n",
      "  1.30890326  1.07767169  2.66194708 -0.14658476 -0.33796368  0.33899706]\n",
      "epoch 478\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014444373610639277 R2: 0.9165227530989682 time: 1703099206.8494525\n",
      "batch_idx: 1 loss: 0.0007264434210930533 R2: 0.9165304972133249 time: 1703099209.5334358\n",
      "batch_idx: 2 loss: 0.0015268243781852995 R2: 0.9165236149464843 time: 1703099212.2640254\n",
      "batch_idx: 3 loss: 0.0008220181265741986 R2: 0.9164992843446053 time: 1703099215.0102515\n",
      "Training [96%] Loss: 0.0011299308217291198 time: 1703099215.0102515\n",
      "weight: [ 1.46823795  1.66162708  0.36028726  0.37315122  0.1457183   1.43604726\n",
      "  1.3091088   1.07804696  2.66244844 -0.14658792 -0.3379982   0.3390493 ]\n",
      "epoch 479\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014443312646944774 R2: 0.9165246101508538 time: 1703099217.6570296\n",
      "batch_idx: 1 loss: 0.0007264234547651601 R2: 0.9165323417296648 time: 1703099220.3776085\n",
      "batch_idx: 2 loss: 0.0015268204674867606 R2: 0.9165254583750182 time: 1703099223.0592847\n",
      "batch_idx: 3 loss: 0.0008220519413555735 R2: 0.9165011562077663 time: 1703099225.7904909\n",
      "Training [96%] Loss: 0.0011299067820754929 time: 1703099225.7904909\n",
      "weight: [ 1.46838388  1.66151707  0.36029453  0.37243618  0.1450092   1.43666528\n",
      "  1.30931429  1.07842095  2.66294468 -0.14659109 -0.33803272  0.3391014 ]\n",
      "epoch 480\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001444226051832791 R2: 0.9165264581348114 time: 1703099228.475193\n",
      "batch_idx: 1 loss: 0.00072640389984826 R2: 0.9165341772112333 time: 1703099231.2741668\n",
      "batch_idx: 2 loss: 0.0015268163065611058 R2: 0.9165272928635202 time: 1703099233.9461312\n",
      "batch_idx: 3 loss: 0.0008220852083972236 R2: 0.9165030192012484 time: 1703099236.728805\n",
      "Training [96%] Loss: 0.001129882866659845 time: 1703099236.728805\n",
      "weight: [ 1.46852899  1.6614072   0.36030141  0.37172121  0.1443002   1.43728397\n",
      "  1.30951974  1.07879367  2.66343585 -0.14659429 -0.33806725  0.33915338]\n",
      "epoch 481\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014441216925564142 R2: 0.9165282970561325 time: 1703099239.4746978\n",
      "batch_idx: 1 loss: 0.0007263847624824065 R2: 0.9165360037437672 time: 1703099242.1424165\n",
      "batch_idx: 2 loss: 0.001526811895979223 R2: 0.9165291184576979 time: 1703099244.9349988\n",
      "batch_idx: 3 loss: 0.0008221179349481576 R2: 0.9165048734159458 time: 1703099247.6285384\n",
      "Training [96%] Loss: 0.0011298590714915503 time: 1703099247.6285384\n",
      "weight: [ 1.4686733   1.66129747  0.3603079   0.37100633  0.14359127  1.43790255\n",
      "  1.30972514  1.07916513  2.663922   -0.1465975  -0.33810178  0.33920522]\n",
      "epoch 482\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001444018184783549 R2: 0.9165301270309666 time: 1703099250.4450772\n",
      "batch_idx: 1 loss: 0.0007263660281386719 R2: 0.9165378214100887 time: 1703099253.0382938\n",
      "batch_idx: 2 loss: 0.0015268072441473818 R2: 0.9165309352929766 time: 1703099255.8265686\n",
      "batch_idx: 3 loss: 0.0008221501213108275 R2: 0.9165067188321437 time: 1703099258.5739887\n",
      "Training [96%] Loss: 0.0011298353945951075 time: 1703099258.5739887\n",
      "weight: [ 1.4688168   1.66118788  0.36031401  0.37029151  0.14288244  1.43852186\n",
      "  1.30993048  1.07953533  2.6644032  -0.14660073 -0.33813632  0.33925693]\n",
      "epoch 483\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014439155253092525 R2: 0.9165319481626584 time: 1703099261.2941434\n",
      "batch_idx: 1 loss: 0.0007263476913959477 R2: 0.9165396302899242 time: 1703099263.9554625\n",
      "batch_idx: 2 loss: 0.001526802352929945 R2: 0.9165327434253705 time: 1703099266.618613\n",
      "batch_idx: 3 loss: 0.0008221817800479706 R2: 0.9165085556107041 time: 1703099269.3250043\n",
      "Training [97%] Loss: 0.001129811837420779 time: 1703099269.3250043\n",
      "weight: [ 1.4689595   1.66107843  0.36031974  0.36957677  0.14217369  1.43914153\n",
      "  1.31013576  1.07990428  2.6648795  -0.14660398 -0.33817086  0.33930852]\n",
      "epoch 484\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014438136946272938 R2: 0.9165337604928819 time: 1703099271.9924068\n",
      "batch_idx: 1 loss: 0.000726329752113432 R2: 0.9165414304638514 time: 1703099274.6283698\n",
      "batch_idx: 2 loss: 0.0015267972248737879 R2: 0.9165345429195499 time: 1703099277.3420553\n",
      "batch_idx: 3 loss: 0.0008222129172279066 R2: 0.9165103838197413 time: 1703099280.008355\n",
      "Training [97%] Loss: 0.0011297883972106052 time: 1703099280.008355\n",
      "weight: [ 1.46910142  1.66096913  0.36032508  0.3688621   0.14146503  1.43976139\n",
      "  1.31034097  1.08027198  2.66535095 -0.14660725 -0.33820541  0.33935998]\n",
      "epoch 485\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.00144371268861382 R2: 0.916535564121135 time: 1703099282.6776772\n",
      "batch_idx: 1 loss: 0.0007263121998993054 R2: 0.916543222009382 time: 1703099285.4160948\n",
      "batch_idx: 2 loss: 0.0015267918659966854 R2: 0.9165363338793732 time: 1703099288.1373067\n",
      "batch_idx: 3 loss: 0.0008222435366119144 R2: 0.9165122034901383 time: 1703099290.853092\n",
      "Training [97%] Loss: 0.0011297650727804312 time: 1703099290.853092\n",
      "weight: [ 1.46924257  1.66085996  0.36033006  0.36814751  0.14075646  1.44038189\n",
      "  1.31054612  1.08063845  2.66581761 -0.14661054 -0.33823996  0.33941131]\n",
      "epoch 486\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014436125003003757 R2: 0.9165373591319425 time: 1703099293.5885022\n",
      "batch_idx: 1 loss: 0.0007262950302159189 R2: 0.9165450050032004 time: 1703099296.2838347\n",
      "batch_idx: 2 loss: 0.0015267862786969117 R2: 0.9165381163659939 time: 1703099299.041052\n",
      "batch_idx: 3 loss: 0.0008222736478745239 R2: 0.9165140147417278 time: 1703099301.8538685\n",
      "Training [97%] Loss: 0.0011297418642719325 time: 1703099301.8538685\n",
      "weight: [ 1.46938294  1.66075095  0.36033467  0.36743298  0.14004798  1.4410026\n",
      "  1.3107512   1.0810037   2.66627953 -0.14661385 -0.33827451  0.33946252]\n",
      "epoch 487\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014435131156208844 R2: 0.9165391455826895 time: 1703099304.6153278\n",
      "batch_idx: 1 loss: 0.0007262782401990474 R2: 0.9165467795206815 time: 1703099307.427353\n",
      "batch_idx: 2 loss: 0.0015267804662346851 R2: 0.9165398904478591 time: 1703099310.2333333\n",
      "batch_idx: 3 loss: 0.0008223032569294967 R2: 0.9165158176366234 time: 1703099312.9796498\n",
      "Training [97%] Loss: 0.0011297187697460282 time: 1703099312.9796498\n",
      "weight: [ 1.46952255  1.66064207  0.36033891  0.36671853  0.13933958  1.44162374\n",
      "  1.3109562   1.08136772  2.66673678 -0.14661717 -0.33830907  0.33951361]\n",
      "epoch 488\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014434145289143638 R2: 0.9165409235598657 time: 1703099315.711149\n",
      "batch_idx: 1 loss: 0.0007262618218307347 R2: 0.9165485456353364 time: 1703099318.349189\n",
      "batch_idx: 2 loss: 0.001526774433398458 R2: 0.9165416562126867 time: 1703099321.1302986\n",
      "batch_idx: 3 loss: 0.000822332368778739 R2: 0.9165176122257892 time: 1703099323.6983368\n",
      "Training [98%] Loss: 0.001129695788230574 time: 1703099323.6983368\n",
      "weight: [ 1.4696614   1.66053334  0.36034278  0.36600415  0.13863127  1.44224532\n",
      "  1.31116113  1.08173053  2.66718939 -0.14662051 -0.33834364  0.33956458]\n",
      "epoch 489\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001443316732052485 R2: 0.9165426931395485 time: 1703099326.5085657\n",
      "batch_idx: 1 loss: 0.0007262457707947106 R2: 0.9165503034200727 time: 1703099329.1594193\n",
      "batch_idx: 2 loss: 0.0015267681829216136 R2: 0.916543413722151 time: 1703099331.8770106\n",
      "batch_idx: 3 loss: 0.000822360991642789 R2: 0.916519398605829 time: 1703099334.6514904\n",
      "Training [98%] Loss: 0.0011296729193528995 time: 1703099334.6514904\n",
      "weight: [ 1.4697995   1.66042476  0.36034629  0.36528984  0.13792305  1.44286717\n",
      "  1.31136596  1.08209214  2.66763742 -0.14662387 -0.3383782   0.33961542]\n",
      "epoch 490\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014432197137458612 R2: 0.9165444543835546 time: 1703099337.2252638\n",
      "batch_idx: 1 loss: 0.0007262300830893785 R2: 0.9165520529456617 time: 1703099339.924774\n",
      "batch_idx: 2 loss: 0.0015267617182892312 R2: 0.916545163044751 time: 1703099342.578541\n",
      "batch_idx: 3 loss: 0.0008223891312510864 R2: 0.9165211768357248 time: 1703099345.2770662\n",
      "Training [98%] Loss: 0.0011296501615938894 time: 1703099345.2770662\n",
      "weight: [ 1.46993685  1.66031632  0.36034945  0.36457559  0.13721491  1.44348951\n",
      "  1.31157071  1.08245255  2.66808093 -0.14662724 -0.33841278  0.33966614]\n",
      "epoch 491\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014431234675743659 R2: 0.9165462073703748 time: 1703099347.9435523\n",
      "batch_idx: 1 loss: 0.0007262147520732596 R2: 0.9165537942822496 time: 1703099350.5690625\n",
      "batch_idx: 2 loss: 0.0015267550435909183 R2: 0.9165469042580835 time: 1703099353.2315402\n",
      "batch_idx: 3 loss: 0.00082241679313927 R2: 0.9165229469746295 time: 1703099355.7746496\n",
      "Training [98%] Loss: 0.0011296275140944534 time: 1703099355.7746496\n",
      "weight: [ 1.47007348  1.66020803  0.36035225  0.36386142  0.13650686  1.44411214\n",
      "  1.31177537  1.08281176  2.66851997 -0.14663064 -0.33844735  0.33971675]\n",
      "epoch 492\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014430279850577844 R2: 0.9165479521699338 time: 1703099358.4417377\n",
      "batch_idx: 1 loss: 0.0007261997735703564 R2: 0.916555527498409 time: 1703099361.1416483\n",
      "batch_idx: 2 loss: 0.001526748161727889 R2: 0.9165486374220201 time: 1703099364.1847506\n",
      "batch_idx: 3 loss: 0.000822443984690086 R2: 0.9165247091044281 time: 1703099366.9634128\n",
      "Training [98%] Loss: 0.001129604976261529 time: 1703099366.9634128\n",
      "weight: [ 1.47020937  1.66009988  0.3603547   0.36314731  0.1357989   1.44473521\n",
      "  1.31197994  1.08316979  2.66895458 -0.14663404 -0.33848193  0.33976724]\n",
      "epoch 493\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014429332565573769 R2: 0.9165496888445974 time: 1703099369.6304283\n",
      "batch_idx: 1 loss: 0.0007261851430556605 R2: 0.9165572526614069 time: 1703099372.3012502\n",
      "batch_idx: 2 loss: 0.0015267410762161361 R2: 0.9165503626048175 time: 1703099374.951411\n",
      "batch_idx: 3 loss: 0.0008224707113345696 R2: 0.9165264632816216 time: 1703099377.6387691\n",
      "Training [99%] Loss: 0.0011295825467909357 time: 1703099377.6387691\n",
      "weight: [ 1.47034454  1.65999189  0.3603568   0.36243327  0.13509102  1.44535861\n",
      "  1.3121844   1.08352664  2.66938482 -0.14663746 -0.33851652  0.33981761]\n",
      "epoch 494\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014428392752614152 R2: 0.9165514174673401 time: 1703099380.2555654\n",
      "batch_idx: 1 loss: 0.0007261708548688635 R2: 0.9165589698375968 time: 1703099382.8907835\n",
      "batch_idx: 2 loss: 0.0015267337906755415 R2: 0.9165520798746971 time: 1703099385.5115879\n",
      "batch_idx: 3 loss: 0.0008224969790497068 R2: 0.9165282095688585 time: 1703099388.1336474\n",
      "Training [99%] Loss: 0.0011295602249638817 time: 1703099388.1336474\n",
      "weight: [ 1.470479    1.65988404  0.36035856  0.36171929  0.13438323  1.44598241\n",
      "  1.31238876  1.08388232  2.66981073 -0.1466409  -0.33855111  0.33986787]\n",
      "epoch 495\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014427460326716744 R2: 0.916553138102174 time: 1703099390.8469288\n",
      "batch_idx: 1 loss: 0.0007261569048425999 R2: 0.916560679091835 time: 1703099393.395421\n",
      "batch_idx: 2 loss: 0.0015267263081224672 R2: 0.9165537892917733 time: 1703099396.0005152\n",
      "batch_idx: 3 loss: 0.0008225227943255465 R2: 0.9165299480363276 time: 1703099398.6321883\n",
      "Training [99%] Loss: 0.001129538009990572 time: 1703099398.6321883\n",
      "weight: [ 1.47061275  1.65977634  0.36035997  0.36100538  0.13367553  1.44660658\n",
      "  1.31259302  1.08423684  2.67023237 -0.14664435 -0.3385857   0.33991801]\n",
      "epoch 496\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014426535203176855 R2: 0.9165548508119958 time: 1703099401.3067374\n",
      "batch_idx: 1 loss: 0.0007261432883377681 R2: 0.9165623804879932 time: 1703099403.9510407\n",
      "batch_idx: 2 loss: 0.00152671863196196 R2: 0.9165554909204363 time: 1703099406.63168\n",
      "batch_idx: 3 loss: 0.0008225481626905155 R2: 0.9165316787404642 time: 1703099409.3110251\n",
      "Training [99%] Loss: 0.0011295159008269822 time: 1703099409.3110251\n",
      "weight: [ 1.47074579  1.65966878  0.36036105  0.36029153  0.13296791  1.44723111\n",
      "  1.31279716  1.0845902   2.67064978 -0.14664782 -0.3386203   0.33996804]\n",
      "epoch 497\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014425617310294997 R2: 0.9165565556631824 time: 1703099412.0496132\n",
      "batch_idx: 1 loss: 0.0007261300003676923 R2: 0.9165640740887536 time: 1703099414.7316105\n",
      "batch_idx: 2 loss: 0.0015267107655346332 R2: 0.9165571848235399 time: 1703099417.3543274\n",
      "batch_idx: 3 loss: 0.0008225730900288844 R2: 0.9165334017424357 time: 1703099420.1985233\n",
      "Training [99%] Loss: 0.0011294938967401774 time: 1703099420.1985233\n",
      "weight: [ 1.47087814  1.65956138  0.36036178  0.35957775  0.13226037  1.44785602\n",
      "  1.31300119  1.0849424   2.67106301 -0.14665131 -0.3386549   0.34001795]\n",
      "epoch 498\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014424706566947775 R2: 0.9165582527168669 time: 1703099422.9010544\n",
      "batch_idx: 1 loss: 0.0007261170367403014 R2: 0.9165657599556832 time: 1703099425.5311441\n",
      "batch_idx: 2 loss: 0.001526702711889213 R2: 0.9165588710597264 time: 1703099428.2077658\n",
      "batch_idx: 3 loss: 0.0008225975823532006 R2: 0.9165351171052262 time: 1703099430.848096\n",
      "Training [100%] Loss: 0.0011294719969193732 time: 1703099430.848096\n",
      "weight: [ 1.47100981  1.65945413  0.36036219  0.35886403  0.13155292  1.4484813\n",
      "  1.3132051   1.08529346  2.67147211 -0.1466548  -0.33868951  0.34006776]\n",
      "epoch 499\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0014423802894572469 R2: 0.9165599420337587 time: 1703099433.5643795\n",
      "batch_idx: 1 loss: 0.0007261043929450733 R2: 0.9165674381492168 time: 1703099436.2858963\n",
      "batch_idx: 2 loss: 0.0015266944742865281 R2: 0.9165605496895083 time: 1703099438.9637806\n",
      "batch_idx: 3 loss: 0.0008226216451687988 R2: 0.9165368248842979 time: 1703099441.659459\n",
      "Training [100%] Loss: 0.0011294502004644117 time: 1703099441.659459\n",
      "weight: [ 1.47114079  1.65934703  0.36036226  0.35815037  0.13084556  1.44910695\n",
      "  1.31340889  1.08564338  2.67187712 -0.14665831 -0.33872412  0.34011746]\n",
      "epoch 500\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.001442290622092108 R2: 0.9165616236755186 time: 1703099444.2940664\n",
      "batch_idx: 1 loss: 0.0007260920644032604 R2: 0.9165691087288282 time: 1703099447.0588229\n",
      "batch_idx: 2 loss: 0.0015266860558921314 R2: 0.9165622207719448 time: 1703099449.8151813\n",
      "batch_idx: 3 loss: 0.0008226452841612256 R2: 0.9165385251380039 time: 1703099452.5843048\n",
      "Training [100%] Loss: 0.0011294285066371812 time: 1703099452.5843048\n",
      "weight: [ 1.4712711   1.65924008  0.36036201  0.35743677  0.13013827  1.44973295\n",
      "  1.31361256  1.08599217  2.6722781  -0.14666184 -0.33875873  0.34016704]\n",
      "train_MSE: 0.001132632239325327\n",
      "train_RMSE: 0.03365460205269596\n",
      "train_MAE: 0.027538295302261075\n",
      "train_MAPE: 0.060271895572845645\n",
      "train_R2: 0.9165385251380039\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "weights =[0.68756473 , 1.2864305 ,  0.5411694 , -0.01524038 , 0.5852044 ,  0.6762012,\n",
    "  0.35593897 , 0.22628789,  0.38244092,  0.35140917,  0.86482936 , 0.8531242,\n",
    "  0.09241156 , 0.6720707  , 0.38071635,  0.95416117 , 0.63409   ,  0.40179932,\n",
    "  0.7345088  , 0.6243114  , 0.3178202 , -0.2618623  , 0.18122938,  1.0447433,\n",
    "  0.48699683 , 0.7739934  , 0.38703147 , 0.48046085,  0.5525667 ,  0.52838504,\n",
    "  0.28538367 , 0.30099392,  0.74503726 , 0.67772216,  0.3839896 ,  0.417687]\n",
    "weights = npp.array(weights, requires_grad=True)\n",
    "'''\n",
    "weights = params\n",
    "loss_list=[]\n",
    "r2_list=[]\n",
    "\n",
    "n_epochs=500\n",
    "\n",
    "start_time=time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss  = []\n",
    "    total_r2  = []\n",
    "    print(f\"epoch {epoch+1}\\n-------------------------------\")\n",
    "    for it, batch_index in enumerate (chain(*[X_batches])):\n",
    "        # Update the weights by one optimizer step\n",
    "        \n",
    "        X_batch = X[batch_index]\n",
    "        Y_batch = Y[batch_index]\n",
    "        weights, _, _ = opt.step(cost, weights, X_batch, Y_batch)\n",
    "\n",
    "        # Compute accuracy\n",
    "        predictions = [QNN(weights, x) for x in X]\n",
    "        r2 = R2(Y, predictions)\n",
    "        cost_t=cost(weights,X_batch,Y_batch)\n",
    "        total_loss.append(cost_t)\n",
    "        total_r2.append(r2)\n",
    "        end_timet=time.time()\n",
    "        print(\"batch_idx:\",it,\"loss:\",cost_t,\"R2:\",r2,\"time:\",end_timet)\n",
    "    loss_list.append(sum(total_loss)/len(total_loss))\n",
    "    r2_list.append(sum(total_r2)/len(total_r2))\n",
    "    print('Training [{:.0f}%]'.format(100. * (epoch + 1) / n_epochs),\"Loss:\", loss_list[-1], \"time:\",end_timet)\n",
    "    print(\"weight:\",weights)\n",
    "    \n",
    "    \n",
    "predictions = [QNN(weights, x) for x in X]\n",
    "\n",
    "train_R2 = R2(Y, predictions)\n",
    "train_MSE=metrics.mean_squared_error(Y,predictions)\n",
    "train_RMSE=train_MSE**(1/2)\n",
    "train_MAE=metrics.mean_absolute_error(Y,predictions)\n",
    "train_MAPE=metrics.mean_absolute_percentage_error(Y,predictions)\n",
    "\n",
    "print(\"train_MSE:\",train_MSE)\n",
    "print(\"train_RMSE:\",train_RMSE)\n",
    "print(\"train_MAE:\",train_MAE)\n",
    "print(\"train_MAPE:\",train_MAPE)\n",
    "print(\"train_R2:\",train_R2)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "plt.title(\"MSE value against epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot(range(len(loss_list)), np.log(loss_list))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75b0ae-734a-45b4-95c6-e9fd2cdf6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=xtv_test_log\n",
    "Y_test=ytv_test_log\n",
    "test_predictions = [QNN(weights, x) for x in X_test]\n",
    "\n",
    "test_R2 = R2(Y_test, test_predictions)\n",
    "test_MSE=metrics.mean_squared_error(Y_test,test_predictions)\n",
    "test_RMSE=test_MSE**(1/2)\n",
    "test_MAE=metrics.mean_absolute_error(Y_test,test_predictions)\n",
    "test_MAPE=metrics.mean_absolute_percentage_error(Y_test,test_predictions)\n",
    "\n",
    "print(\"test_MSE:\",test_MSE)\n",
    "print(\"test_RMSE:\",test_RMSE)\n",
    "print(\"test_MAE:\",test_MAE)\n",
    "print(\"test_MAPE:\",test_MAPE)\n",
    "print(\"test_R2:\",test_R2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
