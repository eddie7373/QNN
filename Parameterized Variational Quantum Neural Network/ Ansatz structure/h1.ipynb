{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586edb7a-4b3b-475e-8736-e85f04cf1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as npp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# set the random seed\n",
    "npp.random.seed(42)\n",
    "\n",
    "# create a device to execute the circuit on\n",
    "dev = qml.device(\"default.qubit\", wires=4)\n",
    "\n",
    "@qml.qnode(dev, diff_method=\"parameter-shift\", interface=\"autograd\")\n",
    "def circuit(params,inputs):\n",
    "    qml.RY(inputs[0], wires=0)\n",
    "    qml.RY(inputs[1], wires=1)\n",
    "    qml.RY(inputs[2], wires=2)\n",
    "    qml.RY(inputs[3], wires=3)\n",
    "    \n",
    "\n",
    "    qml.U3(params[0],params[1],params[2], wires=0)\n",
    "    qml.U3(params[3],params[4],params[5], wires=1)\n",
    "    qml.U3(params[6],params[7],params[8], wires=2)\n",
    "    qml.U3(params[9],params[10],params[11], wires=3)\n",
    "    \n",
    "\n",
    "    #qml.broadcast(qml.CNOT, wires=[0, 1, 2,3], pattern=\"ring\")\n",
    "    '''\n",
    "    qml.RY(inputs[0]*1.5, wires=0)\n",
    "    qml.RY(inputs[1]*1.5, wires=1)\n",
    "    qml.RY(inputs[2]*1.5, wires=2)\n",
    "    qml.RY(inputs[3]*1.5, wires=3)\n",
    "'''\n",
    "    qml.U3(params[12],params[13],params[14], wires=0)\n",
    "    qml.U3(params[15],params[16],params[17], wires=1)\n",
    "    qml.U3(params[18],params[19],params[20], wires=2)\n",
    "    qml.U3(params[21],params[22],params[23], wires=3)\n",
    "\n",
    "    #qml.broadcast(qml.CNOT, wires=[0, 1, 2,3], pattern=\"ring\")\n",
    "    \n",
    "    '''\n",
    "    qml.RY(inputs[0]*2, wires=0)\n",
    "    qml.RY(inputs[1]*2, wires=1)\n",
    "    qml.RY(inputs[2]*2, wires=2)\n",
    "    qml.RY(inputs[3]*2, wires=3)\n",
    "\n",
    "    qml.U3(params[24],params[25],params[26], wires=0)\n",
    "    qml.U3(params[27],params[28],params[29], wires=1)\n",
    "    qml.U3(params[30],params[31],params[32], wires=2)\n",
    "    qml.U3(params[33],params[34],params[35], wires=3)\n",
    "\n",
    "    qml.broadcast(qml.CNOT, wires=[0, 1, 2, 3], pattern=\"ring\")\n",
    "    '''\n",
    "    #return qml.expval(qml.PauliX(0) @ qml.PauliI(1)@ qml.PauliY(2)@ qml.PauliI(3))\n",
    "    return qml.expval(qml.PauliX(0) @  qml.PauliY(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a21f8a3e-a4ab-4c9d-bcf5-73c1b9429ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "def norminv(x):\n",
    "    return ((1.0/math.sqrt(2.0*math.pi)) * math.exp(-x*x*0.5))\n",
    "\n",
    "def d1(S0, K, r, T, sigma, q):\n",
    "    deno = (sigma * math.sqrt(T))\n",
    "    if (deno==0):\n",
    "        return 0\n",
    "    logReturns = math.log(S0/float(K)) if ((S0/float(K)) > 0.0) else 0.0\n",
    "    return (float(logReturns) + (float(r) - float(q) + float(sigma)*float(sigma)*0.5)*float(T)) / float(deno)\n",
    "    \n",
    "def d2(S0, K, r, T, sigma, q):\n",
    "        return d1(S0, K, r, T, sigma, q)-sigma*math.sqrt(T)\n",
    "        \n",
    "def bsformula(callput, S0, K, r, T, sigma, q=0):\n",
    "    N = stats.norm.cdf\n",
    "                \n",
    "    def optionValueOfCall(S0, K, r, T, sigma, q):       \n",
    "        _d1 = d1(S0, K, r, T, sigma, q)\n",
    "        _d2 = d2(S0, K, r, T, sigma, q)\n",
    "        return S0*math.exp(-q*T)*N(_d1)- K*math.exp(-r*T)*N(_d2)\n",
    "      \n",
    "    def optionValueOfPut(S0, K, r, T, sigma, q):\n",
    "        _d1 = d1(S0, K, r, T, sigma, q)\n",
    "        _d2 = d2(S0, K, r, T, sigma, q)\n",
    "        return float(K)*math.exp(-float(r)*float(T))*N(-_d2) - float(S0)*math.exp(-float(q)*float(T))*N(-_d1)\n",
    "        \n",
    "    def delta(callput, S0, K, r, T, sigma, q):\n",
    "        _d1 = d1(S0, K, r, T, sigma, q)        \n",
    "        if callput.lower() == \"call\":            \n",
    "            return N(_d1) * math.exp(-q*T)\n",
    "        else:\n",
    "            return (N(_d1)-1)* math.exp(-q*T)\n",
    "    \n",
    "    def vega(S0, K, r, T, sigma, q):\n",
    "        _d1 = d1(S0, K, r, T, sigma, q)\n",
    "        return S0  * math.sqrt(T) * norminv(_d1)  * math.exp(-q*T)\n",
    "    \n",
    "    if callput.lower()==\"call\":\n",
    "        optionValue = optionValueOfCall(S0, K, r, T, sigma, q)\n",
    "    else:\n",
    "        optionValue = optionValueOfPut(S0, K, r, T, sigma, q)\n",
    "        \n",
    "    _delta = delta(callput, S0, K, r, T, sigma, q)\n",
    "    _vega = vega(S0, K, r, T, sigma, q)\n",
    "    \n",
    "    return (optionValue, _delta, _vega)\n",
    "\n",
    "def bsm_iv_generator(num_sample = 100,tao_bound=[0.01,2.0],  sigma_bound=[0.01,2.0], \n",
    "                     money_bound=[0.3,3.0], rr_bound=[0.01,0.2], callput='call', seed=42):\n",
    "    \n",
    "    # input parameters: when callput is not in 'call' or 'put', randomly generate the option price followed by root-finding methods to\n",
    "    # compute the corresponding implied vol\n",
    "    # return: X_input = [time,stock,rr, dividen, option_price]. Y_outpu  = volatility \n",
    "    np.random.seed(seed)\n",
    "    tao_min,tao_max = tao_bound[0],tao_bound[1]\n",
    "    \n",
    "    sigma_min, sigma_max = sigma_bound[0],sigma_bound[1]\n",
    "    moneyness_min,moneyness_max = money_bound[0],money_bound[1]\n",
    "    rr_min,rr_max = rr_bound[0],rr_bound[1]\n",
    "   \n",
    "    \n",
    "\n",
    "    num_sample = int(num_sample)\n",
    "    xx = np.zeros([num_sample,4],dtype='float')\n",
    "    \n",
    "   \n",
    "    xx[:,0] = np.random.uniform(sigma_min, sigma_max,xx.shape[0])\n",
    "    xx[:,1] = np.random.uniform(tao_min,tao_max,xx.shape[0])\n",
    "    xx[:,2] = np.random.uniform(moneyness_min,moneyness_max,xx.shape[0])\n",
    "    xx[:,3] = np.random.uniform(rr_min,rr_max,xx.shape[0])\n",
    "   \n",
    "    \n",
    "   \n",
    "    strike=1.0 #fixed strike\n",
    "    #callput = 'call' # call option\n",
    "    v = np.zeros(xx.shape[0]) # option value\n",
    "    k = np.ones(xx.shape[0]) # strike price, just in order to match the shape of v\n",
    "    \n",
    "    if callput in ['call','put']:        \n",
    "        for i in range(0,xx.shape[0]):        \n",
    "            sigma, T, S0, interest = xx[i,0],xx[i,1],xx[i,2],xx[i,3]\n",
    "            ## use the Black-Schole function in compfin.py\n",
    "            v[i] = bsformula(callput, S0, strike, interest, T, sigma)[0]              \n",
    "            \n",
    "  \n",
    "    v= v.reshape(xx.shape[0],1)     \n",
    "    xx_sample = np.concatenate((xx,v),axis=1) #sigma, time, s, r, v\n",
    "    \n",
    "    \n",
    "    X_input   = xx_sample[:,1:]   # time,stock,rr, option_price\n",
    "    Y_output  =  xx_sample[:,0] # sigma -implied volatility is the predictive variable.\n",
    "  \n",
    "    return X_input,Y_output\n",
    "#  log-transformation of the option value\n",
    "def logscale_vol(x_train_dat,y_train_dat,otm_lower=0.0000001):\n",
    "   # input data: x_train_dat = [time,stock,rr, option_price], y_train_dat = sigma  \n",
    "   \n",
    "    xtv_train_log=x_train_dat.copy()    \n",
    "    ytv_train_log =y_train_dat.copy()\n",
    "    \n",
    "    \n",
    "    #v_lower[v_lower<0.0]=0.0 # V=max(S-E*exp(-rt),0)  \n",
    "    xintrinsic_train=xtv_train_log[:,1]-1.0*np.exp(-1.0*xtv_train_log[:,2]*xtv_train_log[:,0])\n",
    "    xintrinsic_train[xintrinsic_train<0.0]=0.0 ## \\tilde{V} = max(S-E*exp(-rt),0)\n",
    "    xtv_train_log[:,-1] = xtv_train_log[:,-1] -xintrinsic_train\n",
    "    \n",
    "    ## remove intrisinc values below the threshold (otm_lower \\approx machine pricision)  \n",
    "   \n",
    "    ytv_train_log = ytv_train_log[~np.less(xtv_train_log[:,-1],otm_lower)]\n",
    "    xtv_train_log = xtv_train_log[~np.less(xtv_train_log[:,-1],otm_lower),:]\n",
    "    xtv_train_log[:,-1]=np.log(xtv_train_log[:,-1])\n",
    "\n",
    "    return xtv_train_log,ytv_train_log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce82ab6f-af86-47ed-9c2c-ddf97780cd0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maturity time  range:\n",
      "0.5005522117123602 0.5986886936600517\n",
      "Stock price  range:\n",
      "0.9802780852212476 1.0188712833088385\n",
      "interest rate  range:\n",
      "0.030829391446392806 0.07928252270553005\n",
      "option value  range:\n",
      "0.09746900812834913 0.21773104905666268\n",
      "sigma range:\n",
      "0.308233797718321 0.6879639408647977\n",
      "(50, 4)\n",
      "maturity time  range:\n",
      "0.5005522117123602 0.5986886936600517\n",
      "Stock price  range:\n",
      "0.9802780852212476 1.0188712833088385\n",
      "interest rate  range:\n",
      "0.030829391446392806 0.07928252270553005\n",
      "time option-value  range:\n",
      "-2.614763769983766 -1.672877716398405\n",
      "sigma range:\n",
      "0.308233797718321 0.6879639408647977\n",
      "(40, 4)\n",
      "Parameters: [0.64203165 0.08413996 0.16162871 0.89855419 0.60642906 0.00919705\n",
      " 0.10147154 0.66350177 0.00506158 0.16080805 0.54873379 0.6918952\n",
      " 0.65196126 0.22426931 0.71217922 0.23724909 0.3253997  0.74649141\n",
      " 0.6496329  0.84922341 0.65761289 0.5683086  0.09367477 0.3677158 ]\n",
      "inputs: [0.26520237 0.24398964 0.97301055 0.39309772]\n",
      "Expectation value: 0.5773174523060212\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAIHCAYAAADAehvFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAACqCElEQVR4nOzdd3wT5R8H8E+aNk3SJE0nXSCClL2XDHEAAoJMkb1k7703slQQfgoIgsqQJShLpQgoyBIEQZACsndbSkd2mib5/VF77SVNadrkcmm/79erL3qXy93Tfsrle+t5BFar1QpCCCGEEMJLPp5uACGEEEIIcYyKNUIIIYQQHqNijRBCCCGEx6hYI4QQQgjhMSrWCCGEEEJ4jIo1QgghhBAeo2KNEEIIIYTHqFgjhBBCCOExKtYIIYQQQniMijVCCCGEEB6jYo0QQgghhMeoWCOEEEII4TEq1gghhBBCeIyKNUIIIYQQHqNijRBCCCGEx6hYI4QQQgjhMSrWCCGEEEJ4jIo1QgghhBAeo2KNEEIIIYTHqFgjhBBCCOExKtYIIYQQQniMijVCCCGEEB6jYo0QQgghhMeoWCOEEEII4TEq1gghhBBCeIyKNUIIIYQQHqNijRBCCCGEx6hYI4QQQgjhMSrWCCGEEEJ4zAeAwNONIIQQQggheaMza4QQQgghPOYDwOrpRhBCCCGEkLzRmTVCCCGEEB6jYo0QQgghhMeoWCOEEEII4TEq1gghhBBCeIyKNUIIIYQQHqNijRBCCCGEx6hYI4QQQgjhMSrWCCGEEEJ4jIo1QgghhBAeo2KNEEIIIYTHqFgjhBBCCOExKtYIIYQQQniMijVCCCGEEB6jYo0QQgghhMeoWCOEEEII4TEq1gghhBBCeIyKNUIIIYQQHqNijRBCCCGEx6hYI4QQQgjhMSrWCCGEEEJ4jIo1QgghhBAeo2KNEEIIIYTHqFgjhBBCCOExKtYIIYQQQniMijVCCCGEEB6jYo0QQgghhMeoWCOEEEII4TEq1gghhBBCeIyKNUIIIYQQHqNijRBCCCGEx6hYI4QQQgjhMSrWCCGEEEJ4jIo1QgghhBAeo2KNEEIIIYTHqFgjhBBCCOExX083gBAuabVaxMfH4/Hjx0hJSUFaWhpSU1PRtGlTtGrVytPNK3EoD36hPPiF8iDZqFgjxdrVq1cRFxeH8+fP49KlS7hx4wasVmuey65cuRIymQxKpRIymQxSqRRyuRxBQUEIDAyEXC6HUCjk+CcoXigPfqE8+IXyII4IrI7+EgjxUklJSdiyZQs2b96My5cvu2y9AoEAQUFBUCgUCAgIgEQigUgkgkgkgkwmg0QigVgshkgkglAohI9P1l0GFosFmZmZyMjIgMlkgsFggFqthk6ng1arhV6vZ14zGo0AAIVCgfDwcISHhyM6OhoxMTGoW7cumjRpAoVC4bKfiQuUB79QHvxCeZACsRJSTKSkpFiHDh1q9fPzswIoll9CodDauHFj6/z5862JiYme/pXni/LgF8qDXygP4gwq1kix8MMPP1hLlSpVoB2IQCDw+E7MFV8ymcw6e/Zsq0ql8vSv3w7lwS+UB79QHsRZdBmUeDWLxYIxY8Zg9erVDpeJjiyFd1q+hjo1K6N29cqoXqUCfj99AYePnYHBaIReb4Bao0NySio0Wh3S0tXQ6vTQ641QqTWwWCwc/kTOCw8Px4oVK9CzZ09PN4XyAOXBN5QHv/ApD29CxRrxapMnT8ayZcvs5ovF/uja4W3079ERrzepV+gbbS0WC1RqDdRqLVLTVUh+ngaNVgetTg+jMQPGjAwYjRnQaHUwGLKmMzJMMFvMMJuzdppCoQ+EPkL4+4sgFPpA7O8PhTwAUokEUqkYUokYIpEffIW+8Pf3AwCkpKYj+XkaniY+w5OEZ7h6/RbO/XUFJlOmw7ZOmTIFixYtgq+v554bojxyUB6Uhy3KIwcf8vAmVKwRr7VixQpMmDDBbn7bt5thzbLZKBMT6YFWuY9Wq8OJM39h+w8/49vvfszzCLpjx47Ytm0bJBIJ5+2jPCgPT6I8uGM2m+Hj4wOBQOBwGb7n4W2oWCNe6fDhw2jVqhXrsXZfX1+s+3QOBvTqlO9OpDiIv34b0xaswIG4Y3avtW/fHnv37uX0d0B5UB58Qnm4z9OEZ3iv/3jcuvsAr9ariUb1a6J186aoVb2Sw/fwLQ9vRMUa8TqZmZmoVq0abty4wZr/zaqF6N+zo2ca5SHrN+3GyCkL7S43rF27FkOHDuWkDZRHDsqDXygP1zpz7hLeGzABT54mseZPGzcQS+aMf+H7+ZCHt6LhpojX+fbbb+12fAtnjva6HZ8rDO73Ho7s2YBAhZw1f/LkyXj06BEnbaA8clAe/EJ5uIbVasXSlRvwWtt+doUaANStWbVA6+FDHt6KzqwRr2I2m1G9enVcu3aNmVe3VhWcO7KD6dSxJDp4+ATe6TacNa9Dhw7Yu3evW7dLeeSN8uAXyqPwUtPS0W/EzDwvYWa7e+kQypaJLvA6PZWHN/OOvxZC/nPo0CHWjg8A5k8b6TU7Pndp0/I19OvRgTVv3759iI+Pd+t2KY+8UR78QnkUzpX4f1HvrW75FmrBQYF4qXSUU+v1VB7ezDv+Ygj5T1xcHGu6epUKaNPiNQ+1hl8+XTgFIcFK1ry1a9e6dZuUh2OUB79QHgVntVrxzdY9aNiyJ+7cy//yZJ0alQv1cIAn8vBmVKwRr3LkyBHWdLdOrb3mKNXdgoMCMaTfe6x53377LQwGg9u2SXk4RnnwC+VRMDqdHh+Mmo0PRs+GXs/+3YSFBtstX7dWwe5Xs+WJPLwZv/9qCMklISHB7pJCi9cbeag1/DSkX1fWdGpqKvbv3++WbVEeL0Z58Avlkb9/4m+iYcue2Lh9r91rDepUx4Vfd0IZqLCZX63Q2+MyD29HxRrxGqdPn2ZNK+Qy1K1VxUOt4aeyZaLx5msNWPOOHj3qlm1RHi9GefAL5eHYju9/RoOWPfDPtZt2rw3/oBt+/2kTtDo90tJVrNca1Kle6G1ymYe3o2KNeI179+6xpmtVr0RDleShTfOmrOnLly+7ZTuUR8FQHvxCebCZzWZMX7ACPQZPsbvsKZNJsWPDJ1izbDb8/UU4fe4S6/WoyHDEREcUaftc5eHt+PeXQ4gDjx8/Zk2XiSnaTqK4qla5Amv66tWrsFqtLu8hnPIoGMqDXyiPHFarFZ37jsP+g7/ZvVajaiy++3o5KlZ4mZlnW6w1rl+ryG3gKg9vR2fWiNew3flFlgrzUEv4rWrlV1jTarXa7nfnCpRHwVAe/EJ55BAIBFAGyu3mly0TjbOHt7MKNQA4efYia7pJw9pFbgNXeXg7KtaI13j69ClrOjoy3EMt4bfS0REICGAPjHz9+nWXb8fTeQiCq0EQXA0GgxFA1rA+2fOyNe84EKGvNIWoVC3EVG2O0VMXw2jM4LSdlAflAfB3f7Vg+iiIRH6sefcePEZ01beQmpbOzHuWnIIbN++ylmvSsFaRt89VHt6OijXiNdRqNWs6SKlwsGTJJhAIULY0uzfxpCT7IWKKyhvyqFElFktmj8OaT2ZBLpNi1fpt2LDle07bQHnkoDz456XSURjxQXe7+Smp6WjecRB0Oj3MZjNO2ZxVk0ol+Q7eXlBc5eHt6J414jUsFgtrWigUum1bx06ew5vtP7Cb7+PjA7ksAOXKxqDlG40wfnhfhIYo0aBFD1y8nPWYvq+vL87/ugM1q7F3ZI8eJ6BKow5Qa7QAgJioUvjn9F67cfJcwfbSRlpamsu3wWUehbVi8VSkpKYjLV2F3fsP4/rNux65F4byyEJ58NPMiUPw28lz+Psf9himFy9fw8q1W3Ag7ji0Oh3rtUb1a8LPj31GrrC4yMPbUbFGvIbtzi4z08x5GywWC9JValy8fA0XL1/D5p37ce7wdmxcvRD13uoGkykTmZmZGDhmDs4e3s5q84jJC5lCDQDWr5zvlkIN4Gbnx4c8CiK2fls8T0kDAPTq2haD+nThvA2URw7KwzNSUtORkJiMtHQV9AYjrFYrxGJ/lI6OQExUKXw0dwJadx1q976ZCz/Lc32vN67nsrZRsfZiVKwRr2G787M9cnWnbp1ao16tqlCptdj781Fcic/qiyghMRkrvtiCTxdNwayJQzF36WoAwIVL8VixZjMmjR4AIKsPo9zj6w3s3RmtWzS1246rSCXse0D0er3Lt+HJPICsyydWqxVWqxUAmH9tz9T8sHklEhKTsWz1Ruz4IQ6d2rZAl/YtOW0r5ZGD8nA/s9mMs+cv48+L/+DC3/H44/xl3Lx93+Hyfn6+AJw7w/laozpFbGUOLvLwdlSsEa9h20dRhsnE2bZbN2+K/j07AgAmjuqH8NhmyMjI2n78jdsAgBkTBmPvz78yl0PnLF2NTu2aI0gZiLEzljLriokqheULJ7u1vRKJP2vaHTs/T+YBZP0eHz5OwKMniahQ/iU8eJR1Q3dpm36fmv13BsDXV4gu/cZj4/a9nBcHlEcOysN97t5/hG+27sU32/bg0ZPEAr/PZMp0ajsjB/XA603qO9s8h7jIw9tRsUa8hlzOPlWuUmk80o5AhRyyAClSMrKelMoejNjX15d1OVSvN2DwuHkoHR2BpGcpzPvdefkzm5/NB4XJDR8Uns6jU9vm+OzLreg2cBLatGiKg0dOAgC6vJv1wR935CS2ff8TmjSsDavVis+/3AYAqFmtIqftBCgPgPJwZx4HD5/AkpUbcOLMBbesP/usKQCMHdobKxZPdem9hlzk4e2oWCNeQyqVsqb1/3URwCWVSoON2/ciJTXnkfb3O7Zivq9RtSLrcuhvJ86x3u/uy5/ZfH3Zl2DMZtffL+PpPBbPHgt/fxF27fsFy1dvQmSpMEwZ8wHmTR0BAAgNUeJK/E3s+ekoMjPNiI4Mx7RxAzF3ynBO2wlQHgDl4Y48bt15gImzP8mzU1tHZDIppBIxfHx8oFJrodO9+CxWdqFWqcLLGD+ir8sfCuEiD29HxRrxGhKb+xp0NkOjuNOAUbMwYNQs1jypVIL5U0egwztvsebbXg7NxsXlz2y2O9Psna0reTIPAAgIkOLj+RPx8fyJeb5er3Y1XDy+m9M2OUJ5UB6uzEOn02P24lX4fP3WfC9hVo4thwZ1q6NBnep4tV4NVKtcgdWn2ulzF9GkdZ8Cb/f6zbuo3qQTvlg+Gz3fa+uyoo2LPLwdFWvEa9ju/LjuTNNWp7ZvYfgH3ezm214OzcbF5c9sXHSHwLc8+Izy4BdvzuPOvYfo1GcsLl/9N8/XlYEK9OraFgN7d0btGpUdrsdisWDcjI/yfK1B3ep4+43G2PDt90hITGa9ptZo0XvoNBw+dgbrPp0Lf39R4X+Y/9DQUi9GneISr2Hbp08mh6fKu3VqjcWzx6Jdq9eZeVt3/YROfcbmeRRYo2pFNMo1bt5LpaM4ufzJJU/mQexRHvzijjwuXLqKhi175lmohYUGY8P/5iPh+jGs+nhmvoUaAOzcE4c///onz/Xs+/ZzfDhzNG5fOIjFs8dCIZfZLbdp+z606DSI6YaFuBcVa8Rr2B59cfkofOvmTTF9/GAc2L4aQ/t3ZeYfPnYGW3f9mOd7PHmwyMVlBK7yGDByFmSl6zMfCk+eJqFj7zEIiKkPZdlG6Ddixgtv3t60fR+qNe4I/4jaUJZthN5Dp7qlrY5QHlnuPXjMDEFl+8Ulb8zj+Kk/8Ub7AUh+nsqaLxL5YdKo/rh5/icM7NOlQGe6MjJMmLUo7/7TvlwxFxGlQgFk3eoxffxg3P7rIPp0e9du2ZN//IWuAyYgM9O5p0lt0WXPF6PLoMRr8OVU+dK547Hjhzikq7KGk5n/8Rfo0eUdXvVQbja7v/d0LvL499Y9bN65H4P7dmGeuu01dCqOnzqPWROHIjVdhVXrt8FH4INvVi/Mcx3rN+3GkPHzEPtKWXy6cAqsVitu3Lrn9rbnRnlkCQsJwvb1HzPTp85dwqr129CgTnW3tz03b8vj/sMn6NJvPDQa9igCVSqWx/ebVqBSbDmn1rdu43e4c++R3fze77dDx7bN7eaHhgRh8xdL0LZlMwwcOwdabc5DCb+dOIfJc5ZjxeLCHwBxkYe3KxFn1v79918MHDgQZcuWhb+/P0JDQ9GyZUt89913nm4acYLtkamnijdloAIjB+WMpXfrzgPs3BPnkbY4Yvs0lTt2flzksX7zblgsFnTv3AYAcPXaLRw7+Sdq16iMBTNG4bOl0xEeFowt3x1weDZn4fJ1AICfdqzBgJ4dMWpwT3z+0QyXtzU/lEeWgAApund5h/nKHhh88n+dR3PFm/LQaHTo1Ges3eXG5q+/ij9+2eZ0oaZSaTBm2hK7+S+/FIPVH8/K4x05unVug1MHtyAyIow1f+XaLdiwufAPj3CRh7cr9sXazz//jJo1a+Lrr7/G/fv3kZGRgefPn+PIkSPo1q0b+vfvT6dgvYRtTp480zZuWB9IpTk3EC/+dD2v/o5MNpclXDWGX25c5PHLb6chFArRsG4NAMDNO1m9sJeJiWC2WSYmEmazGXcf2J8peJacggePnsLfX4S23UcgIKY+Ql9pii++3uHytuaH8rB3Jf5fHD52BuXKxqDzuy1c3tb8eEseZrMZPQZPtnuyvO3bzfDTjjWQywOcXmf2wYutrz9fAIXC/t40WzWrVcKezf9jPVUKZA2nd+nKdafbA3CTh7cr1sXa48eP0bNnTxgMWY9MV6lSBQsWLED37jlnRTZt2oQ1a9Z4qonECXYDI/t47s83LDQYg3p3ZqavXr+FPT8e8Vh7bGWPrpBNJCr6E1u2uMjj1t2HCAkOhEQizqcdjovk7CN0ozEDzZs1xHdfL4dQ6IORkxfhSnzeT9O5A+Vhb/nqTQCACSP6wYfj/8veksdXW37Aj4eOs+ZVrPAytn75UaGewkxITMYnn39jN3/4B93wRtMGBV5Pw3o18OWKeax5JlMmJs9Z5nSbAG7y8HbF+p61zz77DOnpWZ2XyuVynDhxAsHBwQAAHx8fbNuW1YP2kiVLMGzYMDr1ynO2vVrbdqToSm80bQBriv2TUrn9b+l0/G/pdIevHzuw0cWtKjjbQaPdcaTKVR65z0hUKPcSAOD+w6yhjKxWKx48egqhUIiXy8QAAAz/dT4qFvsjOCgQwUGBSElNx5ghvVApthx27onD9wcO4+bt+6heJdYtbbZFeWTlke3J0yRs//5nhAQrMeC/Ydy45A156PUGfLhsLWtecFAg9m/9vNBdAHXqMzbP+R/Py7tvvPz069EBl6/+i0/XbGLmHTn+B06fvYjGDWs7tS4u8vB2xfrM2v79+5nv33jjDaZQA4AuXbow3z9+/BgXLrhnmA7iOtlnSLNJxI6P7ks6Li4rcJFHuZdikPw8jfnAr1r5FTRrXA+XrlzH3CWrMGrKIiQ/T0Xv99sxl3AkUXUhiarLvGfkwB4AgAWfrMXab3bi6O9nIZNJ8Wq9mgDAPI1ocGOP/5RHXdbv9/P1W5GRYcKID7qzbicAKI9sa77aYTe+57drlyL2lbKFas+1G7fxx/m/7eafPbwdMpk0j3e82IczRiEqMpw1b+HyL51eD10GfbFiW6wZjUbcuHGDmS5Xjn0Tpu305cuXOWkXKbyMDHankrb3TJActr2a2w4q7Qpc5PH2m41hNptx7q8rzLyt65aiXavXsWz1Jnz73Y/o/X47/G/JNIfrmDVpKMYO7Y3Dx85g4uxlqBz7Mn7cvhpRkeHMfUUCgcCtl+KczcNqtcJoNEKr1TJfOp0OGRkZDu+N9JY8tFod1m3cBbHYH6MG92C95qk83HFVpSh5aDQ6LP3fV6x5bzVrWKS+Gqs06mA3r1+PDmhQt/BP4kqlEkwexX445OCRE7h89YaDd+SNi/2Vtyu2v5HU1FTWTk2hULBetx1kNzmZ3UszX1mtVqhUKojFYohEIt50Z5Efq9UKg8EAlUqFlJQUPHnyBImJiUhOToZKpYJWq0VaWhpSUlKQkpICtVoNo9GIjIwMmEwmZGRkQKfT4fnz56z1+vkV2z/fIsuwuQSj1Wpx48YNqNVqJCQkIDk5mSkC1Go1NBoN9Ho9DAYD9Ho9NBoN1Go1dDod85WRkQGj0Qij0cjkkps78hjctwtWrt2C7/YeQrPG9QAAMdER2L9tlcP32F6+Fon8sHLJNKzMo4D459pNAMCwAe+7tfi3zWPDhg3Ys2cP1Go11Go1DAYDTCYTDAYDjEbjC/vk8vPzg0QigVwuh0KhgEwmw927d22W4WceAQFSpNw5neeynspj2rRpmDNnDkQiEUQiEXx9fSGRSCCTyRAQEACJRAKxWIzAwEAEBQVBoVBAoVAgODgYERERCAwMhEwmg1KpRHBwMGQymd1lUGfyOHDomF1/aotnjS30/n779z/nOd/2vrPCGNLvPSz69EtWe388dBw1qlYs8Dps86Aza/aK7aed7dHni6a9oegBso7WlEolgKw2BwYGIjg4GHK5HIGBgcwORqlUQqlUQqFQICQkBMHBwcxOx9/fH/7+/pBIJAgICGCm/fz84OPjAx8fH1itVpjNZuZD2WQyQaPRQKPRQKfTQavVQq/XMx82Wq0W6enpzAd/YmIikpKS8PTpU6SkpBS508S8iOg/tENp6SrW9IIFC7BgwQK3btMdeVSKLYc+3d7Fph37sGD6KAQHBbp0/cdPnUdUZDiWzhnv0vXass3j5s2buHnzZqHXZzKZYDKZoFKp8Pjx4zyXoTwcs80DyNq32h6AFIXtmUFn8jj8G7uYbfH6q2hYr0ah2mEymdBz8BS7+T/v/MIlBbFUKkH3zm2wav02Zt7xU+cxY8KQAq/DNo/AQNf+XRUHxbZYCw4OhkAgYIoytVrNel2lUtkt7w1y3wdhtVqRlpaGtLQ0zzXICQKBAAqFApGRkYiKikJISAiUSiUCAgKYo9Tg4GAoFArmCDf7SyqVYvDgwTh79iyzPjqz5lhqGvvvWygUQiaTQS6XIywsDOHh4cxZA4VCwTp7kF3wy+VySKVSSKVSSCQSiEQiiMViprDv3r07zpw5w2zDXXlsXL0IG1cvcsu6Rw3uiVGDe7pl3bnZ5jF27Fi8+eabkMvlkMlkEIvF8PPzYx04iUQiCIVC5kPfYrEgMzOTObOZ+2BJo9Fg8uTJuH49p+sEysMx2zy++uortGjRginYss9yajQa5sDUYDAgNTWVOShNT09HcnIyEhMToVKpoNFokJqayny22J4dLWgeVqsVcb+eYs3LPcyds/oOz7tPwTYtXyv0Om293rgeq1g7efYijMaMAj+xapuHt3wec6nYftr5+/sjNjaWuW/t9u3brNdtp2vUKNxRC9cUCgWMRiP0ej30ej1SUlKQlpYGlUoFlUrF7MCzdxrp6el4/vw5UlNTmZ1O9qUsvV4PrVYLo9GY75kvoVAIPz8/yGQyyGQySKVS5sNdLpdDLpczH/rZl2TCwsIQERGB8PBwhIeHMwVBUe5Dsb2sIAso3E2xJYFWp2dNHzx4EC1btnTpNoxG9g3glIdjtnm0bdvW5XnMnTuXNU15OGabR+nSpVGmTBmXrNtsNkOn06FRo0a4evUqM7+geVyJ/xdPE56x5rVuXrh71S5evoYdPxy0m59w/Vih1ufIG03rs06O6HR6nL1wmblU/iK2eQQEON9/XHFXbIs1AHj33XeZYu3YsWN4/vw5QkJCAIA1ekFUVBTq1SvYH5WnCQQC5mxTYGAgIiIiXLJei8UCk8nE6kk6u0jjug8kR1JT2fdwBCkVDpZ0zGq1okGL7jh/8SrEYn/c+SvOrjduT9LrDXi5diskJj1H6egI3Dj3Y759WjmSls4+k+yOywquyMMZ8ddvY9TURTh97hIUchl6vdcWH8+fkOf9LR/97yt89e0PuHXnAaxWK37b/7VT/Ui5WknOIzUtHf1HzsJff8fj2fNUhIcGo0+3d/HhjNEe27e4Mw+hUAi5XA6djj00VEHzOHbyT9Z0mZjIQj0BmpFhQsOWPezmt2nxGkqFhzq9vvyEhgShRtVY/P1PzoMFf/9zo8DFGhf/P7wdPz6F3WTs2LHMgwQajQbNmjXDhx9+iO7du2P37pyhMaZOnVri+1jz8fGBv78/c9lLKpXC39+fN4UaYH8pWyF/cW/btrbs3I/zF7OOdgf16WJXqJ2/+A+6D5yEqCpvwj+iNkpVbIb2PUfhyLEzea0uX7fuPMDoqYvRuFUvxFRtDklUXYgj6yC66lt45/3h2LJzv92lEolEjEkj+wMAHj5OwLJVG53erslkgtHIvvfG9gEbV3BFHgWVmZmJDr1H4/S5S1g4YzSaN2uIlWu3YPGn6/NcXq83oG3LZnj5pWi3tamgSnoe6SoNrv17B0P6dcXKxVMhEAiw+NP1WL1hu9valx++5/EsmV10169drVD3VM/7aLXdU5YAsPmLxU6vqyDKlY1hTds+IOEIV3l4u2J9Zi0mJgZbt25F165dYTQaER8fjzlz5rCW6d27N0aNGuWhFpKCslgsRT5zYDabMWfJamZ63LDerNc3bN6NoRMWsAqopGcpOBB3DAfijmHO5GGYP73gfyuXrlxn3ceR7cnTJDx5moSDR07gQNxxfPfNctbrwwZ0w9yP1kCn0+Pjz7/GmKG9nOoEU6PV2c1z9WUFV+ThjEO/nsKtOw/QuV0LTBo9AGq1Frv2/YLVX+3A3Kkj7JafN20kAODPi//kOWA1l0p6HjFRpXDtj/2s0STGzfio0EMTFRXf86hdoxL69+iI1HQVUtNUqFHV+Y6bz124giUrNtjNnz5+EEJDgpxeX0E0rFsDGRkmyGUBkMsCULdW1QK9j4s8ioNiXawBWZdCL126hI8++ghHjx5FYmIiAgICULt2bQwePJg19BThr7S0NLvBfsOc3On8eOg47j98AgBo3KAWyr+cc4/KpSvXMXzSQqZQe7VeTbRr1Qynzl7CwSMnAGR1qtqgbnW0fbtgN/v6+AhQObYcGtatgajIMARIJbh7/zF27omDWqMFAOzadwhnz/djPeklk0nRvvUb2PHDQWg0Omzavg9jhvZ2tBk7z1PS7eZlX/53FVfk4Yybtx8AyLokBAByeQCClAo8S05Bukpd6B7duVDS88jdZ5bFYsFPv/wOAGjxRiO3tS8/fM+j87st0fndwt/PmJmZiSHj5+X52rSxgwq93heZOnYgpo4d6PT7uMijOCj2xRoAVKpUCd98Yz8eGvEeeT3x6uwH9Ndb9zDfd7HZGS5ZsZ55yOLll2Jw/MeNzGPtTdv0wamzFwEAH36yrsDFmqOd7puvNUCvIVOZ6XsPHts9lv9e+7eZG4O/+vYHp4q1vHp+l0gkeSxZeK7Io6he1BcZX1AeWYzGDPQbMQOHj53BmCG90KPLOxy1jK2457Hyiy2se8eyfTxvQoEGaucaF3kUB/y5IYmQfNh2WuzvL3JqiBSz2cy6cbdR/Zqs1346/Dsz3a5VM1b/Q53btWC+P3vhMpKesTvnLSijMQPX/72DnXviWPOrVnrFbtnc7bsSfxPPklMKvB2DzVOa/v7+Lu9HsKh5OKtC+ayzoPcfZZ0ZTVepkZauRlhoMBRyGQwGo919L3xBeWT1o9XqvSHYuScOc6cMz3dMXXcrjnlku//wCeZ+tMZufkCABMM/4OdVJC7yKA6oWCNeQavVsqZlAVKn/kNfib8JlVrDTNeuUZn5/s69R9Bqcx4dL/dSadZ7bW+cvXz13wJvFwDmLV0NQXA1iCProPKr7bH/4G/Ma5NG9Ue1KhXs3hMVGY7wsKy+hqxWK06fu1Tg7eltjlTdcZRa1Dyc1eqtJij/cmn8fPgElq/aiCHj5sFisWDEB91w/+ETSKLq4qWaOWcxfz99Hhs270bif4X1T7/8jg2bsx4quvfgMQTB1RBRqfB9VzmjpOeh0ejQpHUfHD91Hq2bN0WlCi9jx/c/49ffs/pMpDxcw2q1YsSkD6Gz6QYDACaN7M9JsVgYXORRHJSIy6DE+9nerCuXOXcD6uOnOQMiy2UBEIv9mennKWmsZRVy9rptt1XQp5zy4+vri8WzxmDS6AEOlwkPDUHSs6wzao+fJhV43bmLUsB+aDVXKGoezvL19cXeLZ9h1NRFmLnoM8gCpBgzpBdmTBiCJwn2v5uvt+7Bpu37mOnsp2oH9X2P6QvKV8jN7q+k55Gckor4G1n9WsYdPYm4oycBAK83qYe3mjWkPFxk195D+PnwCbv5gQo5xg3v4/btFxYXeRQHVKwRr/DkyRPWdJSTfaPl7sfH9hF6dw9N9vabjSELkEKt0SL+xh3sj/sNGRkmTJn3KU6du4Rd3yzPs6+w3EVjXsPjOJKuYu/8socnc6Wi5lEY1apUwLEDG+3mly0TbTf+ZH697P9z7RYAYMzQXi5vY15Keh555ZMb5VF0KpUG42Z+lOdr08YNhDKQv11hcJFHcUDFGvEKSUnso/WwUOeGI1EG5hyt2R7JhQQrWdNqDftRctvlnR0PsXHD2mjcsDYzfeLMBTRr2w8AsO/nX7Hmqx0YO8z+yFelzrmU4szO1ra97ngMvqh5eNLxU3+iZrWKmDCiLyfbozzyR3kU3Zwlq+xGPQCybqcY68TDSZ7ARR7FARVrxCvYHqnGRJVy6v1REeHM92qNFgaDkbkUWv7l0ggIkDD3rd2+95D13tt32dOF6fcot9ca1UWQUsGMh3fs1J95FmtJyTkPMkRHhtu97ohtp5phYa4/qi9qHp607MPJnG6P8sgf5VE0f/9zHascdDA8c8KQQo2AwiUu8igO6AED4hVSUthPQwYrnTu7Vb1KBdYNtrk75BQKhWjTPGdQ4wNxx5gn2axWK3bvP8y81qBOddZQLf1HzoQguBoEwdXwxrv9Wdv84cDhPJ9QPHPuEmvg4rwuqz55msTcryYQCNC4Qa0C/qRg+nDL5o6hW4qaR2HEX7+Ntzp8AHFkHYTHNsP4GR/ZjRcLZA1v1KHXaJSu1hziyDooU70FZi78H9O1xLUbt+ETUh1zl6xye5sByuPYyXPM/5HcX2Vrvg2A8igKq9WKUVMW2/XpBgCloyMwsHdnt2zXlbjIozjwBSAAYH3RgoR4UkJCAms6opRzY9v5+vqiWaO6zA24f5y/jFdzdY8xffwg7PnpKMxmM+4/fII33h2Adq2a4cSZv3DuryvMcjMnDinwNj8YPQdCoQ/atGiK2PJlIRAIcP3mXez56ShruXZ59NuW++nP6lUqOHUZxXZQZJnM9X0rFTUPZ2UPb/TwcQIWzhiNC3/HY+XaLVAGyu16zM89vFFYaBCWrNiAxZ+uR0R4KEYP6YXKFcujdfOmWL5mE8aP6Ov2+3lKeh5VKpbH9vUfM9N7f/4VO/fEoWHd6gBAeRTBtt0/4eQff+X52pzJw+DvL3LLdl2JizyKAx9QoUa8gG2/RYXpnT33Ueb3Bw6zXqtTswpWfzKTOcv1x/m/MWvR5zj06ylmmRkTBqN9mzed2mZKajq27voJc5euxpwlq7Bt90/Q6w3M6326vYsBvTrZvW/3/l/ybHdB2O78pFLXP7LvijyckT28UduWzTBp9AB8uWIehEIhVn+1w27Z7OGNZk8ehmEDujH3QuU+m9q1w9vQavXYtvtnt7YboDzCw0LQvcs76N7lHXTr3IbpsHXSqP7MMpSH8zQaHabM+zTP16pXqZDnfoWPuMijOKDLoMQrPH36lDVdmCPVDu+8xQyPc+rsRdy9zx4zcmj/9/HHL9vQtUMrRJQKhZ+fL0JDgtD27WY4tHsdFs0a69T2Ppk/Ed07t0HFCi8jSKmAUChEQIAElSq8jL7d2+PwD+ux+YsldpdB1Wot9scdA5A19FS/Hh2c2q7tDbvuGBTZFXk440XDG+Xm6+vLjEPpaHijJv898JG7GHeXkp5Hbj8eOo7rN+/i9Sb1UL9OdWY+5eG8JSvX44mDLn1WLJrK/B/gOy7yKA7oAQPCe2azGRqNzROZhbgHRCgU4sMZo9BvxExYrVZ8umYzPv9oBmuZBnWr2w2snp/8uogY3O89DO73ntPtXPvNTubs29QxA50epsb2UXhX7/xclUdRFWV4o+wbvm0fHnEHyiPH8tUbAQCTbfoXpDycc+feQyxfvSnP1zq88xaav/6qS7fnTu7Oo7igM2vEK/n4FK438D7d2qNe7aoAgA1bvs/zcXdP0usNWL4maydcOjoCE0f2c3odtp38Bge7vxuHwuZRUK4e3sjHJ2vXZ9uHnjtQHlkuXLqK46fOo3JsObzTsplNeykPZwQpFejg4JaMT+ZPdOm23M0TeXgjOrNGeC+vJ8zy6kS2IAQCAf48urOoTXIbiUSMhOvHi7SOZzYjLISEhBRpfbZcmUdB2Q5vdO6vK6zhjV6u1QqlwkOQcP04M7xR/I3brOGNwsNC8FazhgCAh4+zbgC3HUrMHUp6HtmyR5GYNKq/3aV/ysM5QcpA7PhqGU6evci6FDpuWB9UKP+SS7flbu7Oo7igM2uE93x9fZkj72y29zmQLE8Tntndx1KmTBmXbsMTeWQPb/RqvRqYuegzHP39LDO8kS3b4Y16DJ6CHoOnYMEnXzDLZD9t+/abjd3absojy4NHT7F7/2FElApFr67t7F6nPJwnEAjQo3MbZjpIqcDsycNcvh134iKP4oLOrBHe8/X1RVRUFB49ynkg4PHTJNSpWcWDreIn28f45XI5qlat6tJteCoPVw1vBADf7Y2DVCrJs3BwJcojS5mYSJiSLjlcF+VROFUrvcJ8P3fKcKdHV/E0LvIoLujMGvEKoaHsp6ls73MgWf6+eoM13ahRI7c8FebNeVz/9w7ijp7ChOF93f7hRnm8GOVReNnF2ivlymD4B93dsg134iqP4oDOrBGvUKoUe7iWhKRkB0uWbH/9fY01XbNmTQdLFo0351EpthzMyZc52Rbl8WKUR+FVqVgeQNZDBSKRe+9TdAeu8igO6Mwa8QoRERGs6eyONUmOpwnP8Mtvp1nzatSo4ZZtUR4vRnnwS3HMI7sfxg7vvOWW9bsTl3kUB1SsEa/QsGFD1vRPv/ye57ibJdmmHftYYwRKpVK0a+eee4AojxejPPiluOax6qOZeY4vzHdc5lEcULFGvELHjh1ZOyS1Rotffz/rwRbxi8Viwcbt+1jzunfvDqVS6ZbtUR75ozz4pTjnIZN53/BMXOdRHFCxRrxCZGQkXn2V3St37vEzS7o1X+3AjZt3WfMGDBjgYOmiozzyR3nwC+XBL1znURxQsUa8RqdO7IGJN+88gH9v3fNMY3jk9t0HmDyXPURWbGwsmjRp4tbtUh55ozz4hfLgF0/l4e2oWCNeo3v37vD392emMzMzMXH2Jx5skeep1Vr0HjodBoORNX/VqlVuv4+F8rBHefAL5cEvnszD21GxRrxG6dKlMW7cONa8Hw8dx96fjnqmQR72PCUNb3X8AH+c/5s1f8SIEWjZsqXbt095sFEe/EJ58Iun8/B2AisXI+cS4iIqlQqxsbFITExk5onF/vh55xd487UGHmwZt46dPIehExbYXVaJiorCtWvXoFAoOGkH5ZGF8uAXyoNf+JKHN6Mza8SrKBQKLFmyhDXPYDCiXY+ROHr8Dw+1ihtWqxXx12+jS99xeLP9B3Y7vtDQUOzZs4fTHR/lQXnwBeXBL3zMw5vRmTXidSwWC/r27YutW7ey5gsEAgzp9x4WzRqLkGClZxrnYmnpKty4eQ8HDh3D7v2H7Z6gyla6dGkcOXIEsbGxHLeQ8sgL5cENyoNfvCEPb0XFGvFKmZmZ6N69O77//nu714KDAjFt7ED0fK8toqNK5fFublmtVmi1emh1Oqg1OqSlq5CUnILnKWlIV2lgNGbAYDRCbzBCo9EhTaXGnXuPcOPWXSQ9S3nh+itVqoSDBw+ibNmy7v9hHKA8clAezqE8KA/yYlSsEa9lMpnQq1cv7Nq1y+EyjerXRMe2zdGwbnWUeykGUZHhBRoo2Gq1wmTKhN5ggE5ngFqjhVanh1anR0pqOp4mPkO6SgOtVged3gCtTo+0dDXUGi1S01RQqTXQ6Q3QG4xIS1dDp9O78kcHkHUZYcGCBRg0aBD8/Dw/LiDlQXlQHo5RHvzKw9tQsUa8WmZmJpYtW4YPP/wQOp3uhcv7+fkiKiIcIcFK+Pn6QiAQwJRpQkaGKetIUauDWqOFXm+ExWLh4CdwXmRkJLp3747Zs2cjKCjI081hoTwoD0+jPPiFz3l4FSshxcD9+/etXbt2tQIodl8CgcBavnx568SJE61nzpyxms1mT/+6X4jy4BfKg18oD+IsOrNGipVTp05h3bp12LdvH1QqlaebkydfX1+EhYUhLCwMSqUSEokE/v7+EIvFkMvlkMlkiI6ORmxsLCpWrIhy5cpBJBJ5utmFQnnwC+XBL5QHKSgq1kixZDQa8euvv2Lfvn04ceIE7t69C72+aPdh+Pr6IiAgAHK5HJGRkQgJCUFAQAACAgIglUoRGBgIhUIBpVLJ7NQkEgkUCgVKlSoFuVwOuVwOsVhc4nrrpjz4hfLgF8qDvAgVa6REsFqtSEpKwv379/Ho0SOo1WqYTCZYrVaIRCKIRCL4+/tDJpNBoVBAIpFALBZDKpVCIpFALpezho4hRUN58AvlwS+UB7FFxRohhBBCCI/RCAaEEEIIITxGxRohhBBCCI9RsUYIIYQQwmNUrBFCCCGE8BgVa4QQQgghPEbFGiGEEEIIj1GxRgghhBDCY1SsEUIIIYTwGBVrhBBCCCE8RsUaIYQQQgiPUbFGCCGEEMJjVKwRQgghhPAYFWuEEEIIITxGxRohhBBCCI9RsUYIIYQQwmNUrBFCCCGE8BgVa4QQQgghPObr6QYQwiWtVov4+Hg8fvwYKSkpSEtLQ2pqKpo2bYpWrVp5unklDuXBL5QHIfxExRop1q5evYq4uDicP38ely5dwo0bN2C1WvNcduXKlZDJZFAqlZDJZJBKpZDL5QgKCkJgYCDkcjmEQiHHP0HxQnnwC+VBiHcQWB39zyTESyUlJWHLli3YvHkzLl++7LL1CgQCBAUFQaFQICAgABKJBCKRCCKRCDKZDBKJBGKxGCKRCEKhED4+WXcZWCwWZGZmIiMjAyaTCQaDAWq1GjqdDlqtFnq9nnnNaDQCABQKBcLDwxEeHo7o6GjExMSgbt26aNKkCRQKhct+Ji5QHvxCeRDifahYI8VGamoqpk+fjq+//homk8nTzXELoVCIhg0bolWrVhg2bBjCw8M93SSHKA9+oTwI8V5UrJFiYc+ePRg+fDgSExNfuKxAIHB4qcebyGQyjB8/HpMnT4ZcLvd0c1goD8rD0/icByHOomKNeDWLxYIxY8Zg9erVDpeJjiyFd1q+hjo1K6N29cqoXqUCfj99AYePnYHBaIReb4Bao0NySio0Wh3S0tXQ6vTQ641QqTWwWCwc/kTOCw8Px4oVK9CzZ09PN4XyAOXBN3zKg5DComKNeLXJkydj2bJldvPFYn907fA2+vfoiNeb1Cv0jc8WiwUqtQZqtRap6SokP0+DRquDVqeH0ZgBY0YGjMYMaLQ6GAxZ0xkZJpgtZpjNWR9iQqEPhD5C+PuLIBT6QOzvD4U8AFKJBFKpGFKJGCKRH3yFvvD39wMApKSmI/l5Gp4mPsOThGe4ev0Wzv11BSZTpsO2TpkyBYsWLYKvr+eeG6I8clAelAchrkLFGvFaK1aswIQJE+zmt327GdYsm40yMZEeaJX7aLU6nDjzF7b/8DO+/e7HPM9odOzYEdu2bYNEIuG8fZQH5eFJfM+DkKKgYo14pcOHD6NVq1ase2t8fX2x7tM5GNCrEwQCgQdb537x129j2oIVOBB3zO619u3bY+/evZz+DigPyoNP+JYHIUVFxRrxOpmZmahWrRpu3LjBmv/NqoXo37OjZxrlIes37cbIKQvtLv+sXbsWQ4cO5aQNlEcOyoNf+JAHIa5AxRrxOhs3bsSAAQNY8xbOHI2ZE0vmzvf30+fRvudopKvUzDy5XI74+HjExMS4ffuUBxvlwS+ezoMQV6BijXgVs9mM6tWr49q1a8y8urWq4NyRHUwnmyXRwcMn8E634ax5HTp0wN69e926Xcojb5QHv3gqD0JcpeT+7yVe6dChQ6wPIgCYP21kif4gAoA2LV9Dvx4dWPP27duH+Ph4t26X8sgb5cEvnsqDEFcp2f+DideJi4tjTVevUgFtWrzmodbwy6cLpyAkWMmat3btWrduk/JwjPLgF0/kQYirULFGvMqRI0dY0906tS7xZw2yBQcFYki/91jzvv32WxgMBrdtk/JwjPLgF0/kQYir0P9i4jUSEhLsLvG0eL2Rh1rDT0P6dWVNp6amYv/+/W7ZFuXxYpQHv3CZByGuRMUa8RqnT59mTSvkMtStVcVDreGnsmWi8eZrDVjzjh496pZtUR4vRnnwC5d5EOJKVKwRr3Hv3j3WdK3qlWjomDy0ad6UNX358mW3bIfyKBjKg1+4yoMQV6JijXiNx48fs6bLxER4qCX8Vq1yBdb01atX4Y4eeiiPgqE8+IWrPAhxJSrWiNew/TCKLBXmoZbwW9XKr7Cm1Wq13e/OFSiPgqE8+IWrPAhxJSrWiNd4+vQpazo6MtxDLeG30tERCAhgD1R9/fp1l2/H03kIgqtBEFwNBoMRQNYwS9nzsjXvOBChrzSFqFQtxFRtjtFTF8NozOC0nZRHycyDEFeiYo14DbVazZoOUio81BJ+EwgEKFs6mjUvKSnJ5dvxhjxqVInFktnjsOaTWZDLpFi1fhs2bPme0zZQHjlKUh6EuBLdfUq8hsViYU0LhUK3bevYyXN4s/0HdvN9fHwglwWgXNkYtHyjEcYP74vQECUatOiBi5ezuk3w9fXF+V93oGa1Sqz3PnqcgCqNOkCt0QIAYqJK4Z/TexGokLu8/cpA9jrT0tJcvg0u8yisFYunIiU1HWnpKuzefxjXb96FQCDgvB2UR5aSlAchrkTFGvEath8+mZlmzttgsViQrlLj4uVruHj5Gjbv3I9zh7dj4+qFqPdWN5hMmcjMzMTAMXNw9vB2VptHTF7IFGoAsH7lfLcUagA3H0Z8yKMgYuu3xfOUNABAr65tMahPF87bQHnkKCl5EOJKdBmUeA3bDyPbMwnu1K1Ta3wyfyJmTxqG6lVyniZLSEzGii+2oEbVipg1cSgz/8KleKxYs5mZ3vH9zzgQd4yZHti7M1q3YHch4EpSCfueHL1e7/JteDIPAMwZmewn+bL/tT1T88Pmldj51TLUr1MNO36IY+XAFcojR0nJgxBXomKNeA3bPqMyTCbOtt26eVNMGj0AC2aMwomfN0Mk8mNei79xGwAwY8Jg1K5RmZk/Z+lq3L77ACmp6Rg7YykzPyaqFJYvnOzW9kok/qxpd3wYeTIPIOv3CACPniQCAB48yrrBvnQ0u8uKZo3r4f1OrTFt7ECYzWZs3L6X03YClEduJSUPQlyJLoMSryGXsy9dqFQaj7QjUCGHLECKlIx0AGAGh/b19WVdDtXrDRg8bh5KR0cg6VkK8353Xv7M5mfzwW1ywwe3p/Po1LY5PvtyK7oNnIQ2LZri4JGTAIAu77YEAMQdOYlt3/+EJg1rw2q14vMvtwEAalaryGk7AcoDKHl5EOJKVKwRryGVSlnT+v+6COCSSqXBxu17kZKazsx7v2Mr5vvsy6Fzl64GAPx24hzr/e6+/JnN15d9Scxsdv39S57OY/HssfD3F2HXvl+wfPUmRJYKw5QxH2De1BEAgNAQJa7E38Sen44iM9OM6MhwTBs3EHOnDOe0nQDlAZS8PAhxJSrWiNeQ2NxnotMbONv2gFGzMGDULNY8qVSC+VNHoMM7b7Hmz5gwGHt//pV5OjQbF5c/s9neJ+SOHto9mQcABARI8fH8ifh4/sQ8X69XuxouHt/NaZscoTxKXh6EuBLds0a8hu2HEdedadrq1PYtDP+gm9387Muhfn7sYyEuLn9m46I7BL7lwWeUB794orsQQoqCijXiNfz8/FjTmRxeuujWqTUWzx6Ldq1eZ+Zt3fUTOvUZm+dReY2qFdGofi1m+qXSUZxc/uSSJ/Mg9igPQoovKtaI17A9Guaya4LWzZti+vjBOLB9NYb278rMP3zsDLbu+jHP93jy4J2Lyzpc5TFg5CzIStdn+uZ68jQJHXuPQUBMfSjLNkK/ETPyvZn+2o3baNN1GJRlGyHo5cboP3Im5zffUx453h8wES/XagVxZB1EVn4DwybMh07H7dOYdNmTeBu6Z414Db5culg6dzx2/BCHdFXW8D7zP/4CPbq8w6se481m9/dmz0Ue/966h80792Nw3y7MU7e9hk7F8VPnMWviUKSmq7Bq/Tb4CHzwzeqFdu83GjPQ5v3hePQkEUvnjMPtew+x9pvvYLVasWnNYre3PxvlkePk2b8woGdHvFwmGp99uRXrNu5CgFTK2f2cADd5EOJKxfrM2q5duzBs2DDUq1cP/v7+EAgEzBfxPrZnCjyVozJQgZGDujPTt+48wM49cR5piyO2T7e548OIizzWb94Ni8WC7p3bAACuXruFYyf/RO0albFgxih8tnQ6wsOCseW7A3mezYm/cRv3Hz5BtcqvYNLoAVixaCoA4NvvfmSKbS5QHjnuXjyERbPGYlDf9/DhjNEAgEv/cDuQOhd5EOJKxbpYW7RoEdatW4cLFy4gI4NutvV2tpcuPFl0jxvWB1Jpzg3diz9dz6tLK6bMTNa07f1MrsBFHr/8dhpCoRAN69YAANy8cx8AUCYmgtlmmZhImM1m3H3wyO79pcJC4OPjg7v3H+Pvf64zveVbLBbcvvvQ5e11hPLI4e8vYr7ff/A3AECL1191eVvzw0UehLhSsS7WBAIBypcvj27duuH1119/8RsIr9kNVO3juT/fsNBgDOrdmZm+ev0W9vx4xGPtsZWRwe7kUyQSOViy8LjI49bdhwgJDoREIs6nHY6L5KjIcCyePRY6vQG1mr2HnkOmMkU2l8U15cFmtVoxcdYn+HrrHnRu1wJTxnzgyma+EBd5EOJKxfqetdOnTzOPs8+bNw/Hjx/3cItIUdj2Mm7bsaUrvdG0Aawp/+S7zP+WTsf/lk53+PqxAxtd3KqCsx3E2x1nDrjKI/cZogrlXgIA3H+YNZSR1WrFg0dPIRQK8XKZGACA4b/OYMXirCGFpo4diP49OuD23YcIDwtBgxZZl7CrVCzvlvbmhfLIycNozEDf4dPx3d5DGNi7M9atmMv5ZUgu8iDElYp1sWbb7xDxbgYDu5NPidjx0X1Jx8VlHi7yKPdSDK79ewcGgxFisT+qVn4FzRrXw4kzFzB3ySokp6Qh+Xkq+vXoAIVCltWOqLoAAP2TCxCL/bHs828gkYjhL/LDlHmfIjVNhdmThjFnhwTB1VjLuwPlkfP7fbvLEPx++jzq1qqCFq+/il17DyEgQIp3W78BoPjkQYgrFetijRQvtvcd5h5MnbCZTOwPI9tBvl2BizzefrMx/rl2E+f+uoJmjesBALauW4oRkxdi2epN8BUK0fv9dvjfkmkO15Gckob1m3dDpdbipdKRWLZgEiaM7Acg51KoQCCAjxsvqzubh9VqRUZGBjJzFRUCgQC+vr7w8/PL8340b8nj99PnAQAXLsWjx+ApALL6IXy39Rsey4MeMCB8R8Wal7FarVCpVBCLxRCJRF7xZKvVaoXBYIBKpUJKSgqePHmCxMREJCcnQ6VSQavVIi0tDSkpKUhJSYFarYbRaERGRgZMJhMyMjKg0+nw/Plz1nptRwggOTJsLolptVrcuHEDarUaCQkJSE5OhlarhVarhVqthkajgV6vh8FggF6vh0ajgVqthk6nY74yMjJgNBphNBqZXHJzRx6D+3bByrVb8N3eQ0xxEBMdgf3bVjl8j+3l66Vzx2Pp3PF5LvvPtZsAgGED3ndr8W+bx4YNG7Bnzx6o1Wqo1WoYDAaYTCYYDAYYjcYX9pHm5+cHiUQCuVwOhUIBmUyGu3fv2izDzzzyu73AU3lMmzYNc+bMgUgkgkgkgq+vLyQSCWQyGQICAiCRSCAWixEYGIigoCAoFAooFAoEBwcjIiICgYGBkMlkUCqVCA4Ohkwmg1gs9or9M/EO9GnnZTIyMqBUKgFkHX0GBgYiODgYcrkcgYGBzA5GqVRCqVRCoVAgJCQEwcHBzE7H398f/v7+kEgkCAgIYKb9/Pzg4+MDHx8fWK1WmM1m5kPZZDJBo9FAo9FAp9NBq9VCr9czHzZarRbp6enMB39iYiKSkpLw9OlTpKSksM4QuIqILl04lJauYk0vWLAACxYscOs23ZFHpdhy6NPtXWzasQ8Lpo9CcFCgS9d//NR5REWGY+mcvIs5V7HN4+bNm7h582ah12cymWAymaBSqfD48eM8l6E8HLPNA8jat7qy1wCxWIxSpUox++jg4GBIpVLWPjsgIAAymQyBgYFQKBSQSqUQi8VMsScSiZj9cu5upywWC6xWKywWCzIzM5m/h+yDreyDY5VKBYPBALVajWfPnuH58+fQaDQwGAzYvHkznVH0IlSseZnc96VYrVakpaUhLS3Ncw1ygkAggEKhQGRkJKKiohASEgKlUomAgADmKDU4OBgKhYI5ws3+kkqlGDx4MM6ePcusj86sOZaaxv4wEgqFkMlkkMvlCAsLQ3h4OHPWQKFQsM4eZBf8crkcUqkUUqkUEokEIpEIYrGYKey7d++OM2fOMNtwVx4bVy/CxtWL3LLuUYN7YtTgnm5Zd262eYwdOxZvvvkm5HI588Hs5+fHOnASiUQQCoXM5cDsD+bsM5u5D5Y0Gg0mT56M69dz+iujPByzzeOrr75CixYtmIItu/DRaDTMganBYEBqaipzUJqeno7k5GQkJiZCpVJBo9EgNTUVKlXWug0GA+7fv+/2n6WwvvzySwQEBHi6GaSA6NPOyygUChiNRuj1euj1eqSkpCAtLY05isregWfvNNLT0/H8+XOkpqYyO53sS1l6vR5arRZGozHfM19CoRB+fn6QyWSQyWSQSqXMh7tcLmeOELMvDchkMoSFhSEiIgLh4eEIDw9nCoKi3Idi+7SbLEBa6HUVd1qb4XsOHjyIli1bunQbRqORNU15OGabR9u2bV2ex9y5c1nTlIdjtnmULl0aZcqUccm6zWYzdDodnj17hmfPnrH2wXq9HqmpqUhNTWUKwezCT6VSQafTMUWiwWCw67zXER8fH/j5+UEsFjNfcrkcSqWSOVMXFhaG0NBQyGQySCQSOqvmZahY8zICgYA52xQYGIiIiAiXrNdiscBkMrF2DtlFmjtv9HVGamoqazpIqXB6HVarFQ1adMf5i1chFvvjzl9xiIwIc1UTi0yvN+Dl2q2QmPQcpaMjcOPcj/n2aeVIWjq7d/7AQNdergJck4cz4q/fxqipi3D63CUo5DL0eq8tPp4/Ic8n+axWK5av2oh1m3bh/sMnUMhlGNi7Mz6aN8GtbXSkpOfRvONA/P3PDajUGoSHhqBTu+ZYtmASq4NcLrkzD6FQyBzElitXrkjryt4vZ1/2zE0gEEAoFMLX15c3+2jiPsW6WPviiy9w+/ZtAFl9ruU2adIk5vuZM2ciKCiI07bxjY+PD/z93fOYvKuo1ewdrEIuc3odW3bux/mLVwEAg/p0sSvUzl/8B8tWbcTvZy7geUoalIFyNKxbA2OG9EKLNxo5ta1LV67j58O/4/fTF3Dn/iMkJCYjw2RCRHgomjWui/HD+6J2jcqs90gkYkwa2R+T5y7Hw8cJWLZqI2ZPHubUdk0mE4xG9r03CoXrP7hdkUdBZWZmokPv0Xj4OAELZ4zGhb/jsXLtFigD5Zg7dYTd8rMWfYbFn65HgzrVMXnUAGh1ertLX1yhPIAaVWLRvVMbCATA8tWbsGr9NlSq8DJGDurhtjY6wlUeruAN+2XCDcF/1Tp/xslxoTfeeKNAHeHevXsXZcuWdX+DSKFZLBaIRCLWmb+/ju2yK3byYzabUb5OG9x/+AQAcOvCzyj/cs6ljw2bd2PohAUOn8SbM3kY5k8fVeDttX5vKA79esrh60KhEJvWLEKvru1Y8zUaHUpVeh06nR4ymRSP/jmKQIW8wNtNTUtHcLkmrHkPHjxA6dKlC7yOF3FFHs746ZfjaNd9JDq3a4HvN6+EWq1FULnGCA4KRNK/v7OW1Wp1CK3wGnx9hXh45QhEfn6socG4VtLzyJaSmo60dBVGTFqIQ7+ewupPZmHEwO55LutOXORBiKvRuVPiFdLS0uzu3wgLce5s6I+HjjOFWuMGtViF2qUr1zF80kKmUHu1Xk0snDkabVq8xiyz4JO1+OkX50fBqFmtIiaM6If500ayxkA0m80YNnGB3ZNpMpkU7f/rIFSj0WHT9n1Obe95SrrdvJCQEKfbnR9X5OGMm7cfAADKxEQCAOTyAAQpFXiWnGI3IHv8jdswGIzwF4lQrXEnBMTUR+lqzfHDgcNua19+Snoe2WLrt0X5Om1w6NdT6NW1LQb16eK29uWHizwIcTUfFNOzagBw7NgxWK3WF37RWTX+y+uJV2fONgHA11v3MN93eZd9c/eSFeuZhyxefikGx3/ciJkTh+Ln775Ak4a1meU+/GRdgbfX9NU6OHVwCy79/j2WL5yMOVOG4/CeDRiYa0xRjUaHE2f+snvve+3fZr7/6tsfCrxNIGd4n9xcPZqHK/IoKkdnQLNvnH6ekoZBfTpj8xeLkZyShl5Dp+F5ShqHLcxS0vPI9sPmldj51TLUr1MNO36Iw4G4Y9w0zAYXeRDianRmjXiF5ORk1rS/vwgyWcGfdjObzTh28k9mulH9mqzXfjqcc+mmXatmrA45O7drwXx/9sJlJD1jd87ryKxJQ9E4V6GXrWPbt1jTtoNK27bvSvxNPEtOKdA2AcBg85Smv7+/yzvnLGoezqpQPuss6P1HWWdG01VqpKWrERYaDIVcBoPByNyHVK5sDHPD9bRxg9CnW3tUrVQeBoORObPKpZKeR7Zmjevh/U6tMW3sQJjNZmzcvtdt7csPF3kQ4mpUrBGvoNVqWdOyAKlTO9gr8TehUmuY6dz38ty59whabc6j/OVeYt+7Uq5sDGv68tV/C7zdvNy4eY/53sfHB3Vq2t9XFBUZjvCwYABZTzaePnepwOvX25w5cMdZg6Lm4axWbzVB+ZdL4+fDJ7B81UYMGTcPFosFIz7ohvsPn0ASVRcv1cw6W6oMVKBX17YAgCnzlmPFms24fPVfREWGo0rF8rj34DEEwdUQUel1t7U3t5KeR9yRk+g7fDrWbfwOa7/ZidmLs0Y7qFmtIgAUyzwIcbVi/TQoKT5suyWQy5zrzPHx00TWe3MPEG17aUwhZ6/bdlvJz9ltcca/t+5h0adfMtO932+Hl1+KyXPZ8NAQJD3LOqP2+GlSgbeRuygFALnc9ZfDipqHs3x9fbF3y2cYNXURZi76DLIAKcYM6YUZE4bgSYL97+bzj2ZAIBBg844DsFqteKNJfSz7cBLEYn+mCwRfITe7v5KeR2iIElfib2LPT0eRmWlGdGQ4po0biLlThgNAscyDEFejYo14hSdP2JevopzsGy13v0q2XRrY9l/0ounCnrE4f/EftOsxkulCoknD2vhi2WyHy+cuGvMaHseRdBX7wyh7eDJXKmoehVGtSgUcO7DRbn7ZMtF2400GKuTYtGZxnuv559otAMCYob1c3sa8lPQ86tWuhovHdztcT3HMgxBXo8ugxCskJbGP1sNCg516vzIw5+jZ9sg6JFjJmlZrdKxp2+ULMx7ij4eO4Y32A5CYlHW/21vNGuLgd2vz7VJCpc65tKUMLHg/ULbtdceQMkXNw5OOn/rzvyd0+3KyPcojf8UxD0Jcjc6sEa9ge+YgJqqUU++PighnvldrtDAYjMyl0PIvl0ZAgIS5b+32vYes996+y56uUTXWqW2v+WoHxkxbwnSt0Pv9dvjqsw9ZDzHkJSk550GG6MjwfJZke5bMviQWFub6syxFzcOTln04mdPtUR75K455EOJqdGaNeIWUFPbTkMFK585uVa9SgfV03KUrOQNeC4VCtGme05/agbhjzJNsVqsVu/fn9M/VoE51lAoPZab7j5wJQXA1CIKr4Y13+7O2abVaMWXucoycvJAp1GZPGoYta5e+sFB78jSJuV9NIBCgcYNaBf5Z1Rr2zebuGNqoqHkURvz123irwwcQR9ZBeGwzjJ/xkd14sQVZ9tqN2/AJqY65S1a5vc0A5XHs5Dnm/0jur7I1s7qnKY55EOJqdGaNeIWEhATWdESpUAdL5s3X1xfNGtXFz4dPAAD+OH8Zr+bqHmP6+EHY89NRmM1m3H/4BG+8OwDtWjXDiTN/4dxfV5jlZk4cUuBtjp66GKs3bGemG9SpDoU8AMs+/4a1XOMGtey6+Mj99Gf1KhWcuqxlO0i1TOb6YYeKmoeznBne6EXLVq5YHq2bN8XyNZswfkRfpy4xF0ZJz6NKxfLYvv5jZnrvz79i5544NKxbHQCKZR6EuBqdWSNewbYfqcL0zp67M9rvbXqzr1OzClZ/MpN5eOCP839j1qLPWcNFzZgwGO3bvFng7f1z7SZr+txfVzB57nK7r19+O2333t37f8mz3QVh+2Eklbq+vy1X5OGMQ7+ewq07D9C2ZTNMGj0AX66YB6FQiNVf7SjUsl07vA2tVo9tu392a7sByiM8LATdu7yD7l3eQbfObfD3PzcAAJNG9WeWKW55EOJqVKwRr/D06VPWdGHOHHR45y1meJxTZy/i7v1HrNeH9n8ff/yyDV07tEJEqVD4+fkiNCQIbd9uhkO712HRrLGF/wGcoFZrsf+/3t1lMin69ejg1Pttb6B2xyDVrsjDGc4Mb1SQZbNHpchv7FZXKel55PbjoeO4fvMuXm9SD/XrVGfmF7c8CHE1ugxKeM9sNkOjsXkisxD35AiFQnw4YxT6jZgJq9WKT9dsxucfzWAt06BudXz3zfICr3Pj6kXYuHpRnq/l1a1BQaz9Zif0egMAYOqYgU4PG2TbNYGrP4xclUdRvWh4o/yWzb4B3/bhEXegPHIsX70RADB59ADW/OKUByHuQGfWiFfy8SlcX2d9urVHvdpVAQAbtnyPpwnPXNmsItPrDVi+ZhMAoHR0BCaO7Of0Omw7+Q0Odn83DoXNo6CcGd4ov2WzC9/s4ahs+9Bzh5KeR7YLl67i+KnzqBxbDu+0bGbT3uKdByFFRWfWCO/l9YSZn1/+T1M6IhAI8OfRnUVtkttIJGIkXD9epHU8sxlhISQkpEjrs+XKPArKdnijc39dYQ1v9HKtVigVHoKE68fzXTbbw8dZN+TbDiXmDiU9j2zLVm0EkHWvmm3H0sUpD0Lcgc6sEd7z9fVljryz2d53QrI8TXiGJzZDU5UpU8al2/BEHtnDG71arwZmLvoMR38/ywxvVJhls5+2ffvNxm5tN+WR5cGjp9i9/zAiSoWiV9d2dq8XpzwIcQc6s0Z4z9fXF1FRUXj0KOeBgMdPk1CnZhUPtoqfTv7xF2taLpejatWqLt2Gp/JwZrgpR8tm+25vHKRSSZ6FgytRHlnKxETClHTJ4bqKUx6EuAOdWSNeITSU/XSb7X0nJMvfV2+wphs1agShUOjy7XhzHtf/vYO4o6cwYXjfQg0d5gzK48WKYx6EuBqdWSNeoVQp9vA5CUnJDpYs2f76+xprumbNmg6WLBpvzqNSbDmYky9zsi3K48WKYx6EuBqdWSNeISIigjWd3bEmyfE04ZldB7s1atRwy7YojxejPPiFyzwIcTUq1ohXaNiwIWv6p19+t+saoKTbtGMfMwYpkNUze7t27rkHiPJ4McqDX7jMgxBXo2KNeIWOHTuyHvdXa7T49fezHmwRv1gsFmzcvo81r3v37lAqlW7ZHuWRP8qDX7jOgxBXo2KNeIXIyEi8+uqrrHm5x88s6dZ8tQM3bt5lzRswYICDpYuO8sgf5cEvXOdBiKtRsUa8RqdOnVjTm3cewL+37nmmMTxy++4DTJ7LHiIrNjYWTZo0cet2KY+8UR784qk8CHElKtaI1+jevTv8/f2Z6czMTEyc/YkHW+R5arUWvYdOh8FgZM1ftWqVXS/xrkZ52KM8+MWTeRDiSlSsEa9RunRpjBs3jjXvx0PHsfeno55pkIc9T0nDWx0/wB/n/2bNHzFiBFq2bOn27VMebJQHv3g6D0JcSWDlYuRcQlxEpVIhNjYWiYmJzDyx2B8/7/wCb77WwIMt49axk+cwdMICu8tcUVFRuHbtGhQKBSftoDyyUB78wpc8CHEVOrNGvIpCocCSJUtY8wwGI9r1GImjx//wUKu4YbVaEX/9Nrr0HYc3239g90EUGhqKPXv2cPpBRHlQHnzBxzwIcRU6s0a8jsViQd++fbF161bWfIFAgCH93sOiWWMREqz0TONcLC1dhRs37+HAoWPYvf+w3RNt2UqXLo0jR44gNjaW4xZSHnmhPLjhDXkQ4gpUrBGvlJmZie7du+P777+3ey04KBDTxg5Ez/faIjqqVB7v5pbVaoVWq4dWp4Nao0NaugpJySl4npKGdJUGRmMGDEYj9AYjNBod0lRq3Ln3CDdu3UXSs5QXrr9SpUo4ePAgypYt6/4fxgHKIwfl4ZySkAchRUXFGvFaJpMJvXr1wq5duxwu06h+TXRs2xwN61ZHuZdiEBUZXqCBm61WK0ymTOgNBuh0Bqg1Wmh1emh1eqSkpuNp4jOkqzTQanXQ6Q3Q6vRIS1dDrdEiNU0FlVoDnd4AvcGItHQ1dDq9K390AFmXdRYsWIBBgwbBz8/P5et3FuVBeVAehLgHFWvEq2VmZmLZsmX48MMPodPpXri8n58voiLCERKshJ+vLwQCAUyZJmRkmLKO3LU6qDVa6PVGWCwWDn4C50VGRqJ79+6YPXs2goKCPN0cFsqD8vA0PudBSGFRsUaKhQcPHmDSpEn5nkXwVgKBAOXKlUPHjh3x3nvvoUGDBvDx4fezQZQHv1AehHg3KtZIsXLq1CmsW7cO+/btg0ql8nRz8uTr64uwsDCEhYVBqVRCIpHA398fYrEYcrkcMpkM0dHRiI2NRcWKFVGuXDmIRCJPN7tQKA9+oTwI8U5UrJFiyWg04tdff8W+fftw4sQJ3L17F3p90e6L8fX1RUBAAORyOSIjIxESEoKAgAAEBARAKpUiMDAQCoUCSqWS+ZCRSCRQKBQoVaoU5HI55HI5xGJxies9nfLgF8qDEO9CxRopEaxWK5KSknD//n08evQIarUaJpMJVqsVIpEIIpEI/v7+kMlkUCgUkEgkEIvFkEqlkEgkkMvlrKF8SNFQHvxCeRDCb1SsEUIIIYTwGN2FSQghhBDCY1SsEUIIIYTwGBVrhBBCCCE8RsUaIYQQQgiPUbFGCCGEEMJjVKwRQgghhPAYFWuEEEIIITxGxRohhBBCCI9RsUYIIYQQwmNUrBFCCCGE8BgVa4QQQgghPEbFGiGEEEIIj1GxRgghhBDCY1SsEUIIIYTwGBVrhBBCCCE8RsUaIYQQQgiPUbFGCCGEEMJjvp5uACFc0mq1iI+Px+PHj5GSkoK0tDSkpqaiadOmaNWqlaebV+JQHvxCefAL5UGyUbFGirWrV68iLi4O58+fx6VLl3Djxg1YrdY8l125ciVkMhmUSiVkMhmkUinkcjmCgoIQGBgIuVwOoVDI8U9QvFAe/EJ58AvlQRwRWB39JRDipZKSkrBlyxZs3rwZly9fdtl6BQIBgoKCoFAoEBAQAIlEApFIBJFIBJlMBolEArFYDJFIBKFQCB+frLsMLBYLMjMzkZGRAZPJBIPBALVaDZ1OB61WC71ez7xmNBoBAAqFAuHh4QgPD0d0dDRiYmJQt25dNGnSBAqFwmU/ExcoD36hPPiF8iAFYiWkmEhJSbEOHTrU6ufnZwVQLL+EQqG1cePG1vnz51sTExM9/SvPF+XBL5QHv1AexBlUrJFi4YcffrCWKlWqQDsQgUDg8Z2YK75kMpl19uzZVpVK5elfvx3Kg18oD36hPIiz6DIo8WoWiwVjxozB6tWrHS4THVkK77R8DXVqVkbt6pVRvUoF/H76Ag4fOwOD0Qi93gC1RofklFRotDqkpauh1emh1xuhUmtgsVg4/ImcFx4ejhUrVqBnz56ebgrlAcqDbygPfuFTHt6EijXi1SZPnoxly5bZzReL/dG1w9vo36MjXm9Sr9A32losFqjUGqjVWqSmq5D8PA0arQ5anR5GYwaMGRkwGjOg0epgMGRNZ2SYYLaYYTZn7TSFQh8IfYTw9xdBKPSB2N8fCnkApBIJpFIxpBIxRCI/+Ap94e/vBwBISU1H8vM0PE18hicJz3D1+i2c++sKTKZMh22dMmUKFi1aBF9fzz03RHnkoDwoD1uURw4+5OFNqFgjXmvFihWYMGGC3fy2bzfDmmWzUSYm0gOtch+tVocTZ/7C9h9+xrff/ZjnEXTHjh2xbds2SCQSzttHeVAenkR5cMdsNsPHxwcCgcDhMnzPw9tQsUa80uHDh9GqVSvWY+2+vr5Y9+kcDOjVKd+dSHEQf/02pi1YgQNxx+xea9++Pfbu3cvp74DyoDz4hPJwn6cJz/Be//G4dfcBXq1XE43q10Tr5k1Rq3olh+/hWx7eiIo14nUyMzNRrVo13LhxgzX/m1UL0b9nR880ykPWb9qNkVMW2l1uWLt2LYYOHcpJGyiPHJQHv1AernXm3CW8N2ACnjxNYs2fNm4glswZ/8L38yEPb0XDTRGv8+2339rt+BbOHO11Oz5XGNzvPRzZswGBCjlr/uTJk/Ho0SNO2kB55KA8+IXycA2r1YqlKzfgtbb97Ao1AKhbs2qB1sOHPLwVnVkjXsVsNqN69eq4du0aM69urSo4d2QH06ljSXTw8Am80204a16HDh2wd+9et26X8sgb5cEvlEfhpaalo9+ImXlewsx299IhlC0TXeB1eioPb+Ydfy2E/OfQoUOsHR8AzJ820mt2fO7SpuVr6NejA2vevn37EB8f79btUh55ozz4hfIonCvx/6LeW93yLdSCgwLxUukop9brqTy8mXf8xRDyn7i4ONZ09SoV0KbFax5qDb98unAKQoKVrHlr16516zYpD8coD36hPArOarXim6170LBlT9y5l//lyTo1Khfq4QBP5OHNqFgjXuXIkSOs6W6dWnvNUaq7BQcFYki/91jzvv32WxgMBrdtk/JwjPLgF8qjYHQ6PT4YNRsfjJ4NvZ79uwkLDbZbvm6tgt2vZssTeXgzfv/VEJJLQkKC3SWFFq838lBr+GlIv66s6dTUVOzfv98t26I8Xozy4BfKI3//xN9Ew5Y9sXH7XrvXGtSpjgu/7oQyUGEzv1qht8dlHt6OijXiNU6fPs2aVshlqFuriodaw09ly0TjzdcasOYdPXrULduiPF6M8uAXysOxHd//jAYte+CfazftXhv+QTf8/tMmaHV6pKWrWK81qFO90NvkMg9vR8Ua8Rr37t1jTdeqXomGKslDm+ZNWdOXL192y3Yoj4KhPPiF8mAzm82YvmAFegyeYnfZUyaTYseGT7Bm2Wz4+4tw+twl1utRkeGIiY4o0va5ysPb8e8vhxAHHj9+zJouE1O0nURxVa1yBdb01atXYbVaXd5DOOVRMJQHv1AeOaxWKzr3HYf9B3+ze61G1Vh89/VyVKzwMjPPtlhrXL9WkdvAVR7ejs6sEa9hu/OLLBXmoZbwW9XKr7Cm1Wq13e/OFSiPgqE8+IXyyCEQCKAMlNvNL1smGmcPb2cVagBw8uxF1nSThrWL3Aau8vB2VKwRr/H06VPWdHRkuIdawm+loyMQEMAeGPn69esu346n8xAEV4MguBoMBiOArGF9sudla95xIEJfaQpRqVqIqdoco6cuhtGYwWk7KQ/KA+Dv/mrB9FEQifxY8+49eIzoqm8hNS2dmfcsOQU3bt5lLdekYa0ib5+rPLwdFWvEa6jVatZ0kFLhYMmSTSAQoGxpdm/iSUn2Q8QUlTfkUaNKLJbMHoc1n8yCXCbFqvXbsGHL95y2gfLIQXnwz0ulozDig+5281NS09G84yDodHqYzWacsjmrJpVK8h28vaC4ysPb0T1rxGtYLBbWtFAodNu2jp08hzfbf2A338fHB3JZAMqVjUHLNxph/PC+CA1RokGLHrh4OesxfV9fX5z/dQdqVmPvyB49TkCVRh2g1mgBADFRpfDP6b124+S5gu2ljbS0NJdvg8s8CmvF4qlISU1HWroKu/cfxvWbdz1yLwzlkYXy4KeZE4fgt5Pn8Pc/7DFML16+hpVrt+BA3HFodTrWa43q14SfH/uMXGFxkYe3o2KNeA3bnV1mppnzNlgsFqSr1Lh4+RouXr6GzTv349zh7di4eiHqvdUNJlMmMjMzMXDMHJw9vJ3V5hGTFzKFGgCsXznfLYUawM3Ojw95FERs/bZ4npIGAOjVtS0G9enCeRsojxyUh2ekpKYjITEZaekq6A1GWK1WiMX+KB0dgZioUvho7gS07jrU7n0zF36W5/peb1zPZW2jYu3FqFgjXsN252d75OpO3Tq1Rr1aVaFSa7H356O4Ep/VF1FCYjJWfLEFny6aglkTh2Lu0tUAgAuX4rFizWZMGj0AQFYfRrnH1xvYuzNat2hqtx1XkUrY94Do9XqXb8OTeQBZl0+sViusVisAMP/anqn5YfNKJCQmY9nqjdjxQxw6tW2BLu1bctpWyiMH5eF+ZrMZZ89fxp8X/8GFv+Pxx/nLuHn7vsPl/fx8ATh3hvO1RnWK2MocXOTh7ahYI17Dto+iDJOJs223bt4U/Xt2BABMHNUP4bHNkJGRtf34G7cBADMmDMben39lLofOWboando1R5AyEGNnLGXWFRNVCssXTnZreyUSf9a0O3Z+nswDyPo9PnycgEdPElGh/Et48Cjrhu7SNv0+NfvvDICvrxBd+o3Hxu17OS8OKI8clIf73L3/CN9s3Ytvtu3BoyeJBX6fyZTp1HZGDuqB15vUd7Z5DnGRh7ejYo14DbmcfapcpdJ4pB2BCjlkAVKkZGQ9KZU9GLGvry/rcqheb8DgcfNQOjoCSc9SmPe78/JnNj+bDwqTGz4oPJ1Hp7bN8dmXW9Ft4CS0adEUB4+cBAB0eTfrgz/uyEls+/4nNGlYG1arFZ9/uQ0AULNaRU7bCVAeAOXhzjwOHj6BJSs34MSZC25Zf/ZZUwAYO7Q3Viye6tJ7DbnIw9tRsUa8hlQqZU3r/+sigEsqlQYbt+9FSmrOI+3vd2zFfF+jakXW5dDfTpxjvd/dlz+z+fqyL8GYza6/X8bTeSyePRb+/iLs2vcLlq/ehMhSYZgy5gPMmzoCABAaosSV+JvY89NRZGaaER0ZjmnjBmLulOGcthOgPADKwx153LrzABNnf5Jnp7aOyGRSSCVi+Pj4QKXWQqd78Vms7EKtUoWXMX5EX5c/FMJFHt6OijXiNSQ29zXobIZGcacBo2ZhwKhZrHlSqQTzp45Ah3feYs23vRyajYvLn9lsd6bZO1tX8mQeABAQIMXH8yfi4/kT83y9Xu1quHh8N6dtcoTyoDxcmYdOp8fsxavw+fqt+V7CrBxbDg3qVkeDOtXxar0aqFa5AqtPtdPnLqJJ6z4F3u71m3dRvUknfLF8Nnq+19ZlRRsXeXg7KtaI17Dd+XHdmaatTm3fwvAPutnNt70cmo2Ly5/ZuOgOgW958BnlwS/enMedew/Rqc9YXL76b56vKwMV6NW1LQb27ozaNSo7XI/FYsG4GR/l+VqDutXx9huNseHb75GQmMx6Ta3RovfQaTh87AzWfToX/v6iwv8w/6GhpV6MOsUlXsO2T59MDk+Vd+vUGotnj0W7Vq8z87bu+gmd+ozN8yiwRtWKaJRr3LyXSkdxcvmTS57Mg9ijPPjFHXlcuHQVDVv2zLNQCwsNxob/zUfC9WNY9fHMfAs1ANi5Jw5//vVPnuvZ9+3n+HDmaNy+cBCLZ4+FQi6zW27T9n1o0WkQ0w0LcS8q1ojXsD364vJR+NbNm2L6+ME4sH01hvbvysw/fOwMtu76Mc/3ePJgkYvLCFzlMWDkLMhK12c+FJ48TULH3mMQEFMfyrKN0G/EjHxv3t60fR+qN+kE37CaEARXw8Zte93SzvxQHmybtu9DtcYd4R9RG8qyjdB76FS3tNURb8zj+Kk/8Ub7AUh+nsqaLxL5YdKo/rh5/icM7NOlQGe6MjJMmLUo7/7TvlwxFxGlQgFk3eoxffxg3P7rIPp0e9du2ZN//IWuAyYgM9O5p0lt0WXPF6NijXgNvpwqXzp3POty5vyPv+DdDbFms/t7T+cij39v3cPmnfvRu2s75qnbXkOnYv/B3zBxRD/06fYuNu/Yj7HTlzpch1anR7PGdVGrOvdPHWajPHKs37Qb/UfOhCkzE58unIKFM0cjSBno9rbn5m153H/4BF36jYdGwx5FoErF8vj79+/xyYJJTt1isW7jd7hz75Hd/N7vt0PHts3t5oeGBGHzF0uwY8MnduN4/nbiHCbPWV7gbeeFizy8XbEu1h4/fow1a9bg/fffR7Vq1RAaGgqRSIRSpUrhnXfewZ49ezzdROIE2yNTTxVvykAFRg7KGUvv1p0H2LknziNtccS2eHTHzo+LPNZv3g2LxYLundsAAK5eu4VjJ/9E7RqVsWDGKHy2dDrCw4Kx5bsDDs/mjBjYHas/mYVKFV52efsKivLIsXD5OgDATzvWYEDPjhg1uCc+/2iGy9uaH2/KQ6PRoVOfsXaXG5u//ir++GUbKsWWc2p9KpUGY6YtsZv/8ksxWP3xrDzekaNb5zY4dXALIiPCWPNXrt2CDZsL//AIF3l4u2JdrG3ZsgUjR47Erl27cPXqVTx//hwmkwlJSUk4ePAgOnfujGHDhnm6maSAbE+Ve/JM27hhfSCV5hxhLv50Pa9O5ZtsLku4agy/3LjI45ffTkMoFKJh3RoAgJt3snphLxMTwWyzTEwkzGYz7j6wP1PAF5RHlmfJKXjw6Cn8/UVo230EAmLqI/SVpvji6x0ub2t+vCUPs9mMHoMn2z1Z3vbtZvhpxxrI5QFOrzO7WLb19ecLoFDY35tmq2a1Stiz+X+sp0qBrOH0Ll257nR7AG7y8HbFuljLFhMTgyFDhmDhwoXo06cPq2fpdevW4ciRIx5sHSkou4GRfTz35xsWGoxBvTsz01ev38KeH/nzd5Q9ukI2kajoT2zZ4iKPW3cfIiQ4EBKJOJ928KdIdoTy+K9N/50xMRoz0LxZQ3z39XIIhT4YOXkRrsTn/XSjO3hLHl9t+QE/HjrOmlexwsvY+uVHhXoKMyExGZ98/o3d/OEfdMMbTRsUeD0N69XAlyvmseaZTJmYPGeZ020CuMnD2xXrrjteeuklbNu2De+//z7rtOrbb7+NPn1y+pY5ePAgWrRo4YkmEifY9mpt25GiK73RtAGsKfZPSuX2v6XT8b+l0x2+fuzARhe3quBsB412x5EqV3nkPiNRodxLAID7D7OGMrJarXjw6CmEQiFeLhMDADD81/moWOwPvqA8svIIDgpEcFAgUlLTMWZIL1SKLYede+Lw/YHDuHn7PqpXiXVLm215Qx56vQEfLlvLmhccFIj9Wz8vdBdAnfqMzXP+x/Py7hsvP/16dMDlq//i0zWbmHlHjv+B02cvonHD2k6ti4s8vJ0PnB291Yv06NEDPXr0sLv+3bFjR9Z0Rgb1R+QNDAZ2p5ISseOj+5KOi8sKXORR7qUYJD9PYz7wq1Z+Bc0a18OlK9cxd8kqjJqyCMnPU9H7/XbMJRxJVF1Iouoy7/nr73hs2Lwbt+9mXZb7/fR5bNi8m7lZWxBcDYLgaszy7kB55OQxcmAPAMCCT9Zi7Tc7cfT3s5DJpHi1Xk0AlEe2NV/tsBvf89u1SxH7StlCtefajdv44/zfdvPPHt4OmUyaxzte7MMZoxAVGc6at3D5l06vhy6DvliJuAxq68aNG6zp+vVdNyAtcR/botr2ngmSw7ZXc9tBpV2BizzefrMxzGYzzv11hZm3dd1StGv1Opat3oRvv/sRvd9vh/8tmeZwHfsP/obB4+YxH1TfbNuLwePmITkllbmvSCAQwMeNl9WdzcNqtcJoNEKr1TJfOp0OGRkZDu+N9JY8Zk0airFDe+PwsTOYOHsZKse+jB+3r0ZUZLjH8nDHDe1FyUOj0WHp/75izXurWcMi9dVYpVEHu3n9enRAg7rVC71OqVSCyaMGsOYdPHICl6/ecPCOvHGxv/J2vgD4f8OHC2m1WowYMYKZjo2Nxfvvv+/BFjnHarVCpVJBLBZDJBLxpjuL/FitVhgMBqhUKqSkpODJkydITExEcnIyVCoVtFot0tLSkJKSgpSUFKjVahiNRmRkZMBkMiEjIwM6nQ7Pnz9nrdfPj/5DO5JhcwlGq9Xixo0bUKvVSEhIQHJyMlMEqNVqaDQa6PV6GAwG6PV6aDQaqNVq6HQ65isjIwNGoxFGo5HJJTd35DG4bxesXLsF3+09hGaN6wEAYqIjsH/bKofvsb18PW/aSMybNjLPZbPvkxo24H23Fv+2eWzYsAF79uyBWq2GWq2GwWCAyWSCwWCA0Wh8YZ9cfn5+kEgkkMvlUCgUkMlkuHv3rs0y/MxDJPLDyiXTsDKPgu6fazcBcJ/HtGnTMGfOHIhEIohEIvj6+kIikUAmkyEgIAASiQRisRiBgYEICgqCQqGAQqFAcHAwIiIiEBgYCJlMBqVSieDgYMhkMrvLoM7kceDQMbv+1BbPGlvo/f3273/Oc77tfWeFMaTfe1j06Zes9v546DhqVC14Vzm2edCZNXsl6tPu2bNnaN++Pc6dyxpcu1SpUjhw4ADEXnQ5LSMjA0qlEkDW0WdgYCCCg4Mhl8sRGBjI7GCUSiWUSiUUCgVCQkIQHBzM7HT8/f3h7+8PiUSCgIAAZtrPzw8+Pj7w8fGB1WqF2WxmPpRNJhM0Gg00Gg10Oh20Wi30ej3zYaPVapGens588CcmJiIpKQlPnz5FSkpKkTtNzIuI/kM7lJauYk0vWLAACxYscOs23ZFHpdhy6NPtXWzasQ8Lpo9CcJBr++M6fuo8oiLDsXTOeJeu15ZtHjdv3sTNmzcLvT6TyQSTyQSVSoXHjx/nuQzl4ZhtHkDWvtWVt8TYnhl0Jo/Dv51mTbd4/VU0rFejUO0wmUzoOXiK3fyfd37hkoJYKpWge+c2WLV+GzPv+KnzmDFhSIHXYZtHYCC3/e55gxJTrN28eRNt2rTB7du3AQClS5fGL7/8gthYbm5odZXc90FYrVakpaUhLS3Ncw1ygkAggEKhQGRkJKKiohASEgKlUomAgADmKDU4OBgKhYI5ws3+kkqlGDx4MM6ePcusj86sOZaaxt75CYVCyGQyyOVyhIWFITw8nDlroFAoWGcPsgt+uVwOqVQKqVQKiUQCkUgEsVjMFPbdu3fHmTNnmG24K4+Nqxdh4+pFbln3qME9MWpwT7esOzfbPMaOHYs333wTcrkcMpkMYrEYfn5+rAMnkUgEoVDIfOhbLBZkZmYyZzZzHyxpNBpMnjwZ16/ndJ1AeThmm8dXX32FFi1aMAVb9llOjUbDHJgaDAakpqYyB6Xp6elITk5GYmIiVCoVNBoNUlNToVJlrdv27GhB87BarYj79RRrXu5h7pzVd3jefdi1aflaoddp6/XG9VjF2smzF2E0ZhT4iVXbPIKDg13WtuKiRHzanTp1Ch06dGAuo9WqVQs//vgjoqOjPdwy5ykUChiNRuj1euj1eqSkpCAtLQ0qlQoqlYrZgWfvNNLT0/H8+XOkpqYyO53sS1l6vR5arRZGozHfM19CoRB+fn6QyWSQyWSQSqXMh7tcLodcLmc+9LMvyYSFhSEiIgLh4eEIDw9nCoKi3Idie1lBFlC4m2JLAq1Oz5o+ePAgWrZs6dJtGI3sG8ApD8ds82jbtq3L85g7dy5rmvJwzDaP0qVLo0yZMi5Zt9lshk6nQ6NGjXD16lVmfkHzuBL/L54mPGPNa928cPeqXbx8DTt+OGg3P+H6sUKtz5E3mtaHQCBg7jnU6fQ4e+Eyc6n8RWzzCAhwvv+44q7YF2u7du1C3759mTNS77zzDnbu3AmZ7MWd//GRQCBgzjYFBgYiIiLCJeu1WCwwmUysnqSzizR33ujrjNRU9j0cQUqF0+uwWq1o0KI7zl+8CrHYH3f+irPrjduT9HoDXq7dColJz1E6OgI3zv2Yb59WjqSlq1nT7ris4Io8nBF//TZGTV2E0+cuQSGXodd7bfHx/Al297ccO3kOb7b/wO79L5WOwr2/f3FrGx0pyXkAwDdb92DZqo24c/8RxP7+qFurCj5dONmp+5pcyZ15CIVCyOVy6HTsoaEKmsexk3+ypsvERBbqCdCMDBMatuxhN79Ni9dQKjzU6fXlJzQkCDWqxuLvf3IeLPj7nxsFLta4+P/h7Yp1sbZr1y5069aNqfbDw8PRrFkzrF3L7rumdOnS6NatmyeayBs+Pj7w9+dPv1R5UavZ/6EVcucL7i079+P8xayj3UF9urAKtfWbduPMn5fw58WruPbvHaZwLeqH/OHfTuPz9dtw9sJlpKWrERoShNderYNJo/qjXu1qrGUlEjEmjeyPyXOX4+HjBCxbtRGzJzs3yobJZILRyL73RqFw/Qe3K/IoqMzMTHToPRoPHydg4YzRuPB3PFau3QJloBxzp45gLVulYnlsX/8xM73351+xc08cGhbhqbeiKOl53Ln3EB+Mng2ZTIols8fh8tUb+GbbXgybsACnD211Wxsd4Xsez5LZRXf92tUK9WDBvI9W2z1lCQCbv1js9LoKolzZGFaxZvuAhCNc5eHtinWxdvXqVdZj7klJSZg2zf4JpNdff73EF2t8Z7FYinzmwGw2Y86S1cz0uGG9Wa9Pnrsc6Sq17duKZM7iVXYdWz55moSde+Kwa98vWPfpHAzq+x7r9WEDumHuR2ug0+nx8edfY8zQXk51gqnR6uzmufqygivycMahX0/h1p0H6NyuBSaNHgC1Wotd+37B6q922BUH4WEh6N7lHQBZZ1Lnf/wFAGDSqP5ua19+SnoeFos1635VuQwtXn8VsgAJvtm21+UPJxQU3/OoXaMS+vfoiNR0FVLTVKhR1fn7qs9duIIlKzbYzZ8+fhBCQ4KcXl9BNKxbAxkZJshlAZDLAlC3VtUCvY+LPIqDYl2skeIjLS3NbrDfMCd3Oj8eOo77D58AABo3qIXyL7PvUREKfVA5thzq1a6KK/E3Cz3OXc72jrEKtdbNm6Lpq7Xx0y+/48yff8NisWD4pIWoX6caalarxCwnk0nRvvUb2PHDQWg0Omzavg9jhvbOaxN5ep6SbjcvJCSkSD+LLVfk4Yybtx8AyLokBAByeQCClAo8S05BukrtsJj98dBxXL95F683qYf6dTxzZq2k5/FKuTJY9+lcjJyyENWbdgIAVK9SAd+sWui29uWH73l0frclOr9b+PsZMzMzMWT8vDxfmzZ2UKHX+yJTxw7E1LEDnX4fF3kUB/y4GclN5s2bB6vV+sKvY8eOebqp5AXyeuLV2SFXvt66h/m+Sx47w0f/HEX8H/ux+YslqFmt6PfSfPhJzoDJTRrWxsFdazFz4lAcO7ARL7+UNRRPZmZmnkfA77V/m/n+q29/cGq7efX8LpFI8liy8FyRR1G9qC8yAFi+eiMAYPLoAfkv6EYlPY+U1HQsWbkBclkAtq//GDMmDMaV+JsYNsG9Xck4UtzzWPnFFtblyGwfz5tQoIHaucZFHsVBsS7WSPGRnJzMmvb3Fzk1RIrZbGbduNuofk27ZQpzI78jiUnJrF7eO7fLGXtWJPJDu1bNmOkffzlu90GXu31X4m/iWXJKgbdtsHlK09/f3+WdJxc1D2dVKJ91FvT+o6wzo+kqNdLS1QgLDYZCLoPBYLS77+XCpas4fuo8KseWwzstm9mtkyslPY+jx//A3fuP0PTVOuje5R3MmjgUALA/7pjD0RjcqTjmke3+wyeY+9Eau/kBARIM/6C727dfGFzkURxQsUa8glarZU3LAqRO/Ye+En8TKrWGma5do7LL2paXy1f/ZU2XKxvDnn6pNPO9VqvHnXuPWK9HRYYjPCyrryGr1YrT5y4VeNt6myNVdxylFjUPZ7V6qwnKv1waPx8+geWrNmLIuHmwWCwY8UE33H/4BJKounipJvts6bJVGwFk3auWu233HjyGILgaIioVvu8qZ5T0PGJfeQkCgQC/nTiHVeu3YcKsrIc/qlV+BQKBgPJwEavVihGTPoTOphsMAJg0sj8nxWJhcJFHcUDFGvEKtjfrymXO3YD6+GnOgMhyWQDEYvc++fo8JY01bfskmNxmx5nXk1PhoTn3bTx+mlTgbecuSgFALnf95Zei5uEsX19f7N3yGV6tVwMzF32Go7+fxZghvRz2kv7g0VPs3n8YEaVC0atrO9Zr2WdzfIXc3LJb0vOoWa0SvvpsAcrERGLKvE+xc08c2rR4jXlil/JwjV17D+Hnwyfs5gcq5Bg3vI/bt19YXORRHNADBsQrPHnyhDUd5WTfaLn78XFnlwbZbC/u2F7usZ3O66hbIc/Zwec1PI4j6Sr2zi97eDJXKmoehVGtSgUcO7DRbn7ZMtF240+WiYmEKelSnuv559otAMCYob1c3cQ8UR7AgF6dMKBXpzzXQ3kUnUqlwbiZH+X52rRxA6EM5G9XGFzkURzQmTXiFZKS2GeWwkKdG45EGZhztGZ7JOcOITbdEqg17MsiKjV7Oq9uDHIv48zO1vbnc8dj8EXNw5OOn/oTNatVxIQRfTnZHuWRP8qj6OYsWWU36gGQdTvFWCeeJPcELvIoDujMGvEKtkeqMVGlnHp/VEQ4871ao4XBYHTrpVDbntlv333Inr6XMx0QIEH5l0vDVlLyc+b76Mhwu9cdse1UMyzM9Uf1Rc3Dk5Z9OJnT7VEe+aM8iubvf65j1Ybteb42c8IQlz445Q5c5FEc0Jk14hVSUthPQwYrnetQs3qVCqwbbIvah1q2/iNnQhBcDYLganjj3f7M/IhSoahXO6dTyO8PHGa+NxiMOBB3jJlu27KZ3ZBeT54mIelZ1s8sEAjQuEGtArfJ9iyeO4ZuKWoehRF//Tbe6vABxJF1EB7bDONnfGQ3XmxBlr124zZ8Qqpj7pJVbm8zQHkcO3mO+T+S+6tszazuaSiPwrNarRg1ZbFdn24AUDo6AgN7d3bLdl2JizyKAzqzRrxCQkICazqilHNj2/n6+qJZo7rMDbh/nL+MV22671j86ZdISc3qoDF7SCoASE1TYdLsT5jpgp4JmDVxKDr2HgMAOPPn32jTdRiavlobB+KO4+HjrJ9HKBTm2ZFk7qc/q1ep4NRlFNtBkd0xDm5R83CWM8MbvWjZyhXLo3Xzpli+ZhPGj+jr9vt5SnoeLxr+i/IovG27f8LJP/7K87U5k4fB31/klu26Ehd5FAd0Zo14Bdt+iwrTO3vuo8zcZ7qyfblpN5av3oTlqzfh6vVbzHyVWsPMX756U4G31+GdtzBjwmBmOu7oScxa9DnOXrgMIOuM2epPZqJOzSp27929P2csUmePjm13flKp6x/Zd0Uezsge3qhty2aYNHoAvlwxD0KhEKu/2lGoZbt2eBtarR7bdv/s1nYDlEf28F/du7yDbp3bMB225h7+i/Jwnkajw5R5n+b5WvUqFRw+0ME3XORRHFCxRrzC06dPWdOFOVLt8M5bzPA4p85exN37j17wjqJbNGss4natQ9u3myE0JAh+fr6IjAjD+x1b4Y9ftmFo//ft3qNWa7H/v8ukMpkU/Xp0cGqbtjfsumNQZFfk4YwXDW/k7LJNGtYGkFV0uFtJzyM3R8N/UR7OW7JyPZ446NJnxaKpEAqFLt+mO3CRR3FAl0EJ75nNZmg07P/QhbkHRCgU4sMZo9BvxExYrVZ8umYzPv9oBvP6vb9/yefdedu4ehE2rl6U7zKtmjdBq+ZNCrzOtd/shF5vAABMHTPQ6WFqbB+Fd/XOz1V5FFVBhptytGz2Dd+2D364A+WRw9HwX5SHc+7ce+jwLH+Hd95C89dfden23MndeRQXdGaNeCUfn8L1Bt6nW3vmxv8NW77P83F3T9LrDVi+JmsnXDo6AhNH9nN6HbYd8gYHu78bh8LmUVDODG+U37LZhW/2Ax1cDHdU0vPIlt/wX5SHc4KUCnRo82aer30yf6JLt+VunsjDG9GZNcJ7eT1h5ufnV6h1CQQC/Hl0Z1Gb5DYSiRgJ148XaR3PbEZDCAkJcbBk4bgyj4KyHd7o3F9XWMMbvVyrFUqFhyDh+vF8l82W/YCH7TBg7lDS88jmaPgvgPJwVpAyEDu+WoaTZy+yLoWOG9YHFcq/5NJtuZu78ygu6Mwa4T1fX1+7ri246NjWGz1NeGZ3H0uZMmVcug1P5OHM8EYFWTb7adu332zs1nZTHlnyG/4LoDwKQyAQoEfnNsx0kFKB2ZOHuXw77sRFHsUFnVkjvOfr64uoqCg8epTzQMDjp0l5PkVZ0tk+xi+Xy1G1alUHSxeOp/JwZngjR8tm+25vHKRSSZ6FgytRHlnyG/4LoDwKq2qlV5jv504ZnudIKHzGRR7FBZ1ZI14hNJT9NJXtfQ4ky99Xb7CmGzVq5Janwrw5j+v/3kHc0VOYMLyv2z/cKI8XozwKL7tYe6VcGQz/oLtbtuFOXOVRHNCZNeIVSpViD9eSkJTsYMmS7a+/r7Gma9as6WDJovHmPCrFloM5+TIn26I8XozyKLwqFcsDyHqoQCRy732K7sBVHsUBnVkjXiEiIoI1nd2xJsnxNOEZfvntNGtejRo13LItyuPFKA9+KY55ZPfD2OGdt9yyfnfiMo/igIo14hUaNmzImv7pl9/tugYo6Tbt2McaI1AqlaJdO/fcA0R5vBjlwS/FNY9VH820e7rWG3CZR3FAxRrxCh07dmTtkNQaLX79/awHW8QvFosFG7fvY83r3r07lEqlW7ZHeeSP8uCX4pyHTOZ9wzNxnUdxQMUa8QqRkZF49VV2r9y5x88s6dZ8tQM3bt5lzRswYICDpYuO8sgf5cEvlAe/cJ1HcUDFGvEanTqxBybevPMA/r11zzON4ZHbdx9g8tzlrHmxsbFo0qTgQ1wVBuWRN8qDXygPfvFUHt6OijXiNbp37w5/f39mOjMzExNnf+LBFnmeWq1F76HTYTAYWfNXrVrl9vtYKA97lAe/UB784sk8vB0Va8RrlC5dGuPGjWPN+/HQcez96ahnGuRhz1PS8FbHD/DH+b9Z80eMGIGWLVu6ffuUBxvlwS+UB794Og9vJ7ByMXIuIS6iUqkQGxuLxMREZp5Y7I+fd36BN19r4MGWcevYyXMYOmGB3WWVqKgoXLt2DQqFgpN2UB5ZKA9+oTz4hS95eDM6s0a8ikKhwJIlS1jzDAYj2vUYiaPH//BQq7hhtVoRf/02uvQdhzfbf2C34wsNDcWePXs43fFRHpQHX1Ae/MLHPLwZnVkjXsdisaBv377YunUra75AIMCQfu9h0ayxCAlWeqZxLpaWrsKNm/dw4NAx7N5/2O4JqmylS5fGkSNHEBsby3ELKY+8UB7coDz4xRvy8FZUrBGvlJmZie7du+P777+3ey04KBDTxg5Ez/faIjqqVB7v5pbVaoVWq4dWp4Nao0NaugpJySl4npKGdJUGRmMGDEYj9AYjNBod0lRq3Ln3CDdu3UXSs5QXrr9SpUo4ePAgypYt6/4fxgHKIwfl4RzKg/IgL0bFGvFaJpMJvXr1wq5duxwu06h+TXRs2xwN61ZHuZdiEBUZXqCBgq1WK0ymTOgNBuh0Bqg1Wmh1emh1eqSkpuNp4jOkqzTQanXQ6Q3Q6vRIS1dDrdEiNU0FlVoDnd4AvcGItHQ1dDq9K390AFmXERYsWIBBgwbBz8/z4wJSHpQH5eEY5cGvPLwNFWvEq2VmZmLZsmX48MMPodPpXri8n58voiLCERKshJ+vLwQCAUyZJmRkmLKOFLU6qDVa6PVGWCwWDn4C50VGRqJ79+6YPXs2goKCPN0cFsqD8vA0yoNf+JyHV7ESUgzcv3/f2rVrVyuAYvclEAis5cuXt06cONF65swZq9ls9vSv+4UoD36hPPiF8iDOojNrpFg5deoU1q1bh3379kGlUnm6OXny9fVFWFgYwsLCoFQqIZFI4O/vD7FYDLlcDplMhujoaMTGxqJixYooV64cRCKRp5tdKJQHv1Ae/EJ5kIKiYo0US0ajEb/++iv27duHEydO4O7du9Dri3Yfhq+vLwICAiCXyxEZGYmQkBAEBAQgICAAUqkUgYGBUCgUUCqVzE5NIpFAoVCgVKlSkMvlkMvlEIvFJa63bsqDXygPfqE8yItQsUZKBKvViqSkJNy/fx+PHj2CWq2GyWSC1WqFSCSCSCSCv78/ZDIZFAoFJBIJxGIxpFIpJBIJ5HI5a+gYUjSUB79QHvxCeRBbVKwRQgghhPAYjWBACCGEEMJjVKwRQgghhPAYFWuEEEIIITxGxRohhBBCCI9RsUYIIYQQwmNUrBFCCCGE8BgVa4QQQgghPEbFGiGEEEIIj1GxRgghhBDCY1SsEUIIIYTwGBVrhBBCCCE8RsUaIYQQQgiPUbFGCCGEEMJjVKwRQgghhPAYFWuEEEIIITxGxRohhBBCCI9RsUYIIYQQwmO+nm4AIVzSarWIj4/H48ePkZKSgrS0NKSmpqJp06Zo1aqVp5tX4lAe/EJ5EMJPVKyRYu3q1auIi4vD+fPncenSJdy4cQNWqzXPZVeuXAmZTAalUgmZTAapVAq5XI6goCAEBgZCLpdDKBRy/BMUL5QHv1AehHgHgdXR/0xCvFRSUhK2bNmCzZs34/Llyy5br0AgQFBQEBQKBQICAiCRSCASiSASiSCTySCRSCAWiyESiSAUCuHjk3WXgcViQWZmJjIyMmAymWAwGKBWq6HT6aDVaqHX65nXjEYjAEChUCA8PBzh4eGIjo5GTEwM6tatiyZNmkChULjsZ+IC5cEvlAch3oeKNVJspKamYvr06fj6669hMpk83Ry3EAqFaNiwIVq1aoVhw4YhPDzc001yiPLgF8qDEO9FxRopFvbs2YPhw4cjMTHxhcsKBAKHl3q8iUwmw/jx4zF58mTI5XJPN4eF8qA8PI3PeRDiLCrWiFezWCwYM2YMVq9e7XCZ6MhSeKfla6hTszJqV6+M6lUq4PfTF3D42BkYjEbo9QaoNTokp6RCo9UhLV0NrU4Pvd4IlVoDi8XC4U/kvPDwcKxYsQI9e/b0dFMoD1AefMOnPAgpLCrWiFebPHkyli1bZjdfLPZH1w5vo3+Pjni9Sb1C3/hssVigUmugVmuRmq5C8vM0aLQ6aHV6GI0ZMGZkwGjMgEarg8GQNZ2RYYLZYobZnPUhJhT6QOgjhL+/CEKhD8T+/lDIAyCVSCCViiGViCES+cFX6At/fz8AQEpqOpKfp+Fp4jM8SXiGq9dv4dxfV2AyZTps65QpU7Bo0SL4+nruuSHKIwflQXkQ4ipUrBGvtWLFCkyYMMFuftu3m2HNstkoExPpgVa5j1arw4kzf2H7Dz/j2+9+zPOMRseOHbFt2zZIJBLO20d5UB6exPc8CCkKKtaIVzp8+DBatWrFurfG19cX6z6dgwG9OkEgEHiwde4Xf/02pi1YgQNxx+xea9++Pfbu3cvp74DyoDz4hG95EFJUVKwRr5OZmYlq1arhxo0brPnfrFqI/j07eqZRHrJ+026MnLLQ7vLP2rVrMXToUE7aQHnkoDz4hQ95EOIKVKwRr7Nx40YMGDCANW/hzNGYObFk7nx/P30e7XuORrpKzcyTy+WIj49HTEyM27dPebBRHvzi6TwIcQUq1ohXMZvNqF69Oq5du8bMq1urCs4d2cF0slkSHTx8Au90G86a16FDB+zdu9et26U88kZ58Iun8iDEVUru/17ilQ4dOsT6IAKA+dNGlugPIgBo0/I19OvRgTVv3759iI+Pd+t2KY+8UR784qk8CHGVkv0/mHiduLg41nT1KhXQpsVrHmoNv3y6cApCgpWseWvXrnXrNikPxygPfvFEHoS4ChVrxKscOXKENd2tU+sSf9YgW3BQIIb0e48179tvv4XBYHDbNikPxygPfvFEHoS4Cv0vJl4jISHB7hJPi9cbeag1/DSkX1fWdGpqKvbv3++WbVEeL0Z58AuXeRDiSlSsEa9x+vRp1rRCLkPdWlU81Bp+KlsmGm++1oA17+jRo27ZFuXxYpQHv3CZByGuRMUa8Rr37t1jTdeqXomGjslDm+ZNWdOXL192y3Yoj4KhPPiFqzwIcSUq1ojXePz4MWu6TEyEh1rCb9UqV2BNX716Fe7ooYfyKBjKg1+4yoMQV6JijXgN2w+jyFJhHmoJv1Wt/AprWq1W2/3uXIHyKBjKg1+4yoMQV6JijXiNp0+fsqajI8M91BJ+Kx0dgYAA9kDV169fd/l2PJ2HILgaBMHVYDAYAWQNs5Q9L1vzjgMR+kpTiErVQkzV5hg9dTGMxgxO20l5lMw8CHElKtaI11Cr1azpIKXCQy3hN4FAgLKlo1nzkpKSXL4db8ijRpVYLJk9Dms+mQW5TIpV67dhw5bvOW0D5ZGjJOVBiCvR3afEa1gsFta0UCh027aOnTyHN9t/YDffx8cHclkAypWNQcs3GmH88L4IDVGiQYseuHg5q9sEX19fnP91B2pWq8R676PHCajSqAPUGi0AICaqFP45vReBCrnL268MZK8zLS3N5dvgMo/CWrF4KlJS05GWrsLu/Ydx/eZdCAQCzttBeWQpSXkQ4kpUrBGvYfvhk5lp5rwNFosF6So1Ll6+houXr2Hzzv04d3g7Nq5eiHpvdYPJlInMzEwMHDMHZw9vZ7V5xOSFTKEGAOtXzndLoQZw82HEhzwKIrZ+WzxPSQMA9OraFoP6dOG8DZRHjpKSByGuRJdBidew/TCyPZPgTt06tcYn8ydi9qRhqF4l52myhMRkrPhiC2pUrYhZE4cy8y9ciseKNZuZ6R3f/4wDcceY6YG9O6N1C3YXAq4klbDvydHr9S7fhifzAMCckcl+ki/7X9szNT9sXomdXy1D/TrVsOOHOFYOXKE8cpSUPAhxJSrWiNew7TMqw2TibNutmzfFpNEDsGDGKJz4eTNEIj/mtfgbtwEAMyYMRu0alZn5c5auxu27D5CSmo6xM5Yy82OiSmH5wsluba9E4s+adseHkSfzALJ+jwDw6EkiAODBo6wb7EtHs7usaNa4Ht7v1BrTxg6E2WzGxu17OW0nQHnkVlLyIMSV6DIo8RpyOfvShUql8Ug7AhVyyAKkSMlIBwBmcGhfX1/W5VC93oDB4+ahdHQEkp6lMO935+XPbH42H9wmN3xwezqPTm2b47Mvt6LbwElo06IpDh45CQDo8m5LAEDckZPY9v1PaNKwNqxWKz7/chsAoGa1ipy2E6A8gJKXByGuRMUa8RpSqZQ1rf+viwAuqVQabNy+Fymp6cy89zu2Yr7Pvhw6d+lqAMBvJ86x3u/uy5/ZfH3Zl8TMZtffv+TpPBbPHgt/fxF27fsFy1dvQmSpMEwZ8wHmTR0BAAgNUeJK/E3s+ekoMjPNiI4Mx7RxAzF3ynBO2wlQHkDJy4MQV6JijXgNic19Jjq9gbNtDxg1CwNGzWLNk0olmD91BDq88xZr/owJg7H351+Zp0OzcXH5M5vtfULu6KHdk3kAQECAFB/Pn4iP50/M8/V6tavh4vHdnLbJEcqj5OVBiCvRPWvEa9h+GHHdmaatTm3fwvAPutnNz74c6ufHPhbi4vJnNi66Q+BbHnxGefCLJ7oLIaQoqFgjXsPPz481ncnhpYtunVpj8eyxaNfqdWbe1l0/oVOfsXkeldeoWhGN6tdipl8qHcXJ5U8ueTIPYo/yIKT4omKNeA3bo2EuuyZo3bwppo8fjAPbV2No/67M/MPHzmDrrh/zfI8nD965uKzDVR4DRs6CrHR9pm+uJ0+T0LH3GATE1IeybCP0GzEj35vp3x8wES/XagVxZB1EVn4DwybMh07H7dN/lAfbpu37UK1xR/hH1IaybCP0HjrVLW11hC57Em9D96wRr8GXSxdL547Hjh/ikK7KGt5n/sdfoEeXd3jVY7zZ7P7e7LnI499b97B5534M7tuFeeq219CpOH7qPGZNHIrUdBVWrd8GH4EPvlm9MM91nDz7Fwb07IiXy0Tjsy+3Yt3GXQiQSjm7fxCgPHJbv2k3hoyfh9hXyuLThVNgtVpx49Y9t7c9Ny7yIMSVinWxZjAYsGDBApw/fx7//vsvnj9/DoPBgMDAQFSsWBHt2rXDyJEjoVDwbww9Ys/2TIGnijdloAIjB3XH4k/XAwBu3XmAnXvi0PO9th5pT15sn25zx4cRF3ms37wbFosF3Tu3AQBcvXYLx07+iTo1q2DBjFGwWq34bm8ctnx3AP9bMg0KhcxuHXcvHoK/vwgAEBYajI69x+DSP9wO3E155Fi4fB0A4KcdaxAVEQapVGK3jLtxkQchrlSsL4NqNBosWbIEhw8fxv3796HRaJCZmYnnz5/j9OnTmDFjBurXr4/U1FRPN5UUgO2lC0+eaRs3rA/rQ2bxp+t5dWnFlJnJmra9n8kVuMjjl99OQygUomHdGgCAm3fuAwDKxEQw2ywTEwmz2Yy7Dx7luY7sQg0A9h/8DQDQ4vVXXd7W/FAeWZ4lp+DBo6fw9xehbfcRCIipj9BXmuKLr3e4vK354SIPQlzJBwA/ri25SXR0NLp27YpJkyZh8eLFmDBhAl566SXm9X///RdffvmlB1tICspuoGofzx1rhIUGY1Dvzsz01eu3sOfHIx5rj62MDHYnnyKRyMGShcdFHrfuPkRIcCAkEnE+7XhxkWy1WjFx1if4eusedG7XAlPGfODKZr4Q5fFfm/47g2U0ZqB5s4b47uvlEAp9MHLyIlyJ/9fl7XWEizwIcaVifRk0NDQUjx7ZH91NnDgR0dHRzPS9e/c4bBUpLNtexm07tnSlN5o2gDXln3yX+d/S6fjf0ukOXz92YKOLW1VwtoN4u+PMAVd55D5DVKFc1oHW/YdZQxlZrVY8ePQUQqEQL5eJAQAY/usMVizOGlLIaMxA3+HT8d3eQxjYuzPWrZjL+WUvyiMrj+CgQAQHBSIlNR1jhvRCpdhy2LknDt8fOIybt++jepVYt7TZFhd5EOJKvgD4c+3GzcxmMxISEuzOpFWtWtVDLSLOMBjYnXxKxI6P7ks6Li7zcJFHuZdicO3fOzAYjBCL/VG18ito1rgeTpy5gLlLViE5JQ3Jz1PRr0cH5v4oSVRdAID+yQWIxf54u8sQ/H76POrWqoIWr7+KXXsPISBAindbvwEAEARXYy3vDpRHzu935MAe+HDZWiz4ZC2aNa6Lo7+fhUwmxav1agIoPnkQ4krF+sxatmPHjuHNN9/M87VmzZph0KBBHLeIFEZGBruTz9yDqRM2k4n9YWQ7yLcrcJHH2282xj/XbuLcX1fQrHE9AMDWdUsxYvJCLFu9Cb5CIXq/3w7/WzLN4Tp+P30eAHDhUjx6DJ4CIKvfu3dbv8Hc5yUQCODjxsvqzuZhtVqRkZGBzFxFhUAggK+vL/z8/PK8H81b8pg1aShUag227v4J+w7+hppVY7FkzjhERYZ7LA96wIDwXYko1hzp2bMn1q1bB7EXnaGxWq1QqVQQi8UQiUS86c4iP1arFQaDASqVCikpKXjy5AkSExORnJwMlUoFrVaLtLQ0pKSkICUlBWq1GkajERkZGTCZTMjIyIBOp8Pz589Z67UdIYDkyLC5JKbVanHjxg2o1WokJCQgOTkZWq0WWq0WarUaGo0Ger0eBoMBer0eGo0GarUaOp2O+crIyIDRaITRaGRyyc0deQzu2wUr127Bd3sPMcVBTHQE9m9b5fA9tpev87uc/c+1mwCAYQPed2vxb5vHhg0bsGfPHqjVaqjVahgMBphMJhgMBhiNxhf2kebn5weJRAK5XA6FQgGZTIa7d+/aLMPPPEQiP6xcMg0r8yjoPJXHtGnTMGfOHIhEIohEIvj6+kIikUAmkyEgIAASiQRisRiBgYEICgqCQqGAQqFAcHAwIiIiEBgYCJlMBqVSieDgYMhkMojFYq/YPxPvUCI+7cqXL49PPvkERqMR9+/fxw8//IDnz59j27Zt+OuvvxAXF8d66IDPMjIyoFQqAWQdfQYGBiI4OBhyuRyBgYHMDkapVEKpVEKhUCAkJATBwcHMTsff3x/+/v6QSCQICAhgpv38/ODj4wMfHx9YrVaYzWbmQ9lkMkGj0UCj0UCn00Gr1UKv1zMfNlqtFunp6cwHf2JiIpKSkvD06VOkpKSwzhC4ioguXTiUlq5iTS9YsAALFixw6zbdkUel2HLo0+1dbNqxDwumj0JwUKBL13/81HlERYZj6ZzxLl2vLds8bt68iZs3bxZ6fSaTCSaTCSqVCo8fP85zGcrDMds8gKx9q+0BSFGIxWKUKlWK2UcHBwdDKpWy9tkBAQGQyWQIDAyEQqGAVCqFWCxmij2RSMTslwUCAVP8WSwWWK1WWCwWZGZmMn8P2Qdb2QfHKpUKBoMBarUaz549w/Pnz6HRaGAwGLB582Y6o+hFBFY+9TfAkaSkJNSqVQtPn2bdFNuxY0fs2bPHw60qmPT0dKZY8zYCgQAKhQKRkZGIiopCSEgIlEolAgICmKPU4OBgKBQK5gg3+0sqlWLw4ME4e/Yss77Plk7H6CG9PPgT8Vf5Oq1x517OwzVCoRAymQxyuRxhYWEIDw9nzhooFArW2YPsgl8ul0MqlUIqlUIikUAkEkEsFjOFfffu3XHmzBlmG5SHY7Z5jB07Fm+++Sbkcjnzwezn58c6cBKJRBAKhczlwOwP5uwzm7kPljQaDSZPnozr13P6j6M8HLPN46uvvkKLFi2Ygi278NFoNMyBqcFgQOr/27vz6KbKvA/g3zQpTdIkLW0T2kIBy1BRwFZRFnFwg/JqUVDxtQ6K4IayiYiDIoojMhwUxhkFXh0FwRVGENlkE2QRBxF3URhZy9qFLlkb2zTvH53e9iZpm7Q3yU36/ZzTc3pvbu59mm9P8stz732esjLhS2lFRQVKSkpQWFgIs9kMq9WKsrIymM3ehaAcWa1WxMfHh7sZ5Kc20bPmyWQyoX///kKBtnPnzvA2KAAGgwFOpxMOhwMOhwOlpaUoLy8XvkXVvYHXvWlUVFTgwoULKCsrE9506k5lORwO2Gw2OJ3OJnu+lEolYmNjodPpoNPpoNVqhQ93vV4vfEOsOzWg0+lgNBqRmpoKk8kEk8kkFAStuQ7F8243Xby2xfuKdjaP6ZQ2bdqEIUOGSHoMp9MpWmYejfPMIy8vT/I8Zs2aJVpmHo3zzCMjIwOdO3eWZN8ulwt2ux3FxcUoLi4WvQc7HA6UlZWhrKxMKATrCj+z2Qy73S4UiZWVlV6D9zYmJiYGsbGxUKvVwo9er0diYqLQU2c0GpGSkgKdTgeNRsNetQgT1cXatm3bkJOTA6PRKFpfUlIi6qGJpOsKFAqF0NuUkJCA1NRUSfZbU1ODqqoq0ZtDXZEWzAt9A+E5eHH7xMBnnnC73eg7OB8HvjsItToOx77djLRUY/NPDBGHoxIXXT4UhUUXkNExFYf3b2hyTKvGlFdYRMsJCdKergKkySMQvxw6ionT5+DL/d/DoNdh1Mg8vPSXqT7v5OuanYuTp86K1q159x8YkXdjUNvYmLaex7x/LMGS9z7GkWMFcLvd+HzdUlx3Td+gtq8pwcxDqVQKX2IzMzNbta+69+W6054NKRQKKJVKqFQq2bxHU/BEdbG2aNEibNq0Cbm5ucjOzoZWq8WZM2ewevVqFBYWCtsNGzYsjK2Uh5iYGMTFBec2ealYLOI3WIPeeyqb5ry7ch0OfHcQAPDgvXd4FWoHvvsZ8xcuw+5/f4MLpeVITNCjX5/LMPnhURh83YCWNx7AyVNn0XvgbbBYbcK6txe+iDF/GiEsazRqTJswBk/OWoBTZ85j/sJlePbJRwI6TlVVFZxO8bU3wZhSTYo8/FVdXY3h90zCqTPn8eKMSfjmh1/w99ffRWKCHrOmj/f5nEuyMvFcg9fuqst7Ba19TWEetV9C8oYMwrrNn4tOP4ZDqPKQQiS8L1NoRHWxBtReNLphwwZs2LDB5+M5OTlYsGBBiFtFgaqpqWl1z4HL5cJzcxcJy1MeuUf0+FvvrMK4qS+I7sQrKi7F+s07sX7zTjz35CP4y9MTW9D62h69ByY/JyrUGvPI2Lswa95i2O0OvPTaUkweNwoJBr3fx7La7F7rpL42RYo8ArFlx14cOVaA24cNxrRJY2Gx2PDR2q1YtGRFo8WayZiEvNxrodNpw9p7zjyA55+aAAD4+rufw16shSIPIqlFdd/phAkTMG7cOOTk5MBkMkGlUkGtVqNLly645ZZbsHTpUuzfv9/rNCnJT3l5udf1G8bk9gHtY8OWXcKpsav75qDbRfXXqHz/0yE8Ou1FoVDrf2U2XnxmEm4a/Edhmxdefh0bt+5qUftff3sltu/a59e2Op0Wt/53wFar1Y7lH64N6FgXSiu81iUnJwe0j+ZIkUcgfjtaAADo3CkNAKDXx6N9ogHFJaWoMFt8Pmf3l9/A0KUfNOl9cPvox1BcUhq09jWFechLKPIgklpU96wNGTJE8ot4KTzKy8u91gXS2wQAS9+vv+P3jlvE/xdzX3lTuMnioi6dsGvDMmGcp2tuuhd7v/oOADD75TeQl3ttQMc9UXAGf37+bwCAEXk34JONO5p9zshbc7Hi400AgCXvfYzJ4+5p5hn16qb3aUij0fjYsuWkyKO1mhqL7P5Rt6F7ty6I12qweMkKrNmwHVqNGu+9MS+ELazFPOQlFHkQSS2qizWKHiUlJaLluLh20On8v9vN5XJh5xdfC8sDrsoWPbZx225hedjQQaIBOW8fNlgo1r765kcUFV+AyejfN3G32437Jz0Lq9WOrD90xV9nPuZXsdawfT/98huKS0phTEny65iVHndpxsXFSX4asLV5BKp7t9pe0JOna3tGK8wWlFdYYExJgkGvQ2WlEwqFAnFxtRNyP/fnR4XndkwzYcuOvfjxYOgmCm+IechLKPIgklpUnwal6GGzia/10sUHdh3ST7/8BrPFKixfftklwu/HTpyGzVZ/K39mlwzRczO7dhItB/Khv3jJCny+Zz9iYmKwbOGLft/ZmZ5mgslYW5y53W58uf97v4/p8Og5CEavQWvzCNTQGwai20UZ+HTbHixYuAwPT3keNTU1GH//XTh56iw06X3QJbu2t/THg4eRe/tDeGXxO1j63seY/NRcAMA1/a8AUNvTqUjqhdQegfWQtlRbzwOonfLrrXdWobC4dhaSjVt34613VgGIzjyIpMaeNYoInhdP63WBXRB85lz93b96XbxogugLpeWibQ168b49j1VyQdyWxhw7cQrT/1J7+vOJCfdhQN8cnCjwPdq8L6aUZBQVl/63/UV+P69hUQoAer30p8Nam0egVCoVPnn3VUycPgfPzHkVungtJj88CjOmPoyz58WvjTE5CWp1HOa9ugRl5WakmlIw5ZF7MWfmZAAQhkBQKUPz9tfW8wBqL0FoeO3l/IXLAAAPjh4ZlXkQSY3FGkWEs2fFY2alBzg2WsNxlTyHNPAcv6i5ZX96LOpOf9psDlySlYnZMyYF1N7adtZ/4PqaHqcxFWbxh1EwZrxobR4t0evS7ti5fpnX+q6dO4rmn0xLNTY5V+XPvx4BAEweF5rR/dt6HgCwbNEcLFs0x+d+ojEPIqnxNChFhKIij94TP6/fqpOYUP/t2fObdXJSomjZYhXf2u+5vT/zIa74eBN27T0ApVKJ5YvntOjaHbOl/tRWYoL/wzB4tjcYwxK0No9w2rX3a2T3uhhTx48OyfGYR9OiMQ8iqbFnjSKCZ89Bp/QOAT0/PdUk/G6x2lBZ6RROhXa7KAPx8RrhurWjJ06Jnnv0uHj5sp5ZzR6vsKj22hyXy4W+g+9udLuxE2di7MSZXoPjAkBRyQXh945pJviruER8SiwYQ9O0No9wmj/7yZAej3k0LRrzIJIae9YoIpSWisfISkoMbHqY3pd2F90d9/1P9RNeK5VK3HRj/Xhq6zfvFEY4d7vdWLVum/BY3yt6o4MpRVgeM+EZKJJ6QZHUC9fdMiagNjXl7Lki4Xo1hUKBq/vm+P1cz4F3gzG1UWvzaIlfDh3FDcPvhzrtCpiyBuHxGfO85osFgJ1f7BcyafjTNTsXAPDr4aOISe6NWXMbP1UqpbaeR0PDR00S8qgbQiMa8yCSGnvWKCKcP39etJzaIaWRLX1TqVQYNKAPPt22BwCw78CP6N9geIynH38QazZuh8vlwslTZ3HdLWMxbOgg7Pn3t9j/7U/Cds888bBfx+verbPXWG4AYHdUYtNne4TlKy/viS6d0tG1c7pou4Z3f/a+tHtAp7U8J6nW6aSfdqi1eQQqkOmNLr24Gz588yVh+ZNPd2Dlms3o16c3AOCSi7vhf268BgsWL8fj40cHdIq5Jdp6HnXeXL4K23d7DwwdjXkQSY09axQRPMeRasno7A/cc7vw++r120SPXZF9KRa9/Ixw88C+Az9g5pzXsGXHXmGbGVMfwq03Xe/XsfJyr8Wq5a94/SyeP1O03YQH7saq5a94TWq9at1Wn+32h+eHkVYr/XhbUuQRiLrpjfKGDMK0SWPxz1eeh1KpxKIlK7y2NRmTkX/Hzci/42bcdftN+OHnwwCAaRPHCNvcOTwXNpsDH6z6NKjtBpgHABw5VoDHZ87DKy9O9/l4tOVBJDUWaxQRzp07J1puSc/B8JtvEKbH2fvVdzh+UjxH4bgx/4t9Wz/AncOHIrVDCmJjVUhJbo+83EHYsuoNzJn5WMv/gABYLDas27wTQO3UU/fdPTyg53teQB2MSaqlyCMQLZ3eaMOWXTj023FcO/BKXHVFb2H9wH6XA4CoGA+Wtp6Hy+XCPeOewuBrB+Ch+0b63F+05UEkNZ4GJdlzuVywWj3uyGzBNTlKpRKzZ0zEfeOfgdvtxt8Wv4PX5s0QbdO3T2/86+0Ffu+zqSEJfPE1rIGn199eCYejEgAwffIDAU8b5Dk0gdQfRlLl0Vr+TG+0YNEyAMCTk8aK1tddgO9580gwtPU8XvvnBzh4+Aj+/tfpOHKsQFh/7MRp/CGzM9q1i42qPIiCgT1rFJFiYlo2Ovu9d92KKy/vCQB4693VOHe+WMpmtZrDUYkFi5cDADI6puKJCfcFvA/PQX6TkoI/jENL8/CXP9Mb1d0UUueb7w9i194DuCQrEzcPGeTR3tq3Ps8x9IKhredxouAMrFY7Bgwdhe5X3izso+fVw/GfIyf+297ozoOotdizRrLn6w6z2NhYH1s2T6FQ4OvtK1vbpKDRaNQ4f2hXq/ZR7DHDQnKyf/OY+kvKPPzlOb3R/m9/Ek1vdFHOUHQwJYteu7pR8qdNHOM1kPGpM7UX5HtOJRYMbT2PsaNGCFN9AcCdY6cCAD548yXhNGo05UEUDCzWSPZUKhViYmJEp1k8rzuhWufOF+Osx9RUnTt3lvQY4cgj0OmNCk6fw6p125DaIQWj7hzm9Xjd3ba5118d1HYzDyC7Vw9k9+rhtY/b8m4UxjqMpjyIgoHFGsmeSqVCeno6Tp+uvyHgzLkiXJF9aRhbJU9f7PtWtKzX69GzZ09JjxGuPAKZ3qhzpzRUFX3f6L7+9clmaLUan4WclJiHN1+PRVMeRMHAa9YoIqSkiO9u87zuhGr9cPCwaHnAgAFQKpWSHyeS8zj0n2PYvH0vpj462q+pw1qDeTQvGvMgkhp71igidOggnj7nfFFJI1u2bd/+8KtoOTs7u5EtWyeS8+iRlQlXyY8hORbzaF405kEkNfasUURITU0VLdcNdEr1zp0vxtbPvxStu+yyy4JyLObRPOYhL6HMg0hqLNYoIvTr10+0vHHrbq+hGtq65SvWwuVyCctarRbDhgXnGiDm0TzmIS+hzINIaizWKCKMGDFCNPyCxWrDjt1fhbFF8lJTU4NlH64VrcvPz0diYmJQjsc8msY85CXUeRBJjcUaRYS0tDT0799ftK7h/Jlt3eIlK3D4t+OidWPHjm1k69ZjHk1jHvIS6jyIpMZijSLGbbfdJlp+Z+V6YQT0tuzo8QI8OUs8RVZWVhYGDhwY1OMyD9+Yh7yEKw8iKbFYo4iRn5+PuLg4Ybm6uhpPPPtyGFsUfhaLDfeMexqVlU7R+oULF3qN2i815uGNechLOPMgkhKLNYoYGRkZmDJlimjdhi278MnG7eFpUJhdKC3HDSPux74DP4jWjx8/HkOGDAn68ZmHGPOQl3DnQSQlhTsUM+cSScRsNiMrKwuFhYXCOrU6Dp+u/D9c/8e+YWxZaO38Yj/GTX3B6zRXeno6fv31VxgMhpC0g3nUYh7yIpc8iKTCnjWKKAaDAXPnzhWtq6x0YtjdE7B9174wtSo03G43fjl0FHeMnoLrb73f64MoJSUFa9asCekHEfNgHnIhxzyIpMKeNYo4NTU1GD16NN5//33ReoVCgYfvG4k5Mx9DclJieBonsfIKMw7/dgLrt+zEqnXbvO5oq5ORkYHPPvsMWVlZIW4h8/CFeYRGJORBJAUWaxSRqqurkZ+fj9WrV3s9ltQ+AU899gD+NDIPHdM7+Hh2aLndbthsDtjsdlisdpRXmFFUUooLpeWoMFvhdP6OSqcTjkonrFY7ys0WHDtxGoePHEdRcWmz++/Rowc2bdqErl27Bv+PaQTzqMc8AtMW8iBqLRZrFLGqqqowatQofPTRR41uM+CqbIzIuxH9+vRGZpdOSE8z+TVxs9vtRlVVNRyVlbDbK2Gx2mCzO2CzO1BaVoFzhcWoMFths9lhd1TCZnegvMICi9WGsnIzzBYr7I5KOCqdKK+wwG53SPmnA6g9rfPCCy/gwQcfRGxsrOT7DxTzYB7Mgyg4WKxRRKuursb8+fMxe/Zs2O32ZrePjVUhPdWE5KRExKpUUCgUqKquwu+/V9V+c7fZYbHa4HA4UVNTE4K/IHBpaWnIz8/Hs88+i/bt24e7OSLMg3mEm5zzIGopFmsUFQoKCjBt2rQmexEilUKhQGZmJkaMGIGRI0eib9++iImR971BzENemAdRZGOxRlFl7969eOONN7B27VqYzeZwN8cnlUoFo9EIo9GIxMREaDQaxMXFQa1WQ6/XQ6fToWPHjsjKysLFF1+MzMxMtGvXLtzNbhHmIS/MgygysVijqOR0OrFjxw6sXbsWe/bswfHjx+FwtO66GJVKhfj4eOj1eqSlpSE5ORnx8fGIj4+HVqtFQkICDAYDEhMThQ8ZjUYDg8GADh06QK/XQ6/XQ61Wt7nR05mHvDAPosjCYo3aBLfbjaKiIpw8eRKnT5+GxWJBVVUV3G432rVrh3bt2iEuLg46nQ4GgwEajQZqtRparRYajQZ6vV40lQ+1DvOQF+ZBJG8s1oiIiIhkjFdhEhEREckYizUiIiIiGWOxRkRERCRjLNaIiIiIZIzFGhEREZGMsVgjIiIikjEWa0REREQyxmKNiIiISMZYrBERERHJGIs1IiIiIhljsUZEREQkYyzWiIiIiGSMxRoRERGRjLFYIyIiIpIxFmtEREREMsZijYiIiEjGWKwRERERyRiLNSIiIiIZY7FGREREJGMs1oiIiIhkjMUaERERkYyxWCMiIiKSMRZrRERERDLGYo2IiIhIxlisEREREckYizUiIiIiGWOxRkRERCRjLNaIiIiIZIzFGhEREZGMsVgjIiIikjEWa0REREQyxmKNiIiISMZYrBERERHJGIs1IiIiIhljsUZEREQkYyzWiIiIiGSMxRoRERGRjLFYIyIiIpIxFmtEREREMsZijYiIiEjGWKwRERERyRiLNSIiIiIZiwGgCHcjiIiIiMg39qwRERERyVgMAHe4G0FEREREvrFnjYiIiEjGWKwRERERyRiLNSIiIiIZY7FGREREJGMs1oiIiIhkjMUaERERkYyxWCMiIiKSMRZrRERERDLGYo2IiIhIxlisEREREckYizUiIiIiGWOxRkRERCRjLNaIiIiIZIzFGhEREZGMsVgjIiIikjEWa0REREQyxmKNiIiISMZYrBERERHJGIs1IiIiIhljsUZEREQkYyzWiIiIiGSMxRoRERGRjLFYq6do8EPhxyykx9dUenw9Q4P/u9Ljaxo6rX6d/x+6eByzF1mVKwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xin_input,Yin_output=bsm_iv_generator(num_sample = 50,tao_bound=[0.5,0.6],  sigma_bound=[0.3,0.7], \n",
    "                                      money_bound=[0.98,1.02], rr_bound=[0.03,0.08],callput='call')\n",
    "\n",
    "#check the data value range on each dimension\n",
    "## xin = [maturity time, Stock price, interest rate, dividend, option value]\n",
    "\n",
    "xlabel =  ['maturity time', 'Stock price', 'interest rate','option value']\n",
    "for i in range(0, len(xlabel)):\n",
    "    print(xlabel[i]+'  range:')\n",
    "    print(np.min(Xin_input[:,i]),np.max(Xin_input[:,i]))\n",
    "    \n",
    "print('sigma range:')\n",
    "print(np.min(Yin_output),np.max(Yin_output))\n",
    "print(np.shape(Xin_input))\n",
    "\n",
    "# generate and shuffle the data set into training and test part\n",
    "xtv_train_log_all,ytv_train_log_all=logscale_vol(Xin_input,Yin_output,otm_lower=1e-4)\n",
    "'''\n",
    "for i in range(4):\n",
    "    xtv_train_log_all[:,i]= min_max_normalization(xtv_train_log_all[:,i])\n",
    "'''\n",
    "#ytv_train_log_all=ytv_train_log_all/2\n",
    "xtv_train_log,xtv_test_log, ytv_train_log, ytv_test_log   = train_test_split(xtv_train_log_all,ytv_train_log_all,test_size=0.2,random_state=42)\n",
    "\n",
    "xlabel =  ['maturity time', 'Stock price', 'interest rate','time option-value']\n",
    "for i in range(0, len(xlabel)):\n",
    "    print(xlabel[i]+'  range:')\n",
    "    print(np.min(xtv_train_log_all[:,i]),np.max(xtv_train_log_all[:,i]))\n",
    "    \n",
    "print('sigma range:')\n",
    "print(np.min(ytv_train_log),np.max(ytv_train_log))\n",
    "## how many samples after cleaning\n",
    "print(np.shape(xtv_train_log))\n",
    "\n",
    "\n",
    "params = npp.random.random([24], requires_grad=True)\n",
    "inputs = npp.random.random([4], requires_grad=True)\n",
    "print(\"Parameters:\", params)\n",
    "print(\"inputs:\", inputs)\n",
    "print(\"Expectation value:\", circuit(params,inputs))\n",
    "\n",
    "\n",
    "qnode = qml.QNode(circuit, dev)\n",
    "qml.draw_mpl(circuit, decimals=1, style=\"sketch\")(params,inputs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc5c5020-7fea-46b3-85c1-c67a90814673",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.20659441676516832\n",
      "[-2.06594417e-01 -5.19991062e-01 -1.64696094e-01  5.55111512e-17\n",
      " -1.11022302e-16  0.00000000e+00 -1.53936148e-02 -2.34913798e-01\n",
      " -2.20811583e-01 -1.66533454e-16  0.00000000e+00  2.22044605e-16\n",
      "  1.56027168e-01 -6.81329529e-01 -5.19991062e-01  5.55111512e-17\n",
      "  0.00000000e+00 -5.55111512e-17  1.19536333e-01 -2.15111179e-01\n",
      " -2.34913798e-01 -1.66533454e-16  0.00000000e+00  1.11022302e-16]\n",
      "[-0.20659442 -0.51999106 -0.16469609  0.          0.          0.\n",
      " -0.01539361 -0.2349138  -0.22081158  0.          0.          0.\n",
      "  0.15602717 -0.68132953 -0.51999106  0.          0.          0.\n",
      "  0.11953633 -0.21511118 -0.2349138   0.          0.          0.        ]\n",
      "[-0.20659442 -0.51999106 -0.16469609  0.          0.          0.\n",
      " -0.01539361 -0.2349138  -0.22081158  0.          0.          0.\n",
      "  0.15602717 -0.68132953 -0.51999106  0.          0.          0.\n",
      "  0.11953633 -0.21511118 -0.2349138   0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "def parameter_shift_term(qnode,params,inputs, i):\n",
    "    shifted = params.copy()\n",
    "    shifted[i] += np.pi/2\n",
    "    forward = qnode(shifted,inputs)  # forward evaluation\n",
    "\n",
    "    shifted[i] -= np.pi\n",
    "    backward = qnode(shifted,inputs) # backward evaluation\n",
    "\n",
    "    return 0.5 * (forward - backward)\n",
    "\n",
    "# gradient with respect to the first parameter\n",
    "print(parameter_shift_term(circuit,params,inputs, 0))\n",
    "\n",
    "\n",
    "def parameter_shift(qnode, params,inputs):\n",
    "    gradients = np.zeros([len(params)])\n",
    "\n",
    "    for i in range(len(params)):\n",
    "        gradients[i] = parameter_shift_term(qnode,params,inputs, i)\n",
    "\n",
    "    return gradients\n",
    "\n",
    "print(parameter_shift(circuit, params,inputs))\n",
    "\n",
    "grad_function = qml.grad(circuit)\n",
    "print(grad_function(params,inputs)[0])\n",
    "\n",
    "\n",
    "print(qml.gradients.param_shift(circuit)(params,inputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9715830-fa60-4d48-bbc4-27dc40dbdf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from itertools import chain\n",
    "import time\n",
    "def QNN(weights, angles):\n",
    "    return circuit(weights, angles)\n",
    "\n",
    "def cost(weights, features, labels):\n",
    "    predictions = [QNN(weights, f) for f in features]\n",
    "    \n",
    "    return square_loss(labels, predictions)\n",
    "\n",
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "def R2(labels, predictions):\n",
    "\n",
    "    r2 = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        r2 = r2 + metrics.r2_score(labels, predictions)\n",
    "    r2 = r2 / len(labels)\n",
    "\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48fdbeb0-8d5d-419e-be05-d99d3936b2fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X=xtv_train_log\n",
    "Y=ytv_train_log\n",
    "weights_init = npp.random.random([24], requires_grad=True)\n",
    "opt = qml.AdamOptimizer(0.01)\n",
    "batch_size = 20\n",
    "batches = len (X) // batch_size\n",
    "X_batches = npp.array_split(npp.arange(len(X)) , batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2218b3c-8997-4af9-84d6-318bffadf9d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.03442255533682904 R2: -1.7234020687403337 time: 1703256512.526736\n",
      "batch_idx: 1 loss: 0.03588022914071358 R2: -1.1796426039850538 time: 1703256515.7879622\n",
      "Training [0%] Loss: 0.03515139223877131 time: 1703256515.7879622\n",
      "weight: [ 0.62207514  0.0641671   0.14164903  0.89855419  0.60642906  0.00919705\n",
      "  0.12145838  0.64351291 -0.01487467  0.16080805  0.54873379  0.6918952\n",
      "  0.63199068  0.20429949  0.69220636  0.23724909  0.3253997   0.74649141\n",
      "  0.66962013  0.86920958  0.63762404  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 2\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.020853528791827107 R2: -0.7382025449380221 time: 1703256519.0553095\n",
      "batch_idx: 1 loss: 0.02400852109462165 R2: -0.3997664317245119 time: 1703256522.2419457\n",
      "Training [1%] Loss: 0.022431024943224378 time: 1703256522.2419457\n",
      "weight: [ 0.60281183  0.04471598  0.12214488  0.89855419  0.60642906  0.00919705\n",
      "  0.14104356  0.62382321 -0.03426077  0.16080805  0.54873379  0.6918952\n",
      "  0.61288304  0.18487624  0.67275523  0.23724909  0.3253997   0.74649141\n",
      "  0.6892159   0.88876849  0.61793433  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 3\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.013310847493314876 R2: -0.16913045416517303 time: 1703256525.447228\n",
      "batch_idx: 1 loss: 0.01813566078351357 R2: -0.03707364731558305 time: 1703256528.9419866\n",
      "Training [1%] Loss: 0.015723254138414224 time: 1703256528.9419866\n",
      "weight: [ 0.5852087   0.02665649  0.10395818  0.89855419  0.60642906  0.00919705\n",
      "  0.15941219  0.60519033 -0.05220156  0.16080805  0.54873379  0.6918952\n",
      "  0.59575353  0.16688177  0.65469575  0.23724909  0.3253997   0.74649141\n",
      "  0.70761198  0.90706867  0.59930146  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 4\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011408472139459487 R2: 0.006543581070765692 time: 1703256532.1918082\n",
      "batch_idx: 1 loss: 0.017263185667712774 R2: -0.017091824115779808 time: 1703256535.506755\n",
      "Training [1%] Loss: 0.014335828903586131 time: 1703256535.506755\n",
      "weight: [ 0.57049209  0.0113956   0.08852964  0.89855419  0.60642906  0.00919705\n",
      "  0.17502566  0.58921536 -0.06738877  0.16080805  0.54873379  0.6918952\n",
      "  0.58151198  0.15170076  0.63943485  0.23724909  0.3253997   0.74649141\n",
      "  0.72326258  0.92258838  0.58332648  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 5\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.013195245604632758 R2: -0.08154645083429099 time: 1703256538.6978507\n",
      "batch_idx: 1 loss: 0.01892893077557909 R2: -0.1628461230555524 time: 1703256542.0379345\n",
      "Training [2%] Loss: 0.016062088190105926 time: 1703256542.0379345\n",
      "weight: [ 5.59653710e-01  5.06767049e-04  7.75604052e-02  8.98554189e-01\n",
      "  6.06429060e-01  9.19705162e-03  1.85900625e-01  5.78138862e-01\n",
      " -7.85577424e-02  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  5.70082053e-01  1.40837322e-01  6.28546023e-01  2.37249087e-01\n",
      "  3.25399698e-01  7.46491405e-01  7.34152534e-01  9.33421483e-01\n",
      "  5.72249985e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 6\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.015718407773474018 R2: -0.23354457106815038 time: 1703256545.608893\n",
      "batch_idx: 1 loss: 0.020441765295695833 R2: -0.28388412290531784 time: 1703256548.8788807\n",
      "Training [2%] Loss: 0.018080086534584927 time: 1703256548.8788807\n",
      "weight: [ 0.55295092 -0.00517789  0.07200228  0.89855419  0.60642906  0.00919705\n",
      "  0.1908551   0.57335661 -0.08516334  0.16080805  0.54873379  0.6918952\n",
      "  0.56045352  0.13506027  0.62286137  0.23724909  0.3253997   0.74649141\n",
      "  0.73907055  0.93845652  0.56746773  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 7\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.016820945188664317 R2: -0.30115467365608656 time: 1703256552.1021612\n",
      "batch_idx: 1 loss: 0.020523118955304755 R2: -0.2919763386109775 time: 1703256555.746777\n",
      "Training [2%] Loss: 0.018672032071984535 time: 1703256555.746777\n",
      "weight: [ 0.54990863 -0.00601778  0.07149371  0.89855419  0.60642906  0.00919705\n",
      "  0.19031486  0.57438759 -0.08754125  0.16080805  0.54873379  0.6918952\n",
      "  0.55134392  0.13400646  0.62202147  0.23724909  0.3253997   0.74649141\n",
      "  0.73844678  0.93810627  0.56849871  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 8\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.016128596706911736 R2: -0.25629672900226663 time: 1703256559.0228982\n",
      "batch_idx: 1 loss: 0.019420956217138196 R2: -0.20742188744815593 time: 1703256562.3747482\n",
      "Training [3%] Loss: 0.017774776462024966 time: 1703256562.3747482\n",
      "weight: [ 0.54966952 -0.00311431  0.07485717  0.89855419  0.60642906  0.00919705\n",
      "  0.18566196  0.57969283 -0.08655717  0.16080805  0.54873379  0.6918952\n",
      "  0.54177966  0.13660784  0.62492494  0.23724909  0.3253997   0.74649141\n",
      "  0.73368396  0.93369983  0.57380395  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 9\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.014423495739919898 R2: -0.14973742281661165 time: 1703256565.6508133\n",
      "batch_idx: 1 loss: 0.018021079684611245 R2: -0.09544697445055561 time: 1703256568.9119165\n",
      "Training [3%] Loss: 0.01622228771226557 time: 1703256568.9119165\n",
      "weight: [ 0.55127976  0.00222949  0.08071433  0.89855419  0.60642906  0.00919705\n",
      "  0.17844073  0.58761173 -0.0832755   0.16080805  0.54873379  0.6918952\n",
      "  0.53138547  0.14159735  0.63026874  0.23724909  0.3253997   0.74649141\n",
      "  0.72634337  0.92673849  0.58172285  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 10\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.012716799428705714 R2: -0.04797981078989455 time: 1703256572.122946\n",
      "batch_idx: 1 loss: 0.017111641696910516 R2: -0.01442452624278534 time: 1703256575.4389858\n",
      "Training [3%] Loss: 0.014914220562808115 time: 1703256575.4389858\n",
      "weight: [ 0.55382219  0.00872429  0.08771715  0.89855419  0.60642906  0.00919705\n",
      "  0.17010601  0.59660976 -0.07881879  0.16080805  0.54873379  0.6918952\n",
      "  0.52042912  0.14771885  0.63676355  0.23724909  0.3253997   0.74649141\n",
      "  0.71789191  0.91864826  0.59072088  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 11\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011691694802579115 R2: 0.005076302361612495 time: 1703256578.8089995\n",
      "batch_idx: 1 loss: 0.017013972506185877 R2: 0.009934271034578779 time: 1703256582.0729535\n",
      "Training [4%] Loss: 0.014352833654382495 time: 1703256582.0729535\n",
      "weight: [ 0.55648339  0.01514274  0.09458765  0.89855419  0.60642906  0.00919705\n",
      "  0.16200726  0.60526919 -0.07431528  0.16080805  0.54873379  0.6918952\n",
      "  0.50975764  0.15378361  0.64318199  0.23724909  0.3253997   0.74649141\n",
      "  0.70968954  0.91075787  0.59938032  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 12\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01148608648956372 R2: 0.00305725060623796 time: 1703256585.333885\n",
      "batch_idx: 1 loss: 0.017521883416679054 R2: -0.010963230208085983 time: 1703256588.6748798\n",
      "Training [4%] Loss: 0.014503984953121388 time: 1703256588.6748798\n",
      "weight: [ 0.55861586  0.02039271  0.10018848  0.89855419  0.60642906  0.00919705\n",
      "  0.15532533  0.61235479 -0.07081074  0.16080805  0.54873379  0.6918952\n",
      "  0.50067127  0.15874698  0.64843197  0.23724909  0.3253997   0.74649141\n",
      "  0.7029265   0.90423557  0.60646591  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 13\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011770707313855908 R2: -0.028650585251790872 time: 1703256592.0119412\n",
      "batch_idx: 1 loss: 0.01812618394366904 R2: -0.043778030039058 time: 1703256595.406998\n",
      "Training [4%] Loss: 0.014948445628762474 time: 1703256595.406998\n",
      "weight: [ 0.55979678  0.02367521  0.10368973  0.89855419  0.60642906  0.00919705\n",
      "  0.15090298  0.61700244 -0.06909727  0.16080805  0.54873379  0.6918952\n",
      "  0.49455341  0.16185439  0.65171447  0.23724909  0.3253997   0.74649141\n",
      "  0.69845118  0.89992243  0.61111356  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 14\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.012066064996456237 R2: -0.05531254433859356 time: 1703256598.6120124\n",
      "batch_idx: 1 loss: 0.018386874586099534 R2: -0.05876845336572976 time: 1703256601.9247642\n",
      "Training [5%] Loss: 0.015226469791277886 time: 1703256601.9247642\n",
      "weight: [ 0.55985446  0.02464251  0.10473714  0.89855419  0.60642906  0.00919705\n",
      "  0.14908451  0.61888317 -0.06953835  0.16080805  0.54873379  0.6918952\n",
      "  0.49221296  0.16277969  0.65268176  0.23724909  0.3253997   0.74649141\n",
      "  0.69660797  0.89816978  0.6129943   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 15\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.012083127468587095 R2: -0.05667045493015266 time: 1703256605.1319578\n",
      "batch_idx: 1 loss: 0.01818515669697267 R2: -0.047298348752806696 time: 1703256608.5287523\n",
      "Training [5%] Loss: 0.015134142082779883 time: 1703256608.5287523\n",
      "weight: [ 0.55884909  0.02343198  0.10348476  0.89855419  0.60642906  0.00919705\n",
      "  0.14969885  0.6182008  -0.07202003  0.16080805  0.54873379  0.6918952\n",
      "  0.49347037  0.16165014  0.65147123  0.23724909  0.3253997   0.74649141\n",
      "  0.69722165  0.89881579  0.61231192  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 16\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011847286448173233 R2: -0.03537879441972791 time: 1703256611.7875247\n",
      "batch_idx: 1 loss: 0.017705617273254214 R2: -0.02088396612441179 time: 1703256615.1022193\n",
      "Training [5%] Loss: 0.014776451860713724 time: 1703256615.1022193\n",
      "weight: [ 0.55702239  0.02056191  0.10048073  0.89855419  0.60642906  0.00919705\n",
      "  0.15218024  0.61555258 -0.0760493   0.16080805  0.54873379  0.6918952\n",
      "  0.4973508   0.15895047  0.64860117  0.23724909  0.3253997   0.74649141\n",
      "  0.69972056  0.90130172  0.6096637   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 17\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011578452490395665 R2: -0.008333932656371612 time: 1703256618.3869858\n",
      "batch_idx: 1 loss: 0.01723855418552741 R2: 0.0021755500792132176 time: 1703256621.8222213\n",
      "Training [6%] Loss: 0.014408503337961537 time: 1703256621.8222213\n",
      "weight: [ 0.55473456  0.01677239  0.09649979  0.89855419  0.60642906  0.00919705\n",
      "  0.15572972  0.61176375 -0.08091518  0.16080805  0.54873379  0.6918952\n",
      "  0.50251444  0.15537741  0.64481165  0.23724909  0.3253997   0.74649141\n",
      "  0.70329952  0.90483375  0.60587487  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 18\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01148131931288231 R2: 0.00824100933136196 time: 1703256625.2318144\n",
      "batch_idx: 1 loss: 0.01697820345351592 R2: 0.010466842874839788 time: 1703256628.5317945\n",
      "Training [6%] Loss: 0.014229761383199115 time: 1703256628.5317945\n",
      "weight: [ 0.55239432  0.01287048  0.09238493  0.89855419  0.60642906  0.00919705\n",
      "  0.15946493  0.60774244 -0.08584772  0.16080805  0.54873379  0.6918952\n",
      "  0.50759762  0.15169499  0.64090974  0.23724909  0.3253997   0.74649141\n",
      "  0.70706975  0.90853502  0.60185356  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 19\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011601629158998024 R2: 0.009122176497038614 time: 1703256631.836797\n",
      "batch_idx: 1 loss: 0.016935184963704546 R2: 0.005184627593596169 time: 1703256635.237199\n",
      "Training [6%] Loss: 0.014268407061351285 time: 1703256635.237199\n",
      "weight: [ 0.55038273  0.00958583  0.0889      0.89855419  0.60642906  0.00919705\n",
      "  0.16256731  0.60433499 -0.09016274  0.16080805  0.54873379  0.6918952\n",
      "  0.51146888  0.14859693  0.63762509  0.23724909  0.3253997   0.74649141\n",
      "  0.71020633  0.91159416  0.59844611  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 20\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011822023674829334 R2: 0.0007408059075784124 time: 1703256638.794231\n",
      "batch_idx: 1 loss: 0.016988918233523793 R2: -0.0038741654094733846 time: 1703256642.3541172\n",
      "Training [7%] Loss: 0.014405470954176564 time: 1703256642.3541172\n",
      "weight: [ 0.54897831  0.00743961  0.08659316  0.89855419  0.60642906  0.00919705\n",
      "  0.16443288  0.60217026 -0.09338411  0.16080805  0.54873379  0.6918952\n",
      "  0.51341076  0.14658069  0.63547886  0.23724909  0.3253997   0.74649141\n",
      "  0.71210018  0.9134148   0.59628139  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 21\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011973814894828605 R2: -0.006244435885061295 time: 1703256645.6929688\n",
      "batch_idx: 1 loss: 0.0170144873984793 R2: -0.007392136894259105 time: 1703256649.0218184\n",
      "Training [7%] Loss: 0.014494151146653953 time: 1703256649.0218184\n",
      "weight: [ 0.54830307  0.0066561   0.08570211  0.89855419  0.60642906  0.00919705\n",
      "  0.16478858  0.6015346  -0.0953171   0.16080805  0.54873379  0.6918952\n",
      "  0.51317806  0.14586172  0.63469536  0.23724909  0.3253997   0.74649141\n",
      "  0.71247592  0.91372889  0.59564572  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 22\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011964413759309785 R2: -0.0056175325059886205 time: 1703256652.2721376\n",
      "batch_idx: 1 loss: 0.016975595364676514 R2: -0.0030544031421024798 time: 1703256655.6717837\n",
      "Training [7%] Loss: 0.01447000456199315 time: 1703256655.6717837\n",
      "weight: [ 0.54830836  0.00714754  0.08613645  0.89855419  0.60642906  0.00919705\n",
      "  0.16372297  0.60233705 -0.0960534   0.16080805  0.54873379  0.6918952\n",
      "  0.51094173  0.14635721  0.6351868   0.23724909  0.3253997   0.74649141\n",
      "  0.71142312  0.91262543  0.59644817  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 23\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011823415961560271 R2: 0.0011507223834820168 time: 1703256659.5178\n",
      "batch_idx: 1 loss: 0.016922331268645355 R2: 0.004752822097914167 time: 1703256663.0670443\n",
      "Training [8%] Loss: 0.014372873615102814 time: 1703256663.0670443\n",
      "weight: [ 0.54880457  0.00857116  0.08753757  0.89855419  0.60642906  0.00919705\n",
      "  0.16162305  0.60417674 -0.0959159   0.16080805  0.54873379  0.6918952\n",
      "  0.50718728  0.14773999  0.63661042  0.23724909  0.3253997   0.74649141\n",
      "  0.70933206  0.91048795  0.59828786  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 24\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01165203153968539 R2: 0.008188715024822435 time: 1703256666.431865\n",
      "batch_idx: 1 loss: 0.0169206904915379 R2: 0.010120601772116156 time: 1703256670.0051842\n",
      "Training [8%] Loss: 0.014286361015611645 time: 1703256670.0051842\n",
      "weight: [ 0.54952219  0.0104318   0.08938715  0.89855419  0.60642906  0.00919705\n",
      "  0.15905195  0.60647387 -0.0953665   0.16080805  0.54873379  0.6918952\n",
      "  0.50261068  0.14953718  0.63847106  0.23724909  0.3253997   0.74649141\n",
      "  0.70676977  0.90787477  0.60058499  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 25\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011536469303571115 R2: 0.01091932556224351 time: 1703256673.4752023\n",
      "batch_idx: 1 loss: 0.016986216304014774 R2: 0.010562220981964132 time: 1703256676.9911761\n",
      "Training [8%] Loss: 0.014261342803792945 time: 1703256676.9911761\n",
      "weight: [ 0.55018411  0.01220311  0.0911347   0.89855419  0.60642906  0.00919705\n",
      "  0.15660619  0.60862065 -0.09489669  0.16080805  0.54873379  0.6918952\n",
      "  0.49800222  0.1512473   0.64024237  0.23724909  0.3253997   0.74649141\n",
      "  0.70433581  0.90537872  0.60273177  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 26\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011496661334487356 R2: 0.009282138188644762 time: 1703256680.451026\n",
      "batch_idx: 1 loss: 0.017073859163320267 R2: 0.008040579228427935 time: 1703256683.97039\n",
      "Training [9%] Loss: 0.014285260248903812 time: 1703256683.97039\n",
      "weight: [ 0.55057033  0.01344655  0.09232228  0.89855419  0.60642906  0.00919705\n",
      "  0.15477911  0.61012175 -0.09491701  0.16080805  0.54873379  0.6918952\n",
      "  0.49409197  0.15245419  0.64148581  0.23724909  0.3253997   0.74649141\n",
      "  0.70252485  0.9034919   0.60423287  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 27\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01149846953428245 R2: 0.006593109889643345 time: 1703256687.3347173\n",
      "batch_idx: 1 loss: 0.017121391862676867 R2: 0.006197560651199696 time: 1703256690.7775977\n",
      "Training [9%] Loss: 0.014309930698479658 time: 1703256690.7775977\n",
      "weight: [ 0.55056021  0.01390503  0.09268225  0.89855419  0.60642906  0.00919705\n",
      "  0.15385675  0.61069655 -0.09566817  0.16080805  0.54873379  0.6918952\n",
      "  0.49136896  0.1529151   0.64194428  0.23724909  0.3253997   0.74649141\n",
      "  0.70162269  0.90250118  0.60480768  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 28\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011502676715526013 R2: 0.0059611156544027155 time: 1703256694.3003023\n",
      "batch_idx: 1 loss: 0.017100433341071856 R2: 0.006918226905766045 time: 1703256697.7792153\n",
      "Training [9%] Loss: 0.014301555028298935 time: 1703256697.7792153\n",
      "weight: [ 0.55014535  0.0135464   0.0921822   0.89855419  0.60642906  0.00919705\n",
      "  0.15387166  0.61031972 -0.09717954  0.16080805  0.54873379  0.6918952\n",
      "  0.48994661  0.15260059  0.64158565  0.23724909  0.3253997   0.74649141\n",
      "  0.70166082  0.90244077  0.60443084  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 29\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011500542380788664 R2: 0.007790806327142796 time: 1703256701.0920591\n",
      "batch_idx: 1 loss: 0.01703114181969708 R2: 0.009261774536406198 time: 1703256704.6296284\n",
      "Training [10%] Loss: 0.014265842100242872 time: 1703256704.6296284\n",
      "weight: [ 0.54941462  0.01254553  0.09100582  0.89855419  0.60642906  0.00919705\n",
      "  0.15462445  0.60919476 -0.09928737  0.16080805  0.54873379  0.6918952\n",
      "  0.48955098  0.15167676  0.64058479  0.23724909  0.3253997   0.74649141\n",
      "  0.70243884  0.90311271  0.60330589  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 30\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011509266304072912 R2: 0.010216571447096756 time: 1703256707.9553542\n",
      "batch_idx: 1 loss: 0.01695631087634558 R2: 0.011083857687811394 time: 1703256711.3370383\n",
      "Training [10%] Loss: 0.014232788590209246 time: 1703256711.3370383\n",
      "weight: [ 0.54851895  0.01121515  0.08948053  0.89855419  0.60642906  0.00919705\n",
      "  0.15576022  0.60767632 -0.10170216  0.16080805  0.54873379  0.6918952\n",
      "  0.48963685  0.15043978  0.63925441  0.23724909  0.3253997   0.74649141\n",
      "  0.70360129  0.90416298  0.60178745  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 31\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011542666816882066 R2: 0.011359265995921297 time: 1703256714.7018893\n",
      "batch_idx: 1 loss: 0.016905673637792724 R2: 0.011266286474819864 time: 1703256718.4106317\n",
      "Training [10%] Loss: 0.014224170227337395 time: 1703256718.4106317\n",
      "weight: [ 0.54762587  0.00990926  0.08797685  0.89855419  0.60642906  0.00919705\n",
      "  0.15687501  0.60616506 -0.10410069  0.16080805  0.54873379  0.6918952\n",
      "  0.48957228  0.14922476  0.63794851  0.23724909  0.3253997   0.74649141\n",
      "  0.7047442   0.90518833  0.60027619  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 32\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011591098344002907 R2: 0.01098208586358751 time: 1703256721.8473094\n",
      "batch_idx: 1 loss: 0.016881767432829438 R2: 0.010440951419557254 time: 1703256725.3480852\n",
      "Training [11%] Loss: 0.014236432888416172 time: 1703256725.3480852\n",
      "weight: [ 0.5468744   0.00892607  0.08680724  0.89855419  0.60642906  0.00919705\n",
      "  0.15762522  0.60500039 -0.10621525  0.16080805  0.54873379  0.6918952\n",
      "  0.48882243  0.14831404  0.63696533  0.23724909  0.3253997   0.74649141\n",
      "  0.70552429  0.90584579  0.59911151  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 33\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01162852683619963 R2: 0.01023561105717219 time: 1703256728.638422\n",
      "batch_idx: 1 loss: 0.016872195648330417 R2: 0.009938184424152174 time: 1703256732.076306\n",
      "Training [11%] Loss: 0.014250361242265024 time: 1703256732.076306\n",
      "weight: [ 0.54634159  0.00843541  0.08614986  0.89855419  0.60642906  0.00919705\n",
      "  0.1578126   0.60437716 -0.10789708  0.16080805  0.54873379  0.6918952\n",
      "  0.48708154  0.14786837  0.63647467  0.23724909  0.3253997   0.74649141\n",
      "  0.70574381  0.90593756  0.59848828  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 34\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011635529523858643 R2: 0.010210083807828285 time: 1703256735.4241993\n",
      "batch_idx: 1 loss: 0.016868728609448026 R2: 0.010371032243738032 time: 1703256738.9361517\n",
      "Training [11%] Loss: 0.014252129066653334 time: 1703256738.9361517\n",
      "weight: [ 0.54602963  0.00844818  0.08601646  0.89855419  0.60642906  0.00919705\n",
      "  0.15742201  0.60430919 -0.10914016  0.16080805  0.54873379  0.6918952\n",
      "  0.48432691  0.14789797  0.63648743  0.23724909  0.3253997   0.74649141\n",
      "  0.70538796  0.90544863  0.59842031  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 35\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0116139807481886 R2: 0.010940022091644686 time: 1703256742.4436138\n",
      "batch_idx: 1 loss: 0.016873134613016 R2: 0.01127029896800591 time: 1703256745.9138696\n",
      "Training [12%] Loss: 0.0142435576806023 time: 1703256745.9138696\n",
      "weight: [ 0.54587631  0.00883307  0.08626972  0.89855419  0.60642906  0.00919705\n",
      "  0.15660353  0.60464846 -0.11006349  0.16080805  0.54873379  0.6918952\n",
      "  0.48079559  0.14827855  0.63687233  0.23724909  0.3253997   0.74649141\n",
      "  0.70460675  0.90452903  0.59875958  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 36\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011581631704350535 R2: 0.011682899765812382 time: 1703256749.3207185\n",
      "batch_idx: 1 loss: 0.01688832834819047 R2: 0.011844994673783416 time: 1703256752.7773347\n",
      "Training [12%] Loss: 0.014234980026270503 time: 1703256752.7773347\n",
      "weight: [ 0.54578335  0.00937115  0.08668042  0.89855419  0.60642906  0.00919705\n",
      "  0.15560916  0.60514908 -0.1108614   0.16080805  0.54873379  0.6918952\n",
      "  0.47690086  0.14880323  0.63741041  0.23724909  0.3253997   0.74649141\n",
      "  0.70365137  0.90343088  0.59926021  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 37\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01155578711599844 R2: 0.011872653197127936 time: 1703256756.1320982\n",
      "batch_idx: 1 loss: 0.01690885393889955 R2: 0.01184269965335183 time: 1703256759.530874\n",
      "Training [12%] Loss: 0.014232320527448994 time: 1703256759.530874\n",
      "weight: [ 0.54565135  0.00982938  0.08700463  0.89855419  0.60642906  0.00919705\n",
      "  0.15470718  0.60555218 -0.11173844  0.16080805  0.54873379  0.6918952\n",
      "  0.47310868  0.14925238  0.63786864  0.23724909  0.3253997   0.74649141\n",
      "  0.70278869  0.90242306  0.59966331  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 38\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011542873640671384 R2: 0.01164094391468562 time: 1703256762.813167\n",
      "batch_idx: 1 loss: 0.016922322118044977 R2: 0.01164095246916319 time: 1703256766.220289\n",
      "Training [13%] Loss: 0.01423259787935818 time: 1703256766.220289\n",
      "weight: [ 0.5454095   0.0100308   0.08705724  0.89855419  0.60642906  0.00919705\n",
      "  0.15410093  0.60566421 -0.11284823  0.16080805  0.54873379  0.6918952\n",
      "  0.46979955  0.14945975  0.63807005  0.23724909  0.3253997   0.74649141\n",
      "  0.70222046  0.90170985  0.59977533  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 39\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011540664483882395 R2: 0.011497428833780132 time: 1703256769.6484263\n",
      "batch_idx: 1 loss: 0.016919407610165665 R2: 0.011680249745324112 time: 1703256772.9547374\n",
      "Training [13%] Loss: 0.01423003604702403 time: 1703256772.9547374\n",
      "weight: [ 0.54503148  0.0099009   0.08676048  0.89855419  0.60642906  0.00919705\n",
      "  0.15387577  0.60540616 -0.11425379  0.16080805  0.54873379  0.6918952\n",
      "  0.46715499  0.14935573  0.63794016  0.23724909  0.3253997   0.74649141\n",
      "  0.70203095  0.9013774   0.59951728  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 40\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011546062405608803 R2: 0.01170863266308464 time: 1703256776.2390752\n",
      "batch_idx: 1 loss: 0.0169010703918561 R2: 0.011992146905532852 time: 1703256779.6122994\n",
      "Training [13%] Loss: 0.014223566398732452 time: 1703256779.6122994\n",
      "weight: [ 0.54453557  0.00947768  0.08615439  0.89855419  0.60642906  0.00919705\n",
      "  0.15398771  0.60482205 -0.11592044  0.16080805  0.54873379  0.6918952\n",
      "  0.46511242  0.14897632  0.63751694  0.23724909  0.3253997   0.74649141\n",
      "  0.70217617  0.90138197  0.59893318  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 41\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011558341556121806 R2: 0.012094725547415752 time: 1703256782.9922101\n",
      "batch_idx: 1 loss: 0.01687625556267581 R2: 0.012290017420380162 time: 1703256786.5508907\n",
      "Training [14%] Loss: 0.014217298559398808 time: 1703256786.5508907\n",
      "weight: [ 0.5439713   0.00888513  0.08536904  0.89855419  0.60642906  0.00919705\n",
      "  0.15429364  0.60404865 -0.11774039  0.16080805  0.54873379  0.6918952\n",
      "  0.46340745  0.14843798  0.63692438  0.23724909  0.3253997   0.74649141\n",
      "  0.70251408  0.90157998  0.59815977  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 42\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011576122232423724 R2: 0.01234784977113823 time: 1703256790.1521368\n",
      "batch_idx: 1 loss: 0.016854071308905812 R2: 0.01237571293601114 time: 1703256793.7919357\n",
      "Training [14%] Loss: 0.014215096770664768 time: 1703256793.7919357\n",
      "weight: [ 0.54339899  0.00828233  0.08457124  0.89855419  0.60642906  0.00919705\n",
      "  0.15460936  0.60325991 -0.11957774  0.16080805  0.54873379  0.6918952\n",
      "  0.46168345  0.14788988  0.63632159  0.23724909  0.3253997   0.74649141\n",
      "  0.7028623   0.90178633  0.59737104  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 43\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011594339838118348 R2: 0.012394457016686622 time: 1703256797.2570658\n",
      "batch_idx: 1 loss: 0.016838730137814555 R2: 0.012337094122818582 time: 1703256800.749203\n",
      "Training [14%] Loss: 0.014216534987966451 time: 1703256800.749203\n",
      "weight: [ 0.54286871  0.00780708  0.08390549  0.89855419  0.60642906  0.00919705\n",
      "  0.15477461  0.60260601 -0.12131665  0.16080805  0.54873379  0.6918952\n",
      "  0.4596219   0.14746104  0.63584634  0.23724909  0.3253997   0.74649141\n",
      "  0.7030624   0.90183986  0.59671713  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 44\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011606231030193951 R2: 0.012399459036468441 time: 1703256804.2119398\n",
      "batch_idx: 1 loss: 0.016830340030792205 R2: 0.012374185709534968 time: 1703256807.5819485\n",
      "Training [15%] Loss: 0.014218285530493077 time: 1703256807.5819485\n",
      "weight: [ 0.54240568  0.00753311  0.08344916  0.89855419  0.60642906  0.00919705\n",
      "  0.15470305  0.60216702 -0.1228964   0.16080805  0.54873379  0.6918952\n",
      "  0.45704607  0.14722041  0.63557236  0.23724909  0.3253997   0.74649141\n",
      "  0.70302923  0.90165366  0.59627815  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 45\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011608299808989374 R2: 0.012507534306144596 time: 1703256810.9233603\n",
      "batch_idx: 1 loss: 0.016827865523186297 R2: 0.012546034619548507 time: 1703256814.3918016\n",
      "Training [15%] Loss: 0.014218082666087836 time: 1703256814.3918016\n",
      "weight: [ 0.54200611  0.00745382  0.08319539  0.89855419  0.60642906  0.00919705\n",
      "  0.15440186  0.60193562 -0.12432344  0.16080805  0.54873379  0.6918952\n",
      "  0.45396615  0.14716172  0.63549307  0.23724909  0.3253997   0.74649141\n",
      "  0.70277004  0.90123491  0.59604675  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 46\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011602692509440874 R2: 0.012684751792585325 time: 1703256817.7521882\n",
      "batch_idx: 1 loss: 0.016829973459899426 R2: 0.012745020609560065 time: 1703256821.1433375\n",
      "Training [15%] Loss: 0.01421633298467015 time: 1703256821.1433375\n",
      "weight: [ 0.5416438   0.0074953   0.08306675  0.89855419  0.60642906  0.00919705\n",
      "  0.15395696  0.6018319  -0.12565914  0.16080805  0.54873379  0.6918952\n",
      "  0.4505576   0.14721581  0.63553456  0.23724909  0.3253997   0.74649141\n",
      "  0.70236969  0.90067018  0.59594302  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 47\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01159485606170576 R2: 0.012813722754477452 time: 1703256824.8121686\n",
      "batch_idx: 1 loss: 0.016833941043074195 R2: 0.012866864438524472 time: 1703256828.731974\n",
      "Training [16%] Loss: 0.014214398552389977 time: 1703256828.731974\n",
      "weight: [ 0.54128368  0.00755107  0.08295163  0.89855419  0.60642906  0.00919705\n",
      "  0.15349251  0.60174173 -0.1269904   0.16080805  0.54873379  0.6918952\n",
      "  0.44708573  0.14728333  0.63559033  0.23724909  0.3253997   0.74649141\n",
      "  0.70195051  0.9000847   0.59585285  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 48\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011589520245247002 R2: 0.012861816753277022 time: 1703256832.5220168\n",
      "batch_idx: 1 loss: 0.016835659223047066 R2: 0.012926797423420067 time: 1703256835.9433677\n",
      "Training [16%] Loss: 0.014212589734147034 time: 1703256835.9433677\n",
      "weight: [ 0.54089611  0.00752424  0.08274854  0.89855419  0.60642906  0.00919705\n",
      "  0.15312164  0.60156236 -0.1283951   0.16080805  0.54873379  0.6918952\n",
      "  0.44380417  0.14727403  0.6355635   0.23724909  0.3253997   0.74649141\n",
      "  0.70162371  0.89959276  0.59567349  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 49\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011588797151432061 R2: 0.012902320557870883 time: 1703256839.551788\n",
      "batch_idx: 1 loss: 0.016831840388229383 R2: 0.013001235541677314 time: 1703256843.3858712\n",
      "Training [16%] Loss: 0.014210318769830723 time: 1703256843.3858712\n",
      "weight: [ 0.54046646  0.00736129  0.08240146  0.89855419  0.60642906  0.00919705\n",
      "  0.15290691  0.60123795 -0.12991517  0.16080805  0.54873379  0.6918952\n",
      "  0.44086305  0.14713823  0.63540055  0.23724909  0.3253997   0.74649141\n",
      "  0.7014506   0.89925774  0.59534907  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 50\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011592801568167098 R2: 0.013000599210373665 time: 1703256847.2568786\n",
      "batch_idx: 1 loss: 0.016822234561701115 R2: 0.013114677999472413 time: 1703256851.0221896\n",
      "Training [17%] Loss: 0.014207518064934106 time: 1703256851.0221896\n",
      "weight: [ 0.53999745  0.00706595  0.08191443  0.89855419  0.60642906  0.00919705\n",
      "  0.15284407  0.60077332 -0.13154638  0.16080805  0.54873379  0.6918952\n",
      "  0.43826196  0.14687954  0.63510521  0.23724909  0.3253997   0.74649141\n",
      "  0.70142686  0.89907553  0.59488444  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 51\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011600619546772276 R2: 0.01313861688103335 time: 1703256854.8720381\n",
      "batch_idx: 1 loss: 0.016809486896547272 R2: 0.013226071796864969 time: 1703256858.731896\n",
      "Training [17%] Loss: 0.014205053221659774 time: 1703256858.731896\n",
      "weight: [ 0.53950469  0.00668999  0.08134186  0.89855419  0.60642906  0.00919705\n",
      "  0.15287266  0.6002235  -0.13324672  0.16080805  0.54873379  0.6918952\n",
      "  0.43586524  0.14654609  0.63472924  0.23724909  0.3253997   0.74649141\n",
      "  0.70149305  0.8989851   0.59433462  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 52\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011610396162949024 R2: 0.013260188162491637 time: 1703256862.5571606\n",
      "batch_idx: 1 loss: 0.016797016676270368 R2: 0.013303593145277737 time: 1703256866.4769492\n",
      "Training [17%] Loss: 0.014203706419609696 time: 1703256866.4769492\n",
      "weight: [ 0.53900842  0.00630681  0.08076082  0.89855419  0.60642906  0.00919705\n",
      "  0.15290666  0.59966566 -0.13495795  0.16080805  0.54873379  0.6918952\n",
      "  0.43346996  0.14620602  0.63434606  0.23724909  0.3253997   0.74649141\n",
      "  0.70156487  0.89889943  0.59377679  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 53\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011619448070155886 R2: 0.013348254222368516 time: 1703256870.404061\n",
      "batch_idx: 1 loss: 0.01678713142782636 R2: 0.013367295451113148 time: 1703256874.1620767\n",
      "Training [18%] Loss: 0.014203289748991123 time: 1703256874.1620767\n",
      "weight: [ 0.53852496  0.00597997  0.08023804  0.89855419  0.60642906  0.00919705\n",
      "  0.15287136  0.59916623 -0.13663047  0.16080805  0.54873379  0.6918952\n",
      "  0.43089445  0.14591824  0.63401922  0.23724909  0.3253997   0.74649141\n",
      "  0.7015692   0.89874287  0.59327735  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 54\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011625380133117324 R2: 0.01342942985704243 time: 1703256878.032213\n",
      "batch_idx: 1 loss: 0.016780586607721627 R2: 0.013451955751006328 time: 1703256881.8072314\n",
      "Training [18%] Loss: 0.014202983370419475 time: 1703256881.8072314\n",
      "weight: [ 0.53806125  0.00573948  0.07980506  0.89855419  0.60642906  0.00919705\n",
      "  0.15273131  0.59875632 -0.13824123  0.16080805  0.54873379  0.6918952\n",
      "  0.42804944  0.14571049  0.63377874  0.23724909  0.3253997   0.74649141\n",
      "  0.70147146  0.89847949  0.59286745  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 55\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011627501635784087 R2: 0.013522617399676862 time: 1703256885.6220505\n",
      "batch_idx: 1 loss: 0.016776954142458937 R2: 0.013559090116496031 time: 1703256889.352242\n",
      "Training [18%] Loss: 0.014202227889121512 time: 1703256889.352242\n",
      "weight: [ 0.53761411  0.00557452  0.07945054  0.89855419  0.60642906  0.00919705\n",
      "  0.15249917  0.59842459 -0.13979859  0.16080805  0.54873379  0.6918952\n",
      "  0.42496459  0.14557272  0.63361378  0.23724909  0.3253997   0.74649141\n",
      "  0.70128408  0.89812214  0.59253571  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 56\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01162711296584763 R2: 0.013615893347725727 time: 1703256893.2719085\n",
      "batch_idx: 1 loss: 0.016774913358584522 R2: 0.013663862399209248 time: 1703256897.2922153\n",
      "Training [19%] Loss: 0.014201013162216076 time: 1703256897.2922153\n",
      "weight: [ 0.53717378  0.00544357  0.07913097  0.89855419  0.60642906  0.00919705\n",
      "  0.15222393  0.59812813 -0.14133388  0.16080805  0.54873379  0.6918952\n",
      "  0.42176417  0.14546651  0.63348283  0.23724909  0.3253997   0.74649141\n",
      "  0.70105489  0.89772052  0.59223926  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 57\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011626432260367655 R2: 0.013694308560105672 time: 1703256901.1068523\n",
      "batch_idx: 1 loss: 0.016772572089165376 R2: 0.01375306648886332 time: 1703256904.843195\n",
      "Training [19%] Loss: 0.014199502174766516 time: 1703256904.843195\n",
      "weight: [ 0.53672951  0.00529594  0.07879322  0.89855419  0.60642906  0.00919705\n",
      "  0.15196549  0.59781507 -0.14288471  0.16080805  0.54873379  0.6918952\n",
      "  0.41860585  0.14534506  0.6333352   0.23724909  0.3253997   0.74649141\n",
      "  0.70084226  0.89733552  0.59192619  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 58\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011627269779410313 R2: 0.013765514875417884 time: 1703256908.7220914\n",
      "batch_idx: 1 loss: 0.016768249096802086 R2: 0.013835685412141085 time: 1703256912.5920324\n",
      "Training [19%] Loss: 0.0141977594381062 time: 1703256912.5920324\n",
      "weight: [ 0.53627437  0.00509446  0.07839838  0.89855419  0.60642906  0.00919705\n",
      "  0.1517679   0.59744775 -0.14447806  0.16080805  0.54873379  0.6918952\n",
      "  0.41561051  0.1451741   0.63313372  0.23724909  0.3253997   0.74649141\n",
      "  0.70068902  0.89701199  0.59155888  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 59\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01163040724634843 R2: 0.013846742492921615 time: 1703256916.495338\n",
      "batch_idx: 1 loss: 0.01676138598386604 R2: 0.013920989997533773 time: 1703256920.2170467\n",
      "Training [20%] Loss: 0.014195896615107235 time: 1703256920.2170467\n",
      "weight: [ 0.5358073   0.00482952  0.07793645  0.89855419  0.60642906  0.00919705\n",
      "  0.15164265  0.5970168  -0.14612021  0.16080805  0.54873379  0.6918952\n",
      "  0.41281404  0.14494481  0.63286877  0.23724909  0.3253997   0.74649141\n",
      "  0.70060627  0.89676173  0.59112792  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 60\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011635625230793747 R2: 0.013940177226504424 time: 1703256924.0670533\n",
      "batch_idx: 1 loss: 0.016752776666924835 R2: 0.014004966708314082 time: 1703256927.9070523\n",
      "Training [20%] Loss: 0.01419420094885929 time: 1703256927.9070523\n",
      "weight: [ 0.53533228  0.00451934  0.07742665  0.89855419  0.60642906  0.00919705\n",
      "  0.15156826  0.59654101 -0.14779711  0.16080805  0.54873379  0.6918952\n",
      "  0.41016112  0.14467403  0.6325586   0.23724909  0.3253997   0.74649141\n",
      "  0.70057305  0.89656292  0.59065213  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 61\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01164193809070145 R2: 0.014033834564506464 time: 1703256931.7919266\n",
      "batch_idx: 1 loss: 0.016743915985260906 R2: 0.01408190531689224 time: 1703256935.5824177\n",
      "Training [20%] Loss: 0.014192927037981178 time: 1703256935.5824177\n",
      "weight: [ 0.53485542  0.00419801  0.07690479  0.89855419  0.60642906  0.00919705\n",
      "  0.15150436  0.59605491 -0.14948355  0.16080805  0.54873379  0.6918952\n",
      "  0.40754108  0.14439308  0.63223726  0.23724909  0.3253997   0.74649141\n",
      "  0.70055017  0.89637452  0.59016604  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 62\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011647977709312887 R2: 0.014120078070100628 time: 1703256939.4020486\n",
      "batch_idx: 1 loss: 0.016736098975554958 R2: 0.014156316026699312 time: 1703256943.2430851\n",
      "Training [21%] Loss: 0.014192038342433922 time: 1703256943.2430851\n",
      "weight: [ 0.5343819   0.00389784  0.07640485  0.89855419  0.60642906  0.00919705\n",
      "  0.15141254  0.59559106 -0.15115605  0.16080805  0.54873379  0.6918952\n",
      "  0.40484556  0.14413169  0.6319371   0.23724909  0.3253997   0.74649141\n",
      "  0.70050038  0.89615742  0.58970218  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 63\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011652621127860275 R2: 0.014201659463899285 time: 1703256947.202005\n",
      "batch_idx: 1 loss: 0.016729903932324392 R2: 0.014236169976335367 time: 1703256950.922343\n",
      "Training [21%] Loss: 0.014191262530092334 time: 1703256950.922343\n",
      "weight: [ 0.53391392  0.00363496  0.07594376  0.89855419  0.60642906  0.00919705\n",
      "  0.15127361  0.59516555 -0.15280297  0.16080805  0.54873379  0.6918952\n",
      "  0.40201918  0.14390462  0.63167421  0.23724909  0.3253997   0.74649141\n",
      "  0.70040511  0.89589204  0.58927667  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 64\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011655577066948896 R2: 0.014281823299187146 time: 1703256954.732115\n",
      "batch_idx: 1 loss: 0.016725129212013315 R2: 0.014321397129132718 time: 1703256958.5667894\n",
      "Training [21%] Loss: 0.014190353139481107 time: 1703256958.5667894\n",
      "weight: [ 0.53345057  0.00340415  0.07551611  0.89855419  0.60642906  0.00919705\n",
      "  0.15109377  0.5947732  -0.15442779  0.16080805  0.54873379  0.6918952\n",
      "  0.39907995  0.1437071   0.63144341  0.23724909  0.3253997   0.74649141\n",
      "  0.70027037  0.89558474  0.58888432  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 65\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011657462384360927 R2: 0.01435883301356744 time: 1703256962.561931\n",
      "batch_idx: 1 loss: 0.016720987853934964 R2: 0.01440567619763955 time: 1703256966.3820508\n",
      "Training [22%] Loss: 0.014189225119147945 time: 1703256966.3820508\n",
      "weight: [ 0.53298919  0.00318451  0.07509999  0.89855419  0.60642906  0.00919705\n",
      "  0.15089803  0.59439329 -0.15604478  0.16080805  0.54873379  0.6918952\n",
      "  0.39610273  0.14351992  0.63122377  0.23724909  0.3253997   0.74649141\n",
      "  0.70012035  0.89526107  0.58850441  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 66\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011659319647328411 R2: 0.014431839023954285 time: 1703256970.566915\n",
      "batch_idx: 1 loss: 0.0167164907867886 R2: 0.014485598357439233 time: 1703256974.3269973\n",
      "Training [22%] Loss: 0.014187905217058506 time: 1703256974.3269973\n",
      "weight: [ 0.53252719  0.0029517   0.07466995  0.89855419  0.60642906  0.00919705\n",
      "  0.15071553  0.59400194 -0.1576703   0.16080805  0.54873379  0.6918952\n",
      "  0.3931771   0.14332082  0.63099096  0.23724909  0.3253997   0.74649141\n",
      "  0.69998318  0.89495087  0.58811306  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 67\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011662001040789353 R2: 0.01450480693756917 time: 1703256978.3269098\n",
      "batch_idx: 1 loss: 0.016710951125084483 R2: 0.014562379172166628 time: 1703256982.1524112\n",
      "Training [22%] Loss: 0.014186476082936918 time: 1703256982.1524112\n",
      "weight: [ 0.53206339  0.00269036  0.07420994  0.89855419  0.60642906  0.00919705\n",
      "  0.15056477  0.59358426 -0.15931431  0.16080805  0.54873379  0.6918952\n",
      "  0.39036218  0.14309572  0.63072962  0.23724909  0.3253997   0.74649141\n",
      "  0.69987666  0.8946731   0.58769538  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 68\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011665810725639403 R2: 0.014581430713450638 time: 1703256985.9955766\n",
      "batch_idx: 1 loss: 0.01670433619837452 R2: 0.014636942974194933 time: 1703256989.8019931\n",
      "Training [23%] Loss: 0.01418507346200696 time: 1703256989.8019931\n",
      "weight: [ 0.53159818  0.00240039  0.07371993  0.89855419  0.60642906  0.00919705\n",
      "  0.15044595  0.59314035 -0.16097632  0.16080805  0.54873379  0.6918952\n",
      "  0.3876613   0.14284455  0.63043965  0.23724909  0.3253997   0.74649141\n",
      "  0.69980094  0.89442802  0.58725147  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 69\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011670471432989226 R2: 0.01466032958096708 time: 1703256993.7722535\n",
      "batch_idx: 1 loss: 0.016697205337434552 R2: 0.014708694798820976 time: 1703256997.5272593\n",
      "Training [23%] Loss: 0.014183838385211889 time: 1703256997.5272593\n",
      "weight: [ 0.53113293  0.00209481  0.07321363  0.89855419  0.60642906  0.00919705\n",
      "  0.1503435   0.59268306 -0.16264715  0.16080805  0.54873379  0.6918952\n",
      "  0.38502772  0.14257922  0.63013407  0.23724909  0.3253997   0.74649141\n",
      "  0.69974102  0.89419974  0.58679418  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 70\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011675326414413167 R2: 0.01473776161530671 time: 1703257001.4019084\n",
      "batch_idx: 1 loss: 0.016690305721138367 R2: 0.014778591075942593 time: 1703257005.1270242\n",
      "Training [23%] Loss: 0.014182816067775767 time: 1703257005.1270242\n",
      "weight: [ 0.530669    0.00179126  0.0727096   0.89855419  0.60642906  0.00919705\n",
      "  0.15023628  0.59222961 -0.16431474  0.16080805  0.54873379  0.6918952\n",
      "  0.38239554  0.14231582  0.62983051  0.23724909  0.3253997   0.74649141\n",
      "  0.69967654  0.89396662  0.58634073  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 71\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011679697251922196 R2: 0.014812098219305736 time: 1703257009.0655978\n",
      "batch_idx: 1 loss: 0.016684147973769083 R2: 0.014849281762788102 time: 1703257012.8423061\n",
      "Training [24%] Loss: 0.014181922612845638 time: 1703257012.8423061\n",
      "weight: [ 0.53020711  0.00150225  0.07222099  0.89855419  0.60642906  0.00919705\n",
      "  0.15010921  0.59179215 -0.16597061  0.16080805  0.54873379  0.6918952\n",
      "  0.37971727  0.14206577  0.62954151  0.23724909  0.3253997   0.74649141\n",
      "  0.69959301  0.89371325  0.58590327  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 72\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01168325009428517 R2: 0.014883631338227121 time: 1703257016.7439847\n",
      "batch_idx: 1 loss: 0.01667879782261455 R2: 0.014921735206117503 time: 1703257020.4521022\n",
      "Training [24%] Loss: 0.01418102395844986 time: 1703257020.4521022\n",
      "weight: [ 0.52974731  0.00122947  0.0717496   0.89855419  0.60642906  0.00919705\n",
      "  0.14996032  0.59137233 -0.16761342  0.16080805  0.54873379  0.6918952\n",
      "  0.3769873   0.14183058  0.62926872  0.23724909  0.3253997   0.74649141\n",
      "  0.69948852  0.8934376   0.58548345  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 73\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011686145530408618 R2: 0.014952536115197534 time: 1703257024.3121326\n",
      "batch_idx: 1 loss: 0.016673915971298553 R2: 0.014994210040868983 time: 1703257028.1292117\n",
      "Training [24%] Loss: 0.014180030750853586 time: 1703257028.1292117\n",
      "weight: [ 0.5292893   0.0009645   0.07128667  0.89855419  0.60642906  0.00919705\n",
      "  0.14979976  0.59096215 -0.1692483   0.16080805  0.54873379  0.6918952\n",
      "  0.37424027  0.1416026   0.62900376  0.23724909  0.3253997   0.74649141\n",
      "  0.69937284  0.89315015  0.58507327  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 74\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01168887464867003 R2: 0.01501926658280972 time: 1703257031.926797\n",
      "batch_idx: 1 loss: 0.016668983239098746 R2: 0.015064728230677393 time: 1703257035.8337355\n",
      "Training [25%] Loss: 0.014178928943884388 time: 1703257035.8337355\n",
      "weight: [ 5.28832890e-01  6.94974643e-04  7.08192566e-02  8.98554189e-01\n",
      "  6.06429060e-01  9.19705162e-03  1.49642575e-01  5.90549906e-01\n",
      " -1.70882756e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  3.71527755e-01  1.41370543e-01  6.28734231e-01  2.37249087e-01\n",
      "  3.25399698e-01  7.46491405e-01  6.99260359e-01  8.92866361e-01\n",
      "  5.84661029e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 75\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011691927053544545 R2: 0.015085446604158781 time: 1703257039.9720592\n",
      "batch_idx: 1 loss: 0.01666360421860758 R2: 0.015132706070768331 time: 1703257043.9469564\n",
      "Training [25%] Loss: 0.014177765636076062 time: 1703257043.9469564\n",
      "weight: [ 5.28378167e-01  4.11928032e-04  7.03380469e-02  8.98554189e-01\n",
      "  6.06429060e-01  9.19705162e-03  1.49499667e-01  5.90127217e-01\n",
      " -1.72522036e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  3.68888285e-01  1.41126263e-01  6.28451184e-01  2.37249087e-01\n",
      "  3.25399698e-01  7.46491405e-01  6.99161490e-01  8.92597491e-01\n",
      "  5.84238340e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 76\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011695524382012618 R2: 0.015152491329559648 time: 1703257047.9471915\n",
      "batch_idx: 1 loss: 0.01665772220572815 R2: 0.015198263314872729 time: 1703257051.9221923\n",
      "Training [25%] Loss: 0.014176623293870384 time: 1703257051.9221923\n",
      "weight: [ 5.27925368e-01  1.14336512e-04  6.98420201e-02  8.98554189e-01\n",
      "  6.06429060e-01  9.19705162e-03  1.49372339e-01  5.89693250e-01\n",
      " -1.74166377e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  3.66328088e-01  1.40868825e-01  6.28153593e-01  2.37249087e-01\n",
      "  3.25399698e-01  7.46491405e-01  6.99077462e-01  8.92344919e-01\n",
      "  5.83804374e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 77\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011699545668747171 R2: 0.015220000335244732 time: 1703257056.0921705\n",
      "batch_idx: 1 loss: 0.016651608297306716 R2: 0.015261731734744766 time: 1703257059.9758894\n",
      "Training [26%] Loss: 0.014175576983026943 time: 1703257059.9758894\n",
      "weight: [ 5.27474630e-01 -1.91456881e-04  6.93378695e-02  8.98554189e-01\n",
      "  6.06429060e-01  9.19705162e-03  1.49252932e-01  5.89254096e-01\n",
      " -1.75811515e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  3.63822461e-01  1.40603991e-01  6.27847799e-01  2.37249087e-01\n",
      "  3.25399698e-01  7.46491405e-01  6.99000941e-01  8.92100801e-01\n",
      "  5.83365219e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 78\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01170364114287748 R2: 0.01528646032873604 time: 1703257063.7853253\n",
      "batch_idx: 1 loss: 0.01664565133312271 R2: 0.015324055134326664 time: 1703257067.6522028\n",
      "Training [26%] Loss: 0.014174646238000094 time: 1703257067.6522028\n",
      "weight: [ 5.27025879e-01 -4.96406861e-04  6.88351067e-02  8.98554189e-01\n",
      "  6.06429060e-01  9.19705162e-03  1.49130476e-01  5.88818324e-01\n",
      " -1.77451641e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  3.61334756e-01  1.40339969e-01  6.27542849e-01  2.37249087e-01\n",
      "  3.25399698e-01  7.46491405e-01  6.98921439e-01  8.91853865e-01\n",
      "  5.82929447e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 79\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011707457497253135 R2: 0.0153508301646047 time: 1703257071.4952347\n",
      "batch_idx: 1 loss: 0.016640113644385734 R2: 0.015386367768625009 time: 1703257075.4769883\n",
      "Training [26%] Loss: 0.014173785570819435 time: 1703257075.4769883\n",
      "weight: [ 5.26578962e-01 -7.94332252e-04  6.83402360e-02  8.98554189e-01\n",
      "  6.06429060e-01  9.19705162e-03  1.48997460e-01  5.88391774e-01\n",
      " -1.79082775e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  3.58839980e-01  1.40082365e-01  6.27244924e-01  2.37249087e-01\n",
      "  3.25399698e-01  7.46491405e-01  6.98831787e-01  8.91596397e-01\n",
      "  5.82502897e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 80\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011710846183257528 R2: 0.015412880911307436 time: 1703257079.4523714\n",
      "batch_idx: 1 loss: 0.01663500303537921 R2: 0.015448850241987833 time: 1703257083.3619184\n",
      "Training [27%] Loss: 0.014172924609318368 time: 1703257083.3619184\n",
      "weight: [ 0.52613388 -0.00108503  0.0678535   0.89855419  0.60642906  0.00919705\n",
      "  0.14885366  0.58797469 -0.18070455  0.16080805  0.54873379  0.6918952\n",
      "  0.35633855  0.13983136  0.62695423  0.23724909  0.3253997   0.74649141\n",
      "  0.69873176  0.89132819  0.58208582  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 81\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01171392556432472 R2: 0.015472855022018117 time: 1703257087.3422163\n",
      "batch_idx: 1 loss: 0.01663010795855138 R2: 0.015510635885684354 time: 1703257091.3155768\n",
      "Training [27%] Loss: 0.01417201676143805 time: 1703257091.3155768\n",
      "weight: [ 0.52569093 -0.00137341  0.0673698   0.89855419  0.60642906  0.00919705\n",
      "  0.1487051   0.5875626  -0.1823196   0.16080805  0.54873379  0.6918952\n",
      "  0.35385312  0.13958249  0.62666585  0.23724909  0.3253997   0.74649141\n",
      "  0.69862711  0.89105549  0.58167373  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 82\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011716967431838624 R2: 0.015531455069204503 time: 1703257095.2021613\n",
      "batch_idx: 1 loss: 0.016625153371479278 R2: 0.015570790595497819 time: 1703257099.1018565\n",
      "Training [27%] Loss: 0.014171060401658952 time: 1703257099.1018565\n",
      "weight: [ 0.52525059 -0.00166572  0.06688265  0.89855419  0.60642906  0.00919705\n",
      "  0.14855947  0.58714983 -0.18393127  0.16080805  0.54873379  0.6918952\n",
      "  0.35141234  0.13933009  0.62637354  0.23724909  0.3253997   0.74649141\n",
      "  0.69852513  0.89078626  0.58126095  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 83\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011720205527908638 R2: 0.01558959800367632 time: 1703257103.037177\n",
      "batch_idx: 1 loss: 0.016619978080377636 R2: 0.015628926300176982 time: 1703257106.881871\n",
      "Training [28%] Loss: 0.014170091804143137 time: 1703257106.881871\n",
      "weight: [ 0.52481324 -0.00196546  0.06638843  0.89855419  0.60642906  0.00919705\n",
      "  0.1484211   0.58673325 -0.18554127  0.16080805  0.54873379  0.6918952\n",
      "  0.34903302  0.13907099  0.6260738   0.23724909  0.3253997   0.74649141\n",
      "  0.69842991  0.89052501  0.58084438  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 84\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011723699335056122 R2: 0.015647653401772432 time: 1703257110.8851461\n",
      "batch_idx: 1 loss: 0.01661462130317512 R2: 0.01568515259723513 time: 1703257114.7370727\n",
      "Training [28%] Loss: 0.014169160319115621 time: 1703257114.7370727\n",
      "weight: [ 0.52437894 -0.00227161  0.06588824  0.89855419  0.60642906  0.00919705\n",
      "  0.14828878  0.5863139  -0.18714874  0.16080805  0.54873379  0.6918952\n",
      "  0.3467119   0.1388061   0.62576765  0.23724909  0.3253997   0.74649141\n",
      "  0.69834031  0.8902705   0.58042502  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 85\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011727324208762328 R2: 0.015705146098502376 time: 1703257118.6319745\n",
      "batch_idx: 1 loss: 0.01660926871608005 R2: 0.015739974934847134 time: 1703257122.7090158\n",
      "Training [28%] Loss: 0.014168296462421189 time: 1703257122.7090158\n",
      "weight: [ 0.52394736 -0.0025799   0.06538658  0.89855419  0.60642906  0.00919705\n",
      "  0.14815727  0.58589574 -0.18875101  0.16080805  0.54873379  0.6918952\n",
      "  0.34443107  0.13853929  0.62545935  0.23724909  0.3253997   0.74649141\n",
      "  0.69825134  0.89001737  0.58000686  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 86\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011730874364325595 R2: 0.015761315570858803 time: 1703257126.6620991\n",
      "batch_idx: 1 loss: 0.01660411159485172 R2: 0.015794083906478074 time: 1703257130.6411753\n",
      "Training [29%] Loss: 0.014167492979588657 time: 1703257130.6411753\n",
      "weight: [ 0.52351808 -0.00288595  0.06488806  0.89855419  0.60642906  0.00919705\n",
      "  0.14802119  0.58548284 -0.1903454   0.16080805  0.54873379  0.6918952\n",
      "  0.34217192  0.13827451  0.62515331  0.23724909  0.3253997   0.74649141\n",
      "  0.69815789  0.88976006  0.57959396  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 87\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011734204085690491 R2: 0.01581571706395879 time: 1703257134.4679258\n",
      "batch_idx: 1 loss: 0.016599224939540462 R2: 0.015847843777873405 time: 1703257138.6670523\n",
      "Training [29%] Loss: 0.014166714512615476 time: 1703257138.6670523\n",
      "weight: [ 0.52309093 -0.003188    0.06439454  0.89855419  0.60642906  0.00919705\n",
      "  0.1478784   0.58507683 -0.19193074  0.16080805  0.54873379  0.6918952\n",
      "  0.33992761  0.13801335  0.62485126  0.23724909  0.3253997   0.74649141\n",
      "  0.69805791  0.88949637  0.57918796  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 88\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011737313229030506 R2: 0.015868363606871827 time: 1703257142.7157\n",
      "batch_idx: 1 loss: 0.016594538494752193 R2: 0.015901006401059514 time: 1703257146.9988096\n",
      "Training [29%] Loss: 0.01416592586189135 time: 1703257146.9988096\n",
      "weight: [ 0.5226661  -0.00348759  0.06390444  0.89855419  0.60642906  0.00919705\n",
      "  0.14773082  0.5846764  -0.19350764  0.16080805  0.54873379  0.6918952\n",
      "  0.33770614  0.1377544   0.62455167  0.23724909  0.3253997   0.74649141\n",
      "  0.69795324  0.88922832  0.57878752  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 89\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011740324883379855 R2: 0.015919646086837602 time: 1703257150.9720674\n",
      "batch_idx: 1 loss: 0.016589904441059268 R2: 0.015953019603530683 time: 1703257154.9795969\n",
      "Training [30%] Loss: 0.014165114662219562 time: 1703257154.9795969\n",
      "weight: [ 0.52224409 -0.00378798  0.06341438  0.89855419  0.60642906  0.00919705\n",
      "  0.14758249  0.58427868 -0.19507764  0.16080805  0.54873379  0.6918952\n",
      "  0.33552323  0.13749472  0.62425128  0.23724909  0.3253997   0.74649141\n",
      "  0.69784769  0.88896014  0.5783898   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 90\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011743383832481363 R2: 0.015970116903704534 time: 1703257159.1019726\n",
      "batch_idx: 1 loss: 0.016585206637297047 R2: 0.0160034978018585 time: 1703257163.0069203\n",
      "Training [30%] Loss: 0.014164295234889206 time: 1703257163.0069203\n",
      "weight: [ 0.52182532 -0.00409166  0.0629218   0.89855419  0.60642906  0.00919705\n",
      "  0.14743654  0.58388153 -0.19664183  0.16080805  0.54873379  0.6918952\n",
      "  0.33339103  0.13723206  0.62394759  0.23724909  0.3253997   0.74649141\n",
      "  0.6977442   0.88869506  0.57799265  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 91\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011746556450524723 R2: 0.016020098193527788 time: 1703257167.1321473\n",
      "batch_idx: 1 loss: 0.016580434518848303 R2: 0.016052418548066938 time: 1703257171.1618967\n",
      "Training [30%] Loss: 0.014163495484686513 time: 1703257171.1618967\n",
      "weight: [ 0.52140986 -0.00439878  0.06242658  0.89855419  0.60642906  0.00919705\n",
      "  0.14729316  0.58348489 -0.19820006  0.16080805  0.54873379  0.6918952\n",
      "  0.33131082  0.13696627  0.62364048  0.23724909  0.3253997   0.74649141\n",
      "  0.69764293  0.88843333  0.57759601  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 92\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01174979830753258 R2: 0.016069449277575698 time: 1703257175.1520367\n",
      "batch_idx: 1 loss: 0.01657567456413234 R2: 0.01610008260356588 time: 1703257179.1368861\n",
      "Training [31%] Loss: 0.014162736435832459 time: 1703257179.1368861\n",
      "weight: [ 0.52099744 -0.00470733  0.06193083  0.89855419  0.60642906  0.00919705\n",
      "  0.14714988  0.5830906  -0.19975108  0.16080805  0.54873379  0.6918952\n",
      "  0.32927406  0.13669916  0.62333193  0.23724909  0.3253997   0.74649141\n",
      "  0.69754157  0.8881724   0.57720173  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 93\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011753000547743272 R2: 0.01611776070184412 time: 1703257183.1730614\n",
      "batch_idx: 1 loss: 0.016571035860247836 R2: 0.01614691924394207 time: 1703257187.303307\n",
      "Training [31%] Loss: 0.014162018203995555 time: 1703257187.303307\n",
      "weight: [ 0.52058764 -0.00501485  0.06143714  0.89855419  0.60642906  0.00919705\n",
      "  0.14700364  0.58270091 -0.20129344  0.16080805  0.54873379  0.6918952\n",
      "  0.32726994  0.13643294  0.62302441  0.23724909  0.3253997   0.74649141\n",
      "  0.69743721  0.88790911  0.57681203  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 94\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011756073612042815 R2: 0.016164718067628425 time: 1703257191.311165\n",
      "batch_idx: 1 loss: 0.016566571524640264 R2: 0.016193181906069 time: 1703257195.4765623\n",
      "Training [31%] Loss: 0.01416132256834154 time: 1703257195.4765623\n",
      "weight: [ 0.52018023 -0.00532011  0.0609468   0.89855419  0.60642906  0.00919705\n",
      "  0.14685292  0.58231695 -0.20282634  0.16080805  0.54873379  0.6918952\n",
      "  0.32529336  0.13616873  0.62271915  0.23724909  0.3253997   0.74649141\n",
      "  0.69732841  0.88764191  0.57642808  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 95\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011759006224507854 R2: 0.016210296210923958 time: 1703257199.7020338\n",
      "batch_idx: 1 loss: 0.016562250944709388 R2: 0.01623877750199154 time: 1703257203.7721074\n",
      "Training [32%] Loss: 0.01416062858460862 time: 1703257203.7721074\n",
      "weight: [ 0.51977533 -0.00562374  0.06045918  0.89855419  0.60642906  0.00919705\n",
      "  0.1466985   0.58193824 -0.20434991  0.16080805  0.54873379  0.6918952\n",
      "  0.32334766  0.13590594  0.62241552  0.23724909  0.3253997   0.74649141\n",
      "  0.69721592  0.88737163  0.57604936  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 96\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011761860844155415 R2: 0.016254739261684414 time: 1703257207.967289\n",
      "batch_idx: 1 loss: 0.016557994260580414 R2: 0.016283405050132593 time: 1703257212.0419493\n",
      "Training [32%] Loss: 0.014159927552367915 time: 1703257212.0419493\n",
      "weight: [ 0.5193733  -0.00592742  0.05997254  0.89855419  0.60642906  0.00919705\n",
      "  0.14654253  0.58156333 -0.20586482  0.16080805  0.54873379  0.6918952\n",
      "  0.321441    0.13564305  0.62211183  0.23724909  0.3253997   0.74649141\n",
      "  0.69710173  0.88710052  0.57567445  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 97\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011764717332060106 R2: 0.016298385814693894 time: 1703257216.3471062\n",
      "batch_idx: 1 loss: 0.016553736488511953 R2: 0.016326830896479128 time: 1703257220.462089\n",
      "Training [32%] Loss: 0.01415922691028603 time: 1703257220.462089\n",
      "weight: [ 0.51897446 -0.00623253  0.05948548  0.89855419  0.60642906  0.00919705\n",
      "  0.14638672  0.58119112 -0.20737155  0.16080805  0.54873379  0.6918952\n",
      "  0.31957982  0.13537883  0.62180673  0.23724909  0.3253997   0.74649141\n",
      "  0.69698746  0.88683037  0.57530224  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 98\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011767612988194164 R2: 0.016341426173264284 time: 1703257224.842379\n",
      "batch_idx: 1 loss: 0.016549471781251726 R2: 0.016369050030671572 time: 1703257229.1665988\n",
      "Training [33%] Loss: 0.014158542384722944 time: 1703257229.1665988\n",
      "weight: [ 0.51857882 -0.00653911  0.05899797  0.89855419  0.60642906  0.00919705\n",
      "  0.14623116  0.5808216  -0.20886997  0.16080805  0.54873379  0.6918952\n",
      "  0.31776447  0.13511321  0.62150015  0.23724909  0.3253997   0.74649141\n",
      "  0.69687317  0.8865613   0.57493272  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 99\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011770522420947813 R2: 0.016383785194110323 time: 1703257233.732255\n",
      "batch_idx: 1 loss: 0.016545248159067126 R2: 0.01641026839506632 time: 1703257238.0118957\n",
      "Training [33%] Loss: 0.01415788529000747 time: 1703257238.0118957\n",
      "weight: [ 0.51818614 -0.00684607  0.05851116  0.89855419  0.60642906  0.00919705\n",
      "  0.14607446  0.58045578 -0.21035938  0.16080805  0.54873379  0.6918952\n",
      "  0.31598996  0.13484718  0.62119318  0.23724909  0.3253997   0.74649141\n",
      "  0.69675758  0.8862919   0.5745669   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 100\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011773385655278941 R2: 0.016425237377396207 time: 1703257242.6772392\n",
      "batch_idx: 1 loss: 0.016541123502163024 R2: 0.016450749302385326 time: 1703257246.8992894\n",
      "Training [33%] Loss: 0.014157254578720982 time: 1703257246.8992894\n",
      "weight: [ 0.5177961  -0.00715211  0.05802641  0.89855419  0.60642906  0.00919705\n",
      "  0.145915    0.58009484 -0.21183902  0.16080805  0.54873379  0.6918952\n",
      "  0.31425033  0.13458191  0.62088715  0.23724909  0.3253997   0.74649141\n",
      "  0.69663914  0.88602048  0.57420596  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 101\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01177615731556618 R2: 0.016465618534430515 time: 1703257250.8879347\n",
      "batch_idx: 1 loss: 0.016537121124858823 R2: 0.01649062453655903 time: 1703257254.9121401\n",
      "Training [34%] Loss: 0.014156639220212501 time: 1703257254.9121401\n",
      "weight: [ 0.51740854 -0.00745668  0.05754431  0.89855419  0.60642906  0.00919705\n",
      "  0.1457521   0.5797393  -0.21330849  0.16080805  0.54873379  0.6918952\n",
      "  0.31254309  0.1343179   0.62058258  0.23724909  0.3253997   0.74649141\n",
      "  0.69651721  0.88574634  0.57385042  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 102\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011778837814060866 R2: 0.01650494785946144 time: 1703257259.1169817\n",
      "batch_idx: 1 loss: 0.01653321777172703 R2: 0.016529823701919577 time: 1703257263.122363\n",
      "Training [34%] Loss: 0.014156027792893947 time: 1703257263.122363\n",
      "weight: [ 0.51702357 -0.00776023  0.05706438  0.89855419  0.60642906  0.00919705\n",
      "  0.14558633  0.5793888  -0.21476787  0.16080805  0.54873379  0.6918952\n",
      "  0.31087029  0.13405471  0.62027903  0.23724909  0.3253997   0.74649141\n",
      "  0.69639232  0.88547012  0.57349992  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 103\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011781465671771324 R2: 0.01654340202840132 time: 1703257267.1971004\n",
      "batch_idx: 1 loss: 0.01652936800774283 R2: 0.016568177528076378 time: 1703257271.0422835\n",
      "Training [34%] Loss: 0.014155416839757077 time: 1703257271.0422835\n",
      "weight: [ 0.51664142 -0.00806371  0.05658567  0.89855419  0.60642906  0.00919705\n",
      "  0.1454189   0.5790426  -0.21621744  0.16080805  0.54873379  0.6918952\n",
      "  0.30923601  0.13379151  0.61997555  0.23724909  0.3253997   0.74649141\n",
      "  0.69626561  0.88519307  0.57315372  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 104\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011784082393752539 R2: 0.01658118123705432 time: 1703257275.2120917\n",
      "batch_idx: 1 loss: 0.016525541503574843 R2: 0.01660558059853179 time: 1703257279.1720243\n",
      "Training [35%] Loss: 0.01415481194866369 time: 1703257279.1720243\n",
      "weight: [ 0.51626227 -0.00836773  0.05610755  0.89855419  0.60642906  0.00919705\n",
      "  0.14525059  0.57870021 -0.21765733  0.16080805  0.54873379  0.6918952\n",
      "  0.30764274  0.13352772  0.61967153  0.23724909  0.3253997   0.74649141\n",
      "  0.69613779  0.88491604  0.57281133  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 105\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011786700683895174 R2: 0.016618367957547675 time: 1703257283.1951709\n",
      "batch_idx: 1 loss: 0.016521743135689333 R2: 0.01664207531809847 time: 1703257287.1319115\n",
      "Training [35%] Loss: 0.014154221909792254 time: 1703257287.1319115\n",
      "weight: [ 0.51588606 -0.00867214  0.05563018  0.89855419  0.60642906  0.00919705\n",
      "  0.14508121  0.57836181 -0.21908736  0.16080805  0.54873379  0.6918952\n",
      "  0.30608947  0.13326349  0.61936712  0.23724909  0.3253997   0.74649141\n",
      "  0.6960087   0.88463884  0.57247294  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 106\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011789299592648647 R2: 0.016654894627407657 time: 1703257291.107293\n",
      "batch_idx: 1 loss: 0.01651800294179602 R2: 0.016677814660073675 time: 1703257295.1469998\n",
      "Training [35%] Loss: 0.014153651267222334 time: 1703257295.1469998\n",
      "weight: [ 0.51551257 -0.00897626  0.05515428  0.89855419  0.60642906  0.00919705\n",
      "  0.14490992  0.57802802 -0.22050709  0.16080805  0.54873379  0.6918952\n",
      "  0.30457282  0.13299941  0.61906299  0.23724909  0.3253997   0.74649141\n",
      "  0.69587752  0.88436059  0.57213915  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 107\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011791846188658812 R2: 0.016690637961860766 time: 1703257299.4571228\n",
      "batch_idx: 1 loss: 0.016514348209471647 R2: 0.016712946668591355 time: 1703257303.2573104\n",
      "Training [36%] Loss: 0.01415309719906523 time: 1703257303.2573104\n",
      "weight: [ 0.51514159 -0.00927948  0.05468048  0.89855419  0.60642906  0.00919705\n",
      "  0.14473593  0.5776994  -0.22191614  0.16080805  0.54873379  0.6918952\n",
      "  0.30308967  0.13273605  0.61875977  0.23724909  0.3253997   0.74649141\n",
      "  0.69574352  0.88408051  0.57181052  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 108\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01179432315051342 R2: 0.016725540855153542 time: 1703257307.2821248\n",
      "batch_idx: 1 loss: 0.01651078212539504 R2: 0.016747514523369977 time: 1703257310.9171064\n",
      "Training [36%] Loss: 0.01415255263795423 time: 1703257310.9171064\n",
      "weight: [ 0.51477305 -0.0095817   0.0542089   0.89855419  0.60642906  0.00919705\n",
      "  0.14455912  0.57737606 -0.22331436  0.16080805  0.54873379  0.6918952\n",
      "  0.3016392   0.13247347  0.61845755  0.23724909  0.3253997   0.74649141\n",
      "  0.69560659  0.88379846  0.57148718  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 109\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011796739936242242 R2: 0.016759661748216637 time: 1703257314.992301\n",
      "batch_idx: 1 loss: 0.016507284578438686 R2: 0.01678145395603503 time: 1703257318.9682689\n",
      "Training [36%] Loss: 0.014152012257340463 time: 1703257318.9682689\n",
      "weight: [ 0.51440707 -0.00988333  0.05373911  0.89855419  0.60642906  0.00919705\n",
      "  0.14438002  0.57705771 -0.22470181  0.16080805  0.54873379  0.6918952\n",
      "  0.30022277  0.13221131  0.61815593  0.23724909  0.3253997   0.74649141\n",
      "  0.69546721  0.88351503  0.57116883  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 110\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01179912119353855 R2: 0.016793128478191033 time: 1703257323.1019697\n",
      "batch_idx: 1 loss: 0.016503831080255243 R2: 0.01681467869221598 time: 1703257327.1272016\n",
      "Training [37%] Loss: 0.014151476136896896 time: 1703257327.1272016\n",
      "weight: [ 0.51404379 -0.01018485  0.05327062  0.89855419  0.60642906  0.00919705\n",
      "  0.14419926  0.57674397 -0.22607858  0.16080805  0.54873379  0.6918952\n",
      "  0.29884204  0.13194913  0.61785441  0.23724909  0.3253997   0.74649141\n",
      "  0.69532598  0.88323088  0.57085509  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 111\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011801485062754509 R2: 0.016826043676180902 time: 1703257331.232174\n",
      "batch_idx: 1 loss: 0.016500412134193683 R2: 0.016847168793462264 time: 1703257334.987012\n",
      "Training [37%] Loss: 0.014150948598474097 time: 1703257334.987012\n",
      "weight: [ 0.51368325 -0.01048644  0.05280325  0.89855419  0.60642906  0.00919705\n",
      "  0.14401708  0.57643473 -0.22744463  0.16080805  0.54873379  0.6918952\n",
      "  0.29749722  0.13168676  0.61755281  0.23724909  0.3253997   0.74649141\n",
      "  0.6951831   0.88294627  0.57054585  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 112\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011803830174612603 R2: 0.016858421615877583 time: 1703257338.986906\n",
      "batch_idx: 1 loss: 0.01649703755368061 R2: 0.01687899175733043 time: 1703257342.792011\n",
      "Training [37%] Loss: 0.014150433864146607 time: 1703257342.792011\n",
      "weight: [ 0.51332533 -0.01078788  0.05233724  0.89855419  0.60642906  0.00919705\n",
      "  0.14383318  0.57613021 -0.22879978  0.16080805  0.54873379  0.6918952\n",
      "  0.29618674  0.1314244   0.61725138  0.23724909  0.3253997   0.74649141\n",
      "  0.6950383   0.88266091  0.57024133  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 113\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011806140330505113 R2: 0.016890206300156696 time: 1703257346.7022316\n",
      "batch_idx: 1 loss: 0.016493724570616326 R2: 0.016910251293336898 time: 1703257350.492047\n",
      "Training [38%] Loss: 0.01414993245056072 time: 1703257350.492047\n",
      "weight: [ 0.51296988 -0.01108877  0.05187299  0.89855419  0.60642906  0.00919705\n",
      "  0.14364708  0.57583077 -0.23014376  0.16080805  0.54873379  0.6918952\n",
      "  0.29490837  0.1311624   0.61695048  0.23724909  0.3253997   0.74649141\n",
      "  0.69489112  0.8823743   0.5699419   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 114\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01180840035827313 R2: 0.016921344350403156 time: 1703257354.3160288\n",
      "batch_idx: 1 loss: 0.016490481877937776 R2: 0.016941013395408877 time: 1703257358.106898\n",
      "Training [38%] Loss: 0.014149441118105453 time: 1703257358.106898\n",
      "weight: [ 0.51261679 -0.01138892  0.0514107   0.89855419  0.60642906  0.00919705\n",
      "  0.14345852  0.57553662 -0.23147641  0.16080805  0.54873379  0.6918952\n",
      "  0.29366067  0.13090094  0.61665033  0.23724909  0.3253997   0.74649141\n",
      "  0.69474132  0.88208618  0.56964774  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 115\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011810608609774693 R2: 0.016951843266445832 time: 1703257362.0423543\n",
      "batch_idx: 1 loss: 0.01648730318885786 R2: 0.016971273048148716 time: 1703257365.8671947\n",
      "Training [38%] Loss: 0.014148955899316277 time: 1703257365.8671947\n",
      "weight: [ 0.51226608 -0.01168844  0.05095027  0.89855419  0.60642906  0.00919705\n",
      "  0.14326766  0.57524767 -0.23279768  0.16080805  0.54873379  0.6918952\n",
      "  0.29244355  0.1306399   0.61635081  0.23724909  0.3253997   0.74649141\n",
      "  0.69458904  0.88179673  0.56935879  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 116\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011812776440929667 R2: 0.016981770858140677 time: 1703257369.8472028\n",
      "batch_idx: 1 loss: 0.016484174047257095 R2: 0.01700098480722323 time: 1703257373.527161\n",
      "Training [39%] Loss: 0.014148475244093382 time: 1703257373.527161\n",
      "weight: [ 0.51191782 -0.01198762  0.05049139  0.89855419  0.60642906  0.00919705\n",
      "  0.14307488  0.57496373 -0.23410759  0.16080805  0.54873379  0.6918952\n",
      "  0.29125762  0.13037903  0.61605163  0.23724909  0.3253997   0.74649141\n",
      "  0.69443463  0.88150636  0.56907485  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 117\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01181491713942552 R2: 0.017011205431703647 time: 1703257377.5040786\n",
      "batch_idx: 1 loss: 0.016481084225563596 R2: 0.017030121820871158 time: 1703257381.442251\n",
      "Training [39%] Loss: 0.014148000682494558 time: 1703257381.442251\n",
      "weight: [ 0.51157207 -0.01228666  0.05003385  0.89855419  0.60642906  0.00919705\n",
      "  0.14288044  0.57468465 -0.23540614  0.16080805  0.54873379  0.6918952\n",
      "  0.29010308  0.13011813  0.61575259  0.23724909  0.3253997   0.74649141\n",
      "  0.69427835  0.88121536  0.56879577  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 118\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011817035094808711 R2: 0.017040183841406042 time: 1703257385.3396351\n",
      "batch_idx: 1 loss: 0.016478034609470074 R2: 0.01705870940876797 time: 1703257389.4996102\n",
      "Training [39%] Loss: 0.014147534852139394 time: 1703257389.4996102\n",
      "weight: [ 0.51122879 -0.01258554  0.0495777   0.89855419  0.60642906  0.00919705\n",
      "  0.14268433  0.57441049 -0.23669323  0.16080805  0.54873379  0.6918952\n",
      "  0.28897913  0.12985724  0.61545372  0.23724909  0.3253997   0.74649141\n",
      "  0.69412014  0.8809237   0.56852162  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 119\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011819124122356172 R2: 0.01706869144883838 time: 1703257393.637077\n",
      "batch_idx: 1 loss: 0.01647503372482393 R2: 0.01708680988570399 time: 1703257397.607007\n",
      "Training [40%] Loss: 0.014147078923590052 time: 1703257397.607007\n",
      "weight: [ 0.51088788 -0.01288405  0.04912312  0.89855419  0.60642906  0.00919705\n",
      "  0.14248628  0.57414144 -0.23796871  0.16080805  0.54873379  0.6918952\n",
      "  0.28788434  0.12959651  0.61515521  0.23724909  0.3253997   0.74649141\n",
      "  0.69395979  0.88063114  0.56825256  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 120\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01182117486319795 R2: 0.01709669683286319 time: 1703257401.45725\n",
      "batch_idx: 1 loss: 0.016472088463415014 R2: 0.017114478414123613 time: 1703257405.1654785\n",
      "Training [40%] Loss: 0.014146631663306482 time: 1703257405.1654785\n",
      "weight: [ 0.51054923 -0.01318204  0.04867028  0.89855419  0.60642906  0.00919705\n",
      "  0.14228612  0.57387763 -0.23923244  0.16080805  0.54873379  0.6918952\n",
      "  0.28681744  0.1293361   0.61485722  0.23724909  0.3253997   0.74649141\n",
      "  0.69379712  0.88033749  0.56798875  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 121\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01182318362766643 R2: 0.017124194659672964 time: 1703257409.106945\n",
      "batch_idx: 1 loss: 0.016469197723004602 R2: 0.017141730925081866 time: 1703257412.9378736\n",
      "Training [40%] Loss: 0.014146190675335516 time: 1703257412.9378736\n",
      "weight: [ 0.51021283 -0.01347952  0.04821916  0.89855419  0.60642906  0.00919705\n",
      "  0.14208387  0.57361909 -0.24048437  0.16080805  0.54873379  0.6918952\n",
      "  0.28577785  0.12907599  0.61455973  0.23724909  0.3253997   0.74649141\n",
      "  0.69363214  0.88004278  0.56773021  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 122\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011825155014966542 R2: 0.017151219256859918 time: 1703257416.9182646\n",
      "batch_idx: 1 loss: 0.016466353846039804 R2: 0.017168550001017135 time: 1703257420.831907\n",
      "Training [41%] Loss: 0.014145754430503173 time: 1703257420.831907\n",
      "weight: [ 0.50987872 -0.01377666  0.0477696   0.89855419  0.60642906  0.00919705\n",
      "  0.14187973  0.57336571 -0.24172447  0.16080805  0.54873379  0.6918952\n",
      "  0.28476553  0.12881602  0.6142626   0.23724909  0.3253997   0.74649141\n",
      "  0.69346504  0.87974723  0.56747683  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 123\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011827097092405444 R2: 0.017177822206440818 time: 1703257424.8250115\n",
      "batch_idx: 1 loss: 0.016463549379115375 R2: 0.017194918020691063 time: 1703257428.575579\n",
      "Training [41%] Loss: 0.014145323235760409 time: 1703257428.575579\n",
      "weight: [ 0.50954693 -0.01407359  0.04732145  0.89855419  0.60642906  0.00919705\n",
      "  0.14167391  0.57311739 -0.24295274  0.16080805  0.54873379  0.6918952\n",
      "  0.28378044  0.12855608  0.61396567  0.23724909  0.3253997   0.74649141\n",
      "  0.69329601  0.87945108  0.56722851  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 124\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011829014434998755 R2: 0.017204038209743477 time: 1703257432.361922\n",
      "batch_idx: 1 loss: 0.016460782544091963 R2: 0.01722084468728957 time: 1703257436.1480203\n",
      "Training [41%] Loss: 0.014144898489545359 time: 1703257436.1480203\n",
      "weight: [ 0.50921746 -0.01437035  0.04687469  0.89855419  0.60642906  0.00919705\n",
      "  0.14146647  0.57287412 -0.2441691   0.16080805  0.54873379  0.6918952\n",
      "  0.28282205  0.12829612  0.61366891  0.23724909  0.3253997   0.74649141\n",
      "  0.6931251   0.87915437  0.56698525  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 125\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011830905311042438 R2: 0.017229870238921752 time: 1703257439.9720576\n",
      "batch_idx: 1 loss: 0.016458056949728298 R2: 0.017246366332645646 time: 1703257443.8833947\n",
      "Training [42%] Loss: 0.014144481130385368 time: 1703257443.8833947\n",
      "weight: [ 0.50889024 -0.01466684  0.04642938  0.89855419  0.60642906  0.00919705\n",
      "  0.14125729  0.572636   -0.24537349  0.16080805  0.54873379  0.6918952\n",
      "  0.28188938  0.12803622  0.61337241  0.23724909  0.3253997   0.74649141\n",
      "  0.6929522   0.87885701  0.56674712  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 126\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011832764745146409 R2: 0.017255304125995075 time: 1703257447.8519423\n",
      "batch_idx: 1 loss: 0.016455376731552264 R2: 0.017271522019402186 time: 1703257451.566895\n",
      "Training [42%] Loss: 0.014144070738349336 time: 1703257451.566895\n",
      "weight: [ 0.5085652  -0.01496299  0.04598562  0.89855419  0.60642906  0.00919705\n",
      "  0.14104628  0.57240311 -0.24656581  0.16080805  0.54873379  0.6918952\n",
      "  0.28098145  0.12777646  0.61307627  0.23724909  0.3253997   0.74649141\n",
      "  0.69277722  0.87855889  0.56651423  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 127\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011834589860342873 R2: 0.017280335300297156 time: 1703257455.4019213\n",
      "batch_idx: 1 loss: 0.01645274206812862 R2: 0.017296330560515938 time: 1703257459.331952\n",
      "Training [42%] Loss: 0.014143665964235747 time: 1703257459.331952\n",
      "weight: [ 0.50824231 -0.01525878  0.04554342  0.89855419  0.60642906  0.00919705\n",
      "  0.14083344  0.57217547 -0.24774599  0.16080805  0.54873379  0.6918952\n",
      "  0.28009763  0.12751685  0.61278048  0.23724909  0.3253997   0.74649141\n",
      "  0.69260015  0.87826002  0.56628659  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 128\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01183638250352205 R2: 0.01730498245102874 time: 1703257463.1645815\n",
      "batch_idx: 1 loss: 0.016450148899741057 R2: 0.01732078845542495 time: 1703257466.9820905\n",
      "Training [43%] Loss: 0.014143265701631554 time: 1703257466.9820905\n",
      "weight: [ 0.50792159 -0.01555428  0.0451027   0.89855419  0.60642906  0.00919705\n",
      "  0.14061887  0.57195304 -0.24891402  0.16080805  0.54873379  0.6918952\n",
      "  0.27923759  0.12725731  0.61248497  0.23724909  0.3253997   0.74649141\n",
      "  0.6924211   0.87796052  0.56606416  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 129\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011838147316099 R2: 0.017329278358807598 time: 1703257470.8833025\n",
      "batch_idx: 1 loss: 0.0164475924019083 R2: 0.017344887389747043 time: 1703257474.7822309\n",
      "Training [43%] Loss: 0.014142869859003648 time: 1703257474.7822309\n",
      "weight: [ 0.50760306 -0.0158496   0.04466335  0.89855419  0.60642906  0.00919705\n",
      "  0.14040273  0.57173574 -0.25006988  0.16080805  0.54873379  0.6918952\n",
      "  0.27840111  0.12699775  0.61218965  0.23724909  0.3253997   0.74649141\n",
      "  0.6922402   0.87766055  0.56584686  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 130\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011839887685601045 R2: 0.017353249445124508 time: 1703257478.7071593\n",
      "batch_idx: 1 loss: 0.01644507055679096 R2: 0.017368632746044277 time: 1703257482.5023236\n",
      "Training [43%] Loss: 0.014142479121196002 time: 1703257482.5023236\n",
      "weight: [ 0.50728672 -0.01614478  0.04422534  0.89855419  0.60642906  0.00919705\n",
      "  0.14018507  0.57152357 -0.25121353  0.16080805  0.54873379  0.6918952\n",
      "  0.27758773  0.12673814  0.61189448  0.23724909  0.3253997   0.74649141\n",
      "  0.6920575   0.87736018  0.56563469  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 131\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011841603478530678 R2: 0.017376903773986374 time: 1703257486.4724488\n",
      "batch_idx: 1 loss: 0.016442584668161415 R2: 0.01739204665758387 time: 1703257490.301938\n",
      "Training [44%] Loss: 0.014142094073346045 time: 1703257490.301938\n",
      "weight: [ 0.50697254 -0.01643977  0.04378869  0.89855419  0.60642906  0.00919705\n",
      "  0.13996587  0.57131655 -0.25234491  0.16080805  0.54873379  0.6918952\n",
      "  0.27679675  0.12647851  0.61159948  0.23724909  0.3253997   0.74649141\n",
      "  0.69187297  0.87705937  0.56542767  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 132\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011843292253398058 R2: 0.017400236901698563 time: 1703257494.2789857\n",
      "batch_idx: 1 loss: 0.016440136872651348 R2: 0.01741515538907823 time: 1703257498.1331878\n",
      "Training [44%] Loss: 0.014141714563024703 time: 1703257498.1331878\n",
      "weight: [ 0.50666047 -0.01673454  0.04335345  0.89855419  0.60642906  0.00919705\n",
      "  0.13974509  0.57111473 -0.25346398  0.16080805  0.54873379  0.6918952\n",
      "  0.27602741  0.12621888  0.61130471  0.23724909  0.3253997   0.74649141\n",
      "  0.69168657  0.87675809  0.56522585  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 133\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011844952302229327 R2: 0.017423247560488075 time: 1703257501.9836984\n",
      "batch_idx: 1 loss: 0.016437727353670054 R2: 0.01743797470355357 time: 1703257505.9356022\n",
      "Training [44%] Loss: 0.01414133982794969 time: 1703257505.9356022\n",
      "weight: [ 0.50635048 -0.01702907  0.04291962  0.89855419  0.60642906  0.00919705\n",
      "  0.13952271  0.57091812 -0.25457069  0.16080805  0.54873379  0.6918952\n",
      "  0.27527914  0.12595928  0.61101018  0.23724909  0.3253997   0.74649141\n",
      "  0.69149829  0.87645633  0.56502925  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 134\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011846584515410787 R2: 0.017445947593734745 time: 1703257509.813411\n",
      "batch_idx: 1 loss: 0.016435353786314128 R2: 0.01746050664005716 time: 1703257513.5521061\n",
      "Training [45%] Loss: 0.014140969150862458 time: 1703257513.5521061\n",
      "weight: [ 0.50604259 -0.01732341  0.04248715  0.89855419  0.60642906  0.00919705\n",
      "  0.13929883  0.57072671 -0.25566502  0.16080805  0.54873379  0.6918952\n",
      "  0.27455155  0.12569965  0.61071584  0.23724909  0.3253997   0.74649141\n",
      "  0.69130821  0.87615419  0.56483783  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 135\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011848191603091071 R2: 0.01746835820602277 time: 1703257517.4794717\n",
      "batch_idx: 1 loss: 0.01643301313547591 R2: 0.017482748825915828 time: 1703257521.4569237\n",
      "Training [45%] Loss: 0.014140602369283491 time: 1703257521.4569237\n",
      "weight: [ 0.5057368  -0.01761762  0.04205598  0.89855419  0.60642906  0.00919705\n",
      "  0.13907353  0.57054043 -0.25674697  0.16080805  0.54873379  0.6918952\n",
      "  0.27384433  0.12543994  0.61042163  0.23724909  0.3253997   0.74649141\n",
      "  0.69111642  0.87585175  0.56465155  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 136\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01184977581255102 R2: 0.01749049807924341 time: 1703257525.3021736\n",
      "batch_idx: 1 loss: 0.016430703800658912 R2: 0.017504705911658647 time: 1703257529.1624002\n",
      "Training [45%] Loss: 0.014140239806604966 time: 1703257529.1624002\n",
      "weight: [ 0.50543312 -0.01791173  0.04162608  0.89855419  0.60642906  0.00919705\n",
      "  0.13884688  0.57035928 -0.2578165   0.16080805  0.54873379  0.6918952\n",
      "  0.27315704  0.12518013  0.61012752  0.23724909  0.3253997   0.74649141\n",
      "  0.69092296  0.87554909  0.5644704   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 137\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011851337444977711 R2: 0.017512375366377952 time: 1703257533.2220032\n",
      "batch_idx: 1 loss: 0.016428426118435816 R2: 0.017526392460325724 time: 1703257537.3721814\n",
      "Training [46%] Loss: 0.014139881781706764 time: 1703257537.3721814\n",
      "weight: [ 0.50513152 -0.01820573  0.04119744  0.89855419  0.60642906  0.00919705\n",
      "  0.13861888  0.57018325 -0.25887359  0.16080805  0.54873379  0.6918952\n",
      "  0.27248915  0.12492021  0.60983353  0.23724909  0.3253997   0.74649141\n",
      "  0.69072783  0.8752462   0.56429438  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 138\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011852875374187718 R2: 0.01753399023395452 time: 1703257541.39894\n",
      "batch_idx: 1 loss: 0.016426181044815143 R2: 0.017547826069505668 time: 1703257545.4470823\n",
      "Training [46%] Loss: 0.01413952820950143 time: 1703257545.4470823\n",
      "weight: [ 0.50483199 -0.01849959  0.0407701   0.89855419  0.60642906  0.00919705\n",
      "  0.13838951  0.57001238 -0.25991821  0.16080805  0.54873379  0.6918952\n",
      "  0.27184007  0.12466021  0.60953967  0.23724909  0.3253997   0.74649141\n",
      "  0.69053103  0.87494308  0.56412351  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 139\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011854388759866271 R2: 0.0175553439571261 time: 1703257549.467214\n",
      "batch_idx: 1 loss: 0.016423968519933715 R2: 0.017569018557582528 time: 1703257553.6429174\n",
      "Training [46%] Loss: 0.014139178639899993 time: 1703257553.6429174\n",
      "weight: [ 0.5045345  -0.01879331  0.04034405  0.89855419  0.60642906  0.00919705\n",
      "  0.13815879  0.56984666 -0.26095034  0.16080805  0.54873379  0.6918952\n",
      "  0.27120932  0.12440013  0.60924595  0.23724909  0.3253997   0.74649141\n",
      "  0.69033256  0.87463973  0.56395778  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 140\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0118558781968176 R2: 0.017576445205326308 time: 1703257557.917504\n",
      "batch_idx: 1 loss: 0.016421787057220853 R2: 0.017589973596929154 time: 1703257562.0271645\n",
      "Training [47%] Loss: 0.014138832627019227 time: 1703257562.0271645\n",
      "weight: [ 0.50423905 -0.01908692  0.03991925  0.89855419  0.60642906  0.00919705\n",
      "  0.13792678  0.56968607 -0.26196998  0.16080805  0.54873379  0.6918952\n",
      "  0.2705965   0.12413995  0.60895233  0.23724909  0.3253997   0.74649141\n",
      "  0.69013247  0.87433622  0.56379719  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 141\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011857345347462581 R2: 0.017597308236556697 time: 1703257566.4518828\n",
      "batch_idx: 1 loss: 0.01641963472443194 R2: 0.017610691943084622 time: 1703257571.3627205\n",
      "Training [47%] Loss: 0.014138490035947261 time: 1703257571.3627205\n",
      "weight: [ 0.50394567 -0.01938046  0.03949566  0.89855419  0.60642906  0.00919705\n",
      "  0.13769354  0.56953057 -0.26297712  0.16080805  0.54873379  0.6918952\n",
      "  0.27000129  0.12387961  0.6086588   0.23724909  0.3253997   0.74649141\n",
      "  0.68993083  0.8740326   0.56364169  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 142\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011858791651261228 R2: 0.0176179459797442 time: 1703257576.0921612\n",
      "batch_idx: 1 loss: 0.016417510380665464 R2: 0.01763117821193172 time: 1703257580.7169724\n",
      "Training [47%] Loss: 0.014138151015963347 time: 1703257580.7169724\n",
      "weight: [ 0.50365435 -0.01967394  0.03907327  0.89855419  0.60642906  0.00919705\n",
      "  0.13745913  0.56938014 -0.26397175  0.16080805  0.54873379  0.6918952\n",
      "  0.26942331  0.12361912  0.60836531  0.23724909  0.3253997   0.74649141\n",
      "  0.68972768  0.87372894  0.56349126  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 143\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01186021745330972 R2: 0.01763836521801454 time: 1703257585.5156207\n",
      "batch_idx: 1 loss: 0.016415413987541417 R2: 0.01765144270154706 time: 1703257589.9889617\n",
      "Training [48%] Loss: 0.014137815720425568 time: 1703257589.9889617\n",
      "weight: [ 0.50336507 -0.01996737  0.03865206  0.89855419  0.60642906  0.00919705\n",
      "  0.13722356  0.56923478 -0.26495388  0.16080805  0.54873379  0.6918952\n",
      "  0.26886211  0.12335846  0.60807189  0.23724909  0.3253997   0.74649141\n",
      "  0.68952303  0.87342525  0.5633459   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 144\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011861622283573474 R2: 0.017658568005829323 time: 1703257594.3523605\n",
      "batch_idx: 1 loss: 0.016413345864788757 R2: 0.017671497394771918 time: 1703257598.8572938\n",
      "Training [48%] Loss: 0.014137484074181115 time: 1703257598.8572938\n",
      "weight: [ 0.50307784 -0.02026073  0.03823204  0.89855419  0.60642906  0.00919705\n",
      "  0.13698683  0.56909449 -0.26592348  0.16080805  0.54873379  0.6918952\n",
      "  0.26831722  0.12309765  0.60777852  0.23724909  0.3253997   0.74649141\n",
      "  0.68931689  0.87312152  0.56320561  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 145\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01186300583216249 R2: 0.017678556995302852 time: 1703257603.661914\n",
      "batch_idx: 1 loss: 0.016411305759799 R2: 0.017691350816907647 time: 1703257608.2365994\n",
      "Training [48%] Loss: 0.014137155795980745 time: 1703257608.2365994\n",
      "weight: [ 0.50279263 -0.02055403  0.03781321  0.89855419  0.60642906  0.00919705\n",
      "  0.13674898  0.56895926 -0.26688058  0.16080805  0.54873379  0.6918952\n",
      "  0.26778823  0.12283668  0.60748523  0.23724909  0.3253997   0.74649141\n",
      "  0.68910929  0.87281778  0.56307038  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 146\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011864368596959888 R2: 0.017698339074128988 time: 1703257613.3025634\n",
      "batch_idx: 1 loss: 0.016409292627256558 R2: 0.01771100674348547 time: 1703257618.3792694\n",
      "Training [49%] Loss: 0.014136830612108223 time: 1703257618.3792694\n",
      "weight: [ 0.50250945 -0.02084728  0.03739554  0.89855419  0.60642906  0.00919705\n",
      "  0.13651004  0.56882906 -0.26782517  0.16080805  0.54873379  0.6918952\n",
      "  0.26727481  0.12257553  0.60719198  0.23724909  0.3253997   0.74649141\n",
      "  0.68890025  0.87251407  0.56294019  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 147\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011865711656991778 R2: 0.017717924205024382 time: 1703257623.5565915\n",
      "batch_idx: 1 loss: 0.016407305202156518 R2: 0.01773046734585959 time: 1703257628.1119556\n",
      "Training [49%] Loss: 0.014136508429574148 time: 1703257628.1119556\n",
      "weight: [ 0.50222831 -0.0211405   0.03697901  0.89855419  0.60642906  0.00919705\n",
      "  0.13627006  0.56870387 -0.26875726  0.16080805  0.54873379  0.6918952\n",
      "  0.26677664  0.12231419  0.60689876  0.23724909  0.3253997   0.74649141\n",
      "  0.68868982  0.87221044  0.562815    0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 148\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011867035933287601 R2: 0.017737321352822244 time: 1703257632.6421046\n",
      "batch_idx: 1 loss: 0.01640534269375571 R2: 0.017749737094635787 time: 1703257637.5070024\n",
      "Training [49%] Loss: 0.014136189313521655 time: 1703257637.5070024\n",
      "weight: [ 0.50194922 -0.02143371  0.03656359  0.89855419  0.60642906  0.00919705\n",
      "  0.13602909  0.56858366 -0.26967687  0.16080805  0.54873379  0.6918952\n",
      "  0.26629337  0.12205264  0.60660555  0.23724909  0.3253997   0.74649141\n",
      "  0.68847806  0.87190693  0.56269478  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 149\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011868341717792844 R2: 0.01775653581605241 time: 1703257642.0859768\n",
      "batch_idx: 1 loss: 0.016403404928284702 R2: 0.017768823628131036 time: 1703257646.554549\n",
      "Training [50%] Loss: 0.014135873323038773 time: 1703257646.554549\n",
      "weight: [ 0.50167216 -0.02172691  0.03614929  0.89855419  0.60642906  0.00919705\n",
      "  0.13578715  0.56846842 -0.27058402  0.16080805  0.54873379  0.6918952\n",
      "  0.26582463  0.12179088  0.60631235  0.23724909  0.3253997   0.74649141\n",
      "  0.68826496  0.87160355  0.56257954  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 150\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011869628866852342 R2: 0.01777557025813936 time: 1703257651.0849583\n",
      "batch_idx: 1 loss: 0.016401491900744723 R2: 0.01778773527851496 time: 1703257655.5615046\n",
      "Training [50%] Loss: 0.014135560383798533 time: 1703257655.5615046\n",
      "weight: [ 0.50139714 -0.02202009  0.0357361   0.89855419  0.60642906  0.00919705\n",
      "  0.13554426  0.56835813 -0.27147871  0.16080805  0.54873379  0.6918952\n",
      "  0.26537006  0.12152891  0.60601917  0.23724909  0.3253997   0.74649141\n",
      "  0.68805056  0.87130031  0.56246925  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 151\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011870897362400741 R2: 0.017794427856757666 time: 1703257659.9196436\n",
      "batch_idx: 1 loss: 0.016399603260487196 R2: 0.01780647816080838 time: 1703257664.4934797\n",
      "Training [50%] Loss: 0.01413525031144397 time: 1703257664.4934797\n",
      "weight: [ 0.50112416 -0.02231325  0.03532401  0.89855419  0.60642906  0.00919705\n",
      "  0.13530043  0.56825278 -0.27236098  0.16080805  0.54873379  0.6918952\n",
      "  0.2649293   0.12126673  0.605726    0.23724909  0.3253997   0.74649141\n",
      "  0.68783486  0.87099723  0.5623639   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 152\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011872147647023833 R2: 0.017813114237404837 time: 1703257668.8364332\n",
      "batch_idx: 1 loss: 0.016397738228620902 R2: 0.017825055671059076 time: 1703257673.3750958\n",
      "Training [51%] Loss: 0.014134942937822368 time: 1703257673.3750958\n",
      "weight: [ 0.50085321 -0.02260642  0.03491301  0.89855419  0.60642906  0.00919705\n",
      "  0.13505572  0.56815233 -0.27323084  0.16080805  0.54873379  0.6918952\n",
      "  0.26450206  0.12100432  0.60543284  0.23724909  0.3253997   0.74649141\n",
      "  0.68761792  0.87069436  0.56226345  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 153\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011873380454522062 R2: 0.01783163656240383 time: 1703257678.1395667\n",
      "batch_idx: 1 loss: 0.01639589595049094 R2: 0.017843470477572954 time: 1703257683.1299176\n",
      "Training [51%] Loss: 0.014134638202506501 time: 1703257683.1299176\n",
      "weight: [ 0.50058431 -0.02289959  0.03450308  0.89855419  0.60642906  0.00919705\n",
      "  0.13481016  0.56805676 -0.27408833  0.16080805  0.54873379  0.6918952\n",
      "  0.26408804  0.12074168  0.60513966  0.23724909  0.3253997   0.74649141\n",
      "  0.68739976  0.87039171  0.56216788  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 154\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011874596384682901 R2: 0.01785000112650703 time: 1703257688.0364475\n",
      "batch_idx: 1 loss: 0.0163940758718079 R2: 0.017861726691411395 time: 1703257693.0076869\n",
      "Training [51%] Loss: 0.014134336128245401 time: 1703257693.0076869\n",
      "weight: [ 0.50031746 -0.02319279  0.0340942   0.89855419  0.60642906  0.00919705\n",
      "  0.13456378  0.56796604 -0.27493349  0.16080805  0.54873379  0.6918952\n",
      "  0.26368696  0.12047879  0.60484647  0.23724909  0.3253997   0.74649141\n",
      "  0.68718043  0.87008933  0.56207716  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 155\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011875795672270005 R2: 0.017868212016122542 time: 1703257697.9403627\n",
      "batch_idx: 1 loss: 0.016392277774824673 R2: 0.017879830113475825 time: 1703257702.6263297\n",
      "Training [52%] Loss: 0.01413403672354734 time: 1703257702.6263297\n",
      "weight: [ 0.50005266 -0.023486    0.03368637  0.89855419  0.60642906  0.00919705\n",
      "  0.13431661  0.56788015 -0.27576634  0.16080805  0.54873379  0.6918952\n",
      "  0.2632985   0.12021566  0.60455326  0.23724909  0.3253997   0.74649141\n",
      "  0.68695993  0.86978722  0.56199127  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 156\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011876978337479088 R2: 0.017886271947573307 time: 1703257707.124045\n",
      "batch_idx: 1 loss: 0.01639050149870175 R2: 0.0178977866510055 time: 1703257711.5602636\n",
      "Training [52%] Loss: 0.014133739918090419 time: 1703257711.5602636\n",
      "weight: [ 0.49978992 -0.02377922  0.03327958  0.89855419  0.60642906  0.00919705\n",
      "  0.13406866  0.56779907 -0.27658692  0.16080805  0.54873379  0.6918952\n",
      "  0.26292234  0.11995228  0.60426004  0.23724909  0.3253997   0.74649141\n",
      "  0.6867383   0.86948539  0.56191019  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 157\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011878144507357472 R2: 0.01790418411776251 time: 1703257716.0012012\n",
      "batch_idx: 1 loss: 0.016388746667191798 R2: 0.017915600742862803 time: 1703257720.463654\n",
      "Training [52%] Loss: 0.014133445587274635 time: 1703257720.463654\n",
      "weight: [ 0.49952923 -0.02407247  0.03287383  0.89855419  0.60642906  0.00919705\n",
      "  0.13381998  0.56772276 -0.27739529  0.16080805  0.54873379  0.6918952\n",
      "  0.26255822  0.11968866  0.60396679  0.23724909  0.3253997   0.74649141\n",
      "  0.68651555  0.86918387  0.56183388  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 158\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011879294569572652 R2: 0.017921953117356337 time: 1703257724.9226854\n",
      "batch_idx: 1 loss: 0.01638701268412887 R2: 0.01793327531913369 time: 1703257729.4624171\n",
      "Training [53%] Loss: 0.014133153626850761 time: 1703257729.4624171\n",
      "weight: [ 0.49927061 -0.02436574  0.03246911  0.89855419  0.60642906  0.00919705\n",
      "  0.13357058  0.5676512  -0.27819147  0.16080805  0.54873379  0.6918952\n",
      "  0.26220586  0.11942478  0.60367352  0.23724909  0.3253997   0.74649141\n",
      "  0.68629173  0.86888269  0.56176232  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 159\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011880429039779993 R2: 0.01793958418795294 time: 1703257733.830794\n",
      "batch_idx: 1 loss: 0.01638529895391626 R2: 0.01795081308057256 time: 1703257738.3925345\n",
      "Training [53%] Loss: 0.014132863996848127 time: 1703257738.3925345\n",
      "weight: [ 0.49901406 -0.02465904  0.03206539  0.89855419  0.60642906  0.00919705\n",
      "  0.1333205   0.56758435 -0.27897553  0.16080805  0.54873379  0.6918952\n",
      "  0.26186502  0.11916064  0.60338022  0.23724909  0.3253997   0.74649141\n",
      "  0.68606686  0.86858186  0.56169548  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 160\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011881548321034639 R2: 0.01795708182914868 time: 1703257742.781736\n",
      "batch_idx: 1 loss: 0.01638360507395143 R2: 0.0179682176375352 time: 1703257747.1678944\n",
      "Training [53%] Loss: 0.014132576697493035 time: 1703257747.1678944\n",
      "weight: [ 0.49875958 -0.02495238  0.03166268  0.89855419  0.60642906  0.00919705\n",
      "  0.13306978  0.56752219 -0.27974752  0.16080805  0.54873379  0.6918952\n",
      "  0.26153542  0.11889624  0.60308687  0.23724909  0.3253997   0.74649141\n",
      "  0.68584096  0.8682814   0.56163331  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 161\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011882652607418775 R2: 0.017974449225021782 time: 1703257751.6331265\n",
      "batch_idx: 1 loss: 0.016381930816904446 R2: 0.017985493429519495 time: 1703257756.494255\n",
      "Training [54%] Loss: 0.014132291712161611 time: 1703257756.494255\n",
      "weight: [ 0.4985072  -0.02524576  0.03126096  0.89855419  0.60642906  0.00919705\n",
      "  0.13281843  0.56746468 -0.28050749  0.16080805  0.54873379  0.6918952\n",
      "  0.26121682  0.11863156  0.6027935   0.23724909  0.3253997   0.74649141\n",
      "  0.68561408  0.86798134  0.5615758   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 162\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011883742000365172 R2: 0.017991688915133364 time: 1703257761.4594724\n",
      "batch_idx: 1 loss: 0.0163802759566251 R2: 0.018002644716529126 time: 1703257766.4445922\n",
      "Training [54%] Loss: 0.014132008978495135 time: 1703257766.4445922\n",
      "weight: [ 0.4982569  -0.02553917  0.03086023  0.89855419  0.60642906  0.00919705\n",
      "  0.13256648  0.56741179 -0.2812555   0.16080805  0.54873379  0.6918952\n",
      "  0.26090895  0.11836662  0.60250009  0.23724909  0.3253997   0.74649141\n",
      "  0.68538621  0.86768167  0.56152291  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 163\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011884816687249772 R2: 0.018008803847834765 time: 1703257771.3746903\n",
      "batch_idx: 1 loss: 0.016378640134533724 R2: 0.0180196747915502 time: 1703257776.415583\n",
      "Training [54%] Loss: 0.014131728410891747 time: 1703257776.415583\n",
      "weight: [ 0.49800869 -0.02583262  0.03046048  0.89855419  0.60642906  0.00919705\n",
      "  0.13231395  0.56736348 -0.28199162  0.16080805  0.54873379  0.6918952\n",
      "  0.26061158  0.11810142  0.60220664  0.23724909  0.3253997   0.74649141\n",
      "  0.6851574   0.86738243  0.56147461  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 164\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01188587699718815 R2: 0.01802579772437163 time: 1703257780.8017876\n",
      "batch_idx: 1 loss: 0.016377022889597283 R2: 0.018036586153064915 time: 1703257785.220865\n",
      "Training [55%] Loss: 0.014131449943392715 time: 1703257785.220865\n",
      "weight: [ 0.49776259 -0.02612611  0.0300617   0.89855419  0.60642906  0.00919705\n",
      "  0.13206088  0.56731973 -0.28271591  0.16080805  0.54873379  0.6918952\n",
      "  0.26032448  0.11783594  0.60191315  0.23724909  0.3253997   0.74649141\n",
      "  0.68492768  0.86708361  0.56143086  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 165\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011886923301259119 R2: 0.018042674422152682 time: 1703257789.5475192\n",
      "batch_idx: 1 loss: 0.016375423793388903 R2: 0.018053381308433702 time: 1703257794.1085045\n",
      "Training [55%] Loss: 0.014131173547324011 time: 1703257794.1085045\n",
      "weight: [ 0.49751861 -0.02641965  0.02966389  0.89855419  0.60642906  0.00919705\n",
      "  0.13180729  0.5672805  -0.28342845  0.16080805  0.54873379  0.6918952\n",
      "  0.26004742  0.11757018  0.60161961  0.23724909  0.3253997   0.74649141\n",
      "  0.68469706  0.86678525  0.56139162  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 166\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0118879558826055 R2: 0.018059437226509045 time: 1703257798.59057\n",
      "batch_idx: 1 loss: 0.016373842538797142 R2: 0.01807006331341643 time: 1703257802.9793868\n",
      "Training [55%] Loss: 0.01413089921070132 time: 1703257802.9793868\n",
      "weight: [ 0.49727674 -0.02671323  0.02926703  0.89855419  0.60642906  0.00919705\n",
      "  0.1315532   0.56724574 -0.28412931  0.16080805  0.54873379  0.6918952\n",
      "  0.25978019  0.11730415  0.60132603  0.23724909  0.3253997   0.74649141\n",
      "  0.68446558  0.86648736  0.56135686  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 167\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011888974909569517 R2: 0.018076088665406442 time: 1703257807.4620564\n",
      "batch_idx: 1 loss: 0.016372278902988906 R2: 0.018086635565164655 time: 1703257812.0582623\n",
      "Training [56%] Loss: 0.014130626906279212 time: 1703257812.0582623\n",
      "weight: [ 0.497037   -0.02700685  0.02887112  0.89855419  0.60642906  0.00919705\n",
      "  0.13129864  0.56721542 -0.28481856  0.16080805  0.54873379  0.6918952\n",
      "  0.25952257  0.11703785  0.6010324   0.23724909  0.3253997   0.74649141\n",
      "  0.68423326  0.86618995  0.56132654  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 168\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011889980520284376 R2: 0.018092631011043747 time: 1703257816.6768599\n",
      "batch_idx: 1 loss: 0.016370732643092255 R2: 0.018103101185955706 time: 1703257821.6490679\n",
      "Training [56%] Loss: 0.014130356581688316 time: 1703257821.6490679\n",
      "weight: [ 0.4967994  -0.02730052  0.02847616  0.89855419  0.60642906  0.00919705\n",
      "  0.13104363  0.5671895  -0.28549629  0.16080805  0.54873379  0.6918952\n",
      "  0.25927433  0.11677126  0.60073874  0.23724909  0.3253997   0.74649141\n",
      "  0.68400012  0.86589302  0.56130063  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 169\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011890972915666691 R2: 0.018109066840982013 time: 1703257826.785752\n",
      "batch_idx: 1 loss: 0.016369203439311454 R2: 0.01811946268283493 time: 1703257831.6826708\n",
      "Training [56%] Loss: 0.014130088177489073 time: 1703257831.6826708\n",
      "weight: [ 0.49656395 -0.02759423  0.02808213  0.89855419  0.60642906  0.00919705\n",
      "  0.1307882   0.56716796 -0.28616258  0.16080805  0.54873379  0.6918952\n",
      "  0.25903527  0.1165044   0.60044502  0.23724909  0.3253997   0.74649141\n",
      "  0.68376619  0.86559659  0.56127908  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 170\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011891952367501846 R2: 0.01812539909581712 time: 1703257836.7379599\n",
      "batch_idx: 1 loss: 0.016367690933113486 R2: 0.018135722177334168 time: 1703257841.6430519\n",
      "Training [57%] Loss: 0.014129821650307666 time: 1703257841.6430519\n",
      "weight: [ 0.49633065 -0.02788799  0.02768903  0.89855419  0.60642906  0.00919705\n",
      "  0.13053237  0.56715073 -0.28681752  0.16080805  0.54873379  0.6918952\n",
      "  0.25880521  0.11623727  0.60015126  0.23724909  0.3253997   0.74649141\n",
      "  0.68353149  0.86530067  0.56126185  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 171\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011892919148844754 R2: 0.0181416306668285 time: 1703257846.4609299\n",
      "batch_idx: 1 loss: 0.016366194805436243 R2: 0.018151881881743326 time: 1703257851.1539142\n",
      "Training [57%] Loss: 0.014129556977140498 time: 1703257851.1539142\n",
      "weight: [ 0.49609951 -0.0281818   0.02729685  0.89855419  0.60642906  0.00919705\n",
      "  0.13027616  0.56713779 -0.2874612   0.16080805  0.54873379  0.6918952\n",
      "  0.25858394  0.11596985  0.59985746  0.23724909  0.3253997   0.74649141\n",
      "  0.68329606  0.86500528  0.56124891  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 172\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01189387346984615 R2: 0.018157764009294808 time: 1703257855.6248648\n",
      "batch_idx: 1 loss: 0.01636471480976394 R2: 0.01816794430789148 time: 1703257860.3277857\n",
      "Training [57%] Loss: 0.014129294139805045 time: 1703257860.3277857\n",
      "weight: [ 0.49587056 -0.02847565  0.02690559  0.89855419  0.60642906  0.00919705\n",
      "  0.1300196   0.56712908 -0.28809371  0.16080805  0.54873379  0.6918952\n",
      "  0.25837128  0.11570216  0.59956361  0.23724909  0.3253997   0.74649141\n",
      "  0.68305991  0.86471043  0.56124021  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 173\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011894815482084618 R2: 0.018173801166468273 time: 1703257864.822169\n",
      "batch_idx: 1 loss: 0.016363250734881818 R2: 0.018183912049759 time: 1703257869.2248712\n",
      "Training [58%] Loss: 0.014129033108483218 time: 1703257869.2248712\n",
      "weight: [ 0.49564378 -0.02876954  0.02651524  0.89855419  0.60642906  0.00919705\n",
      "  0.1297627   0.56712458 -0.28871516  0.16080805  0.54873379  0.6918952\n",
      "  0.25816704  0.11543419  0.59926971  0.23724909  0.3253997   0.74649141\n",
      "  0.68282306  0.86441611  0.5612357   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 174\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0118957453350482 R2: 0.018189744112303896 time: 1703257873.833907\n",
      "batch_idx: 1 loss: 0.016361802346768404 R2: 0.018199787434873715 time: 1703257878.3023324\n",
      "Training [58%] Loss: 0.014128773840908302 time: 1703257878.3023324\n",
      "weight: [ 0.4954192  -0.02906348  0.02612579  0.89855419  0.60642906  0.00919705\n",
      "  0.12950549  0.56712423 -0.28932564  0.16080805  0.54873379  0.6918952\n",
      "  0.25797103  0.11516595  0.59897578  0.23724909  0.3253997   0.74649141\n",
      "  0.68258554  0.86412234  0.56123535  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 175\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011896663219185614 R2: 0.018205595017352305 time: 1703257882.518695\n",
      "batch_idx: 1 loss: 0.016360369371223208 R2: 0.01821557242010785 time: 1703257887.3924847\n",
      "Training [58%] Loss: 0.01412851629520441 time: 1703257887.3924847\n",
      "weight: [ 0.49519682 -0.02935746  0.02573725  0.89855419  0.60642906  0.00919705\n",
      "  0.129248    0.56712799 -0.28992525  0.16080805  0.54873379  0.6918952\n",
      "  0.25778308  0.11489743  0.59868179  0.23724909  0.3253997   0.74649141\n",
      "  0.68234738  0.86382913  0.56123911  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 176\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011897569354976989 R2: 0.018221356187461013 time: 1703257892.3911567\n",
      "batch_idx: 1 loss: 0.016358951527124698 R2: 0.01823126879700321 time: 1703257897.3753645\n",
      "Training [59%] Loss: 0.014128260441050844 time: 1703257897.3753645\n",
      "weight: [ 0.49497666 -0.02965148  0.02534959  0.89855419  0.60642906  0.00919705\n",
      "  0.12899023  0.56713581 -0.29051411  0.16080805  0.54873379  0.6918952\n",
      "  0.25760302  0.11462864  0.59838777  0.23724909  0.3253997   0.74649141\n",
      "  0.68210859  0.86353649  0.56124694  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 177\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011898463948788093 R2: 0.018237029797062876 time: 1703257902.249088\n",
      "batch_idx: 1 loss: 0.016357548567638873 R2: 0.018246878448849313 time: 1703257907.343589\n",
      "Training [59%] Loss: 0.014128006258213484 time: 1703257907.343589\n",
      "weight: [ 0.49475872 -0.02994555  0.02496282  0.89855419  0.60642906  0.00919705\n",
      "  0.12873222  0.56714766 -0.29109232  0.16080805  0.54873379  0.6918952\n",
      "  0.25743069  0.11435958  0.59809371  0.23724909  0.3253997   0.74649141\n",
      "  0.68186921  0.86324442  0.56125878  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 178\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011899347165967755 R2: 0.018252617724542453 time: 1703257912.3120368\n",
      "batch_idx: 1 loss: 0.016356160286274977 R2: 0.018262403394125215 time: 1703257917.2509277\n",
      "Training [59%] Loss: 0.014127753726121366 time: 1703257917.2509277\n",
      "weight: [ 0.49454302 -0.03023965  0.02457693  0.89855419  0.60642906  0.00919705\n",
      "  0.12847398  0.56716348 -0.29166     0.16080805  0.54873379  0.6918952\n",
      "  0.25726592  0.11409025  0.5977996   0.23724909  0.3253997   0.74649141\n",
      "  0.68162925  0.86295293  0.5612746   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 179\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011900219145335235 R2: 0.018268121640802826 time: 1703257922.0517426\n",
      "batch_idx: 1 loss: 0.01635478648776032 R2: 0.01827784561263357 time: 1703257927.0129707\n",
      "Training [60%] Loss: 0.014127502816547777 time: 1703257927.0129707\n",
      "weight: [ 0.49432956 -0.03053379  0.02419192  0.89855419  0.60642906  0.00919705\n",
      "  0.12821553  0.56718322 -0.29221726  0.16080805  0.54873379  0.6918952\n",
      "  0.25710855  0.11382064  0.59750546  0.23724909  0.3253997   0.74649141\n",
      "  0.68138873  0.86266203  0.56129435  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 180\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011901080033173828 R2: 0.01828354321980874 time: 1703257931.452195\n",
      "batch_idx: 1 loss: 0.016353426959226543 R2: 0.018293206871034973 time: 1703257936.002752\n",
      "Training [60%] Loss: 0.014127253496200184 time: 1703257936.002752\n",
      "weight: [ 0.49411836 -0.03082797  0.02380778  0.89855419  0.60642906  0.00919705\n",
      "  0.12795689  0.56720685 -0.29276423  0.16080805  0.54873379  0.6918952\n",
      "  0.25695843  0.11355077  0.59721128  0.23724909  0.3253997   0.74649141\n",
      "  0.68114767  0.86237171  0.56131797  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 181\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011901929999030322 R2: 0.01829888423913817 time: 1703257940.834099\n",
      "batch_idx: 1 loss: 0.016352081470576096 R2: 0.018308488727268868 time: 1703257945.6154888\n",
      "Training [60%] Loss: 0.01412700573480321 time: 1703257945.6154888\n",
      "weight: [ 0.49390942 -0.03112218  0.0234245   0.89855419  0.60642906  0.00919705\n",
      "  0.12769808  0.5672343  -0.29330102  0.16080805  0.54873379  0.6918952\n",
      "  0.2568154   0.11328064  0.59691707  0.23724909  0.3253997   0.74649141\n",
      "  0.68090611  0.86208199  0.56134542  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 182\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01190276922073996 R2: 0.01831414649043317 time: 1703257950.4591062\n",
      "batch_idx: 1 loss: 0.016350749798208494 R2: 0.018323692680381143 time: 1703257955.3636413\n",
      "Training [61%] Loss: 0.014126759509474226 time: 1703257955.3636413\n",
      "weight: [ 0.49370277 -0.03141643  0.02304209  0.89855419  0.60642906  0.00919705\n",
      "  0.12743912  0.56726553 -0.29382776  0.16080805  0.54873379  0.6918952\n",
      "  0.25667931  0.11301024  0.59662283  0.23724909  0.3253997   0.74649141\n",
      "  0.68066406  0.86179287  0.56137666  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 183\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011903597859528419 R2: 0.018329331626668388 time: 1703257960.2703068\n",
      "batch_idx: 1 loss: 0.016349431743633746 R2: 0.018338820290191515 time: 1703257965.0697021\n",
      "Training [61%] Loss: 0.014126514801581082 time: 1703257965.0697021\n",
      "weight: [ 0.4934984  -0.03171071  0.02266052  0.89855419  0.60642906  0.00919705\n",
      "  0.12718003  0.5673005  -0.29434459  0.16080805  0.54873379  0.6918952\n",
      "  0.25655003  0.11273958  0.59632855  0.23724909  0.3253997   0.74649141\n",
      "  0.68042153  0.86150436  0.56141162  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 184\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011904416052391504 R2: 0.018344441115167087 time: 1703257969.9410026\n",
      "batch_idx: 1 loss: 0.01634812712878534 R2: 0.018353873152614053 time: 1703257974.7197015\n",
      "Training [61%] Loss: 0.014126271590588421 time: 1703257974.7197015\n",
      "weight: [ 0.49329634 -0.03200501  0.02227981  0.89855419  0.60642906  0.00919705\n",
      "  0.12692082  0.56733915 -0.29485162  0.16080805  0.54873379  0.6918952\n",
      "  0.25642741  0.11246867  0.59603424  0.23724909  0.3253997   0.74649141\n",
      "  0.68017856  0.86121645  0.56145027  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 185\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011905223926521465 R2: 0.01835947632786017 time: 1703257979.2094696\n",
      "batch_idx: 1 loss: 0.016346835776813694 R2: 0.018368852783563927 time: 1703257983.7938995\n",
      "Training [62%] Loss: 0.014126029851667578 time: 1703257983.7938995\n",
      "weight: [ 0.49309658 -0.03229935  0.02189994  0.89855419  0.60642906  0.00919705\n",
      "  0.12666151  0.56738143 -0.29534899  0.16080805  0.54873379  0.6918952\n",
      "  0.25631132  0.1121975   0.59573991  0.23724909  0.3253997   0.74649141\n",
      "  0.67993516  0.86092915  0.56149255  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 186\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011906021617239453 R2: 0.01837443865462396 time: 1703257988.6028156\n",
      "batch_idx: 1 loss: 0.016345557500373933 R2: 0.018383760548382533 time: 1703257993.7459018\n",
      "Training [62%] Loss: 0.014125789558806693 time: 1703257993.7459018\n",
      "weight: [ 0.49289914 -0.03259371  0.02152091  0.89855419  0.60642906  0.00919705\n",
      "  0.12640212  0.56742728 -0.29583684  0.16080805  0.54873379  0.6918952\n",
      "  0.25620161  0.11192608  0.59544555  0.23724909  0.3253997   0.74649141\n",
      "  0.67969136  0.86064247  0.56153841  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 187\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011906809270635366 R2: 0.018389329521899644 time: 1703257998.4608743\n",
      "batch_idx: 1 loss: 0.01634429210783212 R2: 0.018398597703010888 time: 1703258003.0533156\n",
      "Training [62%] Loss: 0.014125550689233744 time: 1703258003.0533156\n",
      "weight: [ 0.49270404 -0.03288809  0.02114272  0.89855419  0.60642906  0.00919705\n",
      "  0.12614266  0.56747667 -0.2963153   0.16080805  0.54873379  0.6918952\n",
      "  0.25609816  0.1116544   0.59515117  0.23724909  0.3253997   0.74649141\n",
      "  0.67944717  0.8603564   0.56158779  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 188\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011907587031104557 R2: 0.018404150316431056 time: 1703258007.600259\n",
      "batch_idx: 1 loss: 0.016343039417632897 R2: 0.01841336548693895 time: 1703258012.614687\n",
      "Training [63%] Loss: 0.014125313224368727 time: 1703258012.614687\n",
      "weight: [ 0.49251128 -0.03318249  0.02076535  0.89855419  0.60642906  0.00919705\n",
      "  0.12588316  0.56752953 -0.29678452  0.16080805  0.54873379  0.6918952\n",
      "  0.25600085  0.11138248  0.59485677  0.23724909  0.3253997   0.74649141\n",
      "  0.67920261  0.86007095  0.56164066  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 189\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011908355029384522 R2: 0.01841890231115073 time: 1703258017.6475806\n",
      "batch_idx: 1 loss: 0.01634179926447337 R2: 0.018428065165357443 time: 1703258022.4440546\n",
      "Training [63%] Loss: 0.014125077146928946 time: 1703258022.4440546\n",
      "weight: [ 0.49232088 -0.03347691  0.02038881  0.89855419  0.60642906  0.00919705\n",
      "  0.12562362  0.56758582 -0.29724464  0.16080805  0.54873379  0.6918952\n",
      "  0.25590953  0.11111032  0.59456235  0.23724909  0.3253997   0.74649141\n",
      "  0.6789577   0.85978613  0.56169694  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 190\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011909113383375853 R2: 0.018433586670860147 time: 1703258027.206492\n",
      "batch_idx: 1 loss: 0.016340571492273283 R2: 0.018442697988166223 time: 1703258032.2036357\n",
      "Training [63%] Loss: 0.014124842437824567 time: 1703258032.2036357\n",
      "weight: [ 0.49213284 -0.03377134  0.02001309  0.89855419  0.60642906  0.00919705\n",
      "  0.12536406  0.56764548 -0.2976958   0.16080805  0.54873379  0.6918952\n",
      "  0.2558241   0.11083792  0.59426791  0.23724909  0.3253997   0.74649141\n",
      "  0.67871247  0.85950192  0.5617566   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 191\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011909862208704146 R2: 0.018448204519545763 time: 1703258036.8449845\n",
      "batch_idx: 1 loss: 0.01633935594339225 R2: 0.01845726512467616 time: 1703258041.3446426\n",
      "Training [64%] Loss: 0.014124609076048199 time: 1703258041.3446426\n",
      "weight: [ 0.49194717 -0.03406579  0.01963818  0.89855419  0.60642906  0.00919705\n",
      "  0.12510451  0.56770845 -0.29813815  0.16080805  0.54873379  0.6918952\n",
      "  0.25574444  0.11056527  0.59397347  0.23724909  0.3253997   0.74649141\n",
      "  0.67846692  0.85921834  0.56181957  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 192\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011910601626518287 R2: 0.01846275699088573 time: 1703258045.7307858\n",
      "batch_idx: 1 loss: 0.016338152455705447 R2: 0.018471767647365467 time: 1703258050.260331\n",
      "Training [64%] Loss: 0.014124377041111868 time: 1703258050.260331\n",
      "weight: [ 0.49176389 -0.03436025  0.01926408  0.89855419  0.60642906  0.00919705\n",
      "  0.12484496  0.56777469 -0.29857184  0.16080805  0.54873379  0.6918952\n",
      "  0.25567042  0.1102924   0.59367901  0.23724909  0.3253997   0.74649141\n",
      "  0.67822109  0.85893538  0.56188581  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 193\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01191133176111421 R2: 0.01847724521465688 time: 1703258054.7754922\n",
      "batch_idx: 1 loss: 0.016336960868970372 R2: 0.018486206574428232 time: 1703258059.2903335\n",
      "Training [64%] Loss: 0.014124146315042291 time: 1703258059.2903335\n",
      "weight: [ 0.491583   -0.03465472  0.01889079  0.89855419  0.60642906  0.00919705\n",
      "  0.12458544  0.56784414 -0.29899703  0.16080805  0.54873379  0.6918952\n",
      "  0.25560194  0.11001929  0.59338454  0.23724909  0.3253997   0.74649141\n",
      "  0.67797498  0.85865304  0.56195526  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 194\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011912052731787158 R2: 0.018491670266258198 time: 1703258063.6618996\n",
      "batch_idx: 1 loss: 0.01633578103208766 R2: 0.018500582918337893 time: 1703258068.241628\n",
      "Training [65%] Loss: 0.014123916881937409 time: 1703258068.241628\n",
      "weight: [ 0.49140453 -0.03494919  0.0185183   0.89855419  0.60642906  0.00919705\n",
      "  0.12432596  0.56791674 -0.29941385  0.16080805  0.54873379  0.6918952\n",
      "  0.25553887  0.10974596  0.59309007  0.23724909  0.3253997   0.74649141\n",
      "  0.67772862  0.85837134  0.56202787  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 195\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01191276464846972 R2: 0.0185060331397342 time: 1703258072.9911883\n",
      "batch_idx: 1 loss: 0.016334612803457924 R2: 0.01851489769077197 time: 1703258077.907593\n",
      "Training [65%] Loss: 0.014123688725963823 time: 1703258077.907593\n",
      "weight: [ 0.49122846 -0.03524367  0.0181466   0.89855419  0.60642906  0.00919705\n",
      "  0.12406654  0.56799245 -0.29982247  0.16080805  0.54873379  0.6918952\n",
      "  0.25548112  0.1094724   0.59279559  0.23724909  0.3253997   0.74649141\n",
      "  0.67748203  0.85809025  0.56210358  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 196\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011913467615072995 R2: 0.01852033476962034 time: 1703258082.9079049\n",
      "batch_idx: 1 loss: 0.01633345604518425 R2: 0.018529151868208582 time: 1703258087.8906338\n",
      "Training [65%] Loss: 0.014123461830128622 time: 1703258087.8906338\n",
      "weight: [ 0.49105483 -0.03553814  0.0177757   0.89855419  0.60642906  0.00919705\n",
      "  0.12380718  0.56807121 -0.30022304  0.16080805  0.54873379  0.6918952\n",
      "  0.25542858  0.10919862  0.59250111  0.23724909  0.3253997   0.74649141\n",
      "  0.67723521  0.8578098   0.56218233  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 197\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011914161735773556 R2: 0.018534576071768893 time: 1703258092.7031567\n",
      "batch_idx: 1 loss: 0.016332310618104674 R2: 0.01854334636241728 time: 1703258097.7966251\n",
      "Training [66%] Loss: 0.014123236176939115 time: 1703258097.7966251\n",
      "weight: [ 0.49088363 -0.03583262  0.01740558  0.89855419  0.60642906  0.00919705\n",
      "  0.12354791  0.56815296 -0.30061572  0.16080805  0.54873379  0.6918952\n",
      "  0.25538113  0.10892463  0.59220664  0.23724909  0.3253997   0.74649141\n",
      "  0.67698819  0.85752996  0.56226408  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 198\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011914847117298651 R2: 0.018548757958923412 time: 1703258102.6395583\n",
      "batch_idx: 1 loss: 0.01633117638251782 R2: 0.018557482027057315 time: 1703258107.2646399\n",
      "Training [66%] Loss: 0.014123011749908234 time: 1703258107.2646399\n",
      "weight: [ 0.49071487 -0.03612709  0.01703624  0.89855419  0.60642906  0.00919705\n",
      "  0.12328873  0.56823765 -0.30100067  0.16080805  0.54873379  0.6918952\n",
      "  0.25533869  0.10865042  0.59191217  0.23724909  0.3253997   0.74649141\n",
      "  0.67674099  0.85725075  0.56234877  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 199\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011915523865665757 R2: 0.01856288132089079 time: 1703258111.610519\n",
      "batch_idx: 1 loss: 0.016330053202683804 R2: 0.01857155968845614 time: 1703258116.116073\n",
      "Training [66%] Loss: 0.014122788534174781 time: 1703258116.116073\n",
      "weight: [ 0.49054857 -0.03642155  0.01666767  0.89855419  0.60642906  0.00919705\n",
      "  0.12302965  0.56832522 -0.30137804  0.16080805  0.54873379  0.6918952\n",
      "  0.25530115  0.108376    0.59161771  0.23724909  0.3253997   0.74649141\n",
      "  0.67649362  0.85697217  0.56243635  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 200\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011916192081863923 R2: 0.018576946997672095 time: 1703258120.755837\n",
      "batch_idx: 1 loss: 0.01632894094961545 R2: 0.01858558016560863 time: 1703258125.1496112\n",
      "Training [67%] Loss: 0.014122566515739687 time: 1703258125.1496112\n",
      "weight: [ 0.49038473 -0.036716    0.01629988  0.89855419  0.60642906  0.00919705\n",
      "  0.1227707   0.56841563 -0.301748    0.16080805  0.54873379  0.6918952\n",
      "  0.25526841  0.10810137  0.59132325  0.23724909  0.3253997   0.74649141\n",
      "  0.6762461   0.85669421  0.56252675  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 201\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011916851861245002 R2: 0.01859095577615788 time: 1703258129.7525022\n",
      "batch_idx: 1 loss: 0.01632783949945247 R2: 0.018599544261852508 time: 1703258134.7053661\n",
      "Training [67%] Loss: 0.014122345680348735 time: 1703258134.7053661\n",
      "weight: [ 0.49022337 -0.03701044  0.01593286  0.89855419  0.60642906  0.00919705\n",
      "  0.12251187  0.56850881 -0.30211072  0.16080805  0.54873379  0.6918952\n",
      "  0.25524038  0.10782654  0.59102881  0.23724909  0.3253997   0.74649141\n",
      "  0.67599844  0.85641687  0.56261993  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 202\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011917503296667414 R2: 0.018604908410877474 time: 1703258139.6116083\n",
      "batch_idx: 1 loss: 0.016326748729801208 R2: 0.01861345274330739 time: 1703258144.8197389\n",
      "Training [67%] Loss: 0.014122126013234312 time: 1703258144.8197389\n",
      "weight: [ 0.49006448 -0.03730487  0.01556659  0.89855419  0.60642906  0.00919705\n",
      "  0.12225319  0.56860471 -0.30246635  0.16080805  0.54873379  0.6918952\n",
      "  0.25521696  0.10755152  0.59073439  0.23724909  0.3253997   0.74649141\n",
      "  0.67575067  0.85614016  0.56271583  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 203\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011918146481518677 R2: 0.01861880564412488 time: 1703258149.8450892\n",
      "batch_idx: 1 loss: 0.01632566851811134 R2: 0.0186273063302016 time: 1703258154.8707747\n",
      "Training [68%] Loss: 0.014121907499815008 time: 1703258154.8707747\n",
      "weight: [ 0.48990809 -0.03759927  0.01520109  0.89855419  0.60642906  0.00919705\n",
      "  0.12199466  0.56870328 -0.30281505  0.16080805  0.54873379  0.6918952\n",
      "  0.25519806  0.1072763   0.59043998  0.23724909  0.3253997   0.74649141\n",
      "  0.67550279  0.85586406  0.5628144   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 204\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011918781509572917 R2: 0.01863264820579802 time: 1703258159.6714594\n",
      "batch_idx: 1 loss: 0.01632459874329891 R2: 0.018641105709019223 time: 1703258164.390434\n",
      "Training [68%] Loss: 0.014121690126435913 time: 1703258164.390434\n",
      "weight: [ 0.48975419 -0.03789366  0.01483633  0.89855419  0.60642906  0.00919705\n",
      "  0.12173629  0.56880445 -0.303157    0.16080805  0.54873379  0.6918952\n",
      "  0.2551836   0.10700088  0.59014559  0.23724909  0.3253997   0.74649141\n",
      "  0.67525482  0.85558858  0.56291558  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 205\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011919408472492404 R2: 0.01864643679799316 time: 1703258168.809434\n",
      "batch_idx: 1 loss: 0.016323539288221466 R2: 0.01865485155017188 time: 1703258173.2613237\n",
      "Training [68%] Loss: 0.014121473880356936 time: 1703258173.2613237\n",
      "weight: [ 0.4896028  -0.03818803  0.01447233  0.89855419  0.60642906  0.00919705\n",
      "  0.1214781   0.56890819 -0.30349235  0.16080805  0.54873379  0.6918952\n",
      "  0.25517348  0.10672529  0.58985123  0.23724909  0.3253997   0.74649141\n",
      "  0.67500677  0.85531373  0.56301931  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 206\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011920027458091481 R2: 0.018660172084371096 time: 1703258177.9217165\n",
      "batch_idx: 1 loss: 0.01632249004016044 R2: 0.01866854451285227 time: 1703258182.4141073\n",
      "Training [69%] Loss: 0.01412125874912596 time: 1703258182.4141073\n",
      "weight: [ 0.48945392 -0.03848237  0.01410906  0.89855419  0.60642906  0.00919705\n",
      "  0.12122009  0.56901443 -0.30382127  0.16080805  0.54873379  0.6918952\n",
      "  0.25516763  0.1064495   0.58955689  0.23724909  0.3253997   0.74649141\n",
      "  0.67475867  0.85503948  0.56312555  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 207\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011920638551113404 R2: 0.018673854695667735 time: 1703258186.6284003\n",
      "batch_idx: 1 loss: 0.016321450889072955 R2: 0.01868218523539189 time: 1703258191.1759238\n",
      "Training [69%] Loss: 0.014121044720093179 time: 1703258191.1759238\n",
      "weight: [ 0.48930756 -0.03877668  0.01374654  0.89855419  0.60642906  0.00919705\n",
      "  0.12096228  0.56912312 -0.30414394  0.16080805  0.54873379  0.6918952\n",
      "  0.25516595  0.10617354  0.58926258  0.23724909  0.3253997   0.74649141\n",
      "  0.67451053  0.85476585  0.56323424  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 208\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011921241835320196 R2: 0.018687485243729762 time: 1703258196.461646\n",
      "batch_idx: 1 loss: 0.016320421725754493 R2: 0.018695774324925307 time: 1703258201.6171312\n",
      "Training [69%] Loss: 0.014120831780537343 time: 1703258201.6171312\n",
      "weight: [ 0.48916373 -0.03907096  0.01338474  0.89855419  0.60642906  0.00919705\n",
      "  0.12070468  0.56923421 -0.3044605   0.16080805  0.54873379  0.6918952\n",
      "  0.25516836  0.1058974   0.5889683   0.23724909  0.3253997   0.74649141\n",
      "  0.67426235  0.85449284  0.56334533  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 209\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011921837394538653 R2: 0.01870106432884644 time: 1703258206.4453814\n",
      "batch_idx: 1 loss: 0.016319402441759442 R2: 0.018709312358334618 time: 1703258211.3441973\n",
      "Training [70%] Loss: 0.014120619918149047 time: 1703258211.3441973\n",
      "weight: [ 0.48902244 -0.03936521  0.01302368  0.89855419  0.60642906  0.00919705\n",
      "  0.12044729  0.56934763 -0.30477113  0.16080805  0.54873379  0.6918952\n",
      "  0.25517478  0.10562109  0.58867405  0.23724909  0.3253997   0.74649141\n",
      "  0.67401416  0.85422044  0.56345876  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 210\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011922425311828215 R2: 0.018714592534919916 time: 1703258216.397383\n",
      "batch_idx: 1 loss: 0.016318392930748653 R2: 0.018722799892404107 time: 1703258221.8667357\n",
      "Training [70%] Loss: 0.014120409121288435 time: 1703258221.8667357\n",
      "weight: [ 0.48888368 -0.03965942  0.01266334  0.89855419  0.60642906  0.00919705\n",
      "  0.12019013  0.56946335 -0.305076    0.16080805  0.54873379  0.6918952\n",
      "  0.25518514  0.10534461  0.58837984  0.23724909  0.3253997   0.74649141\n",
      "  0.67376596  0.85394864  0.56357448  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 211\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011923005668056543 R2: 0.01872807042071567 time: 1703258227.3494515\n",
      "batch_idx: 1 loss: 0.01631739308949943 R2: 0.01873623747180364 time: 1703258232.545645\n",
      "Training [70%] Loss: 0.014120199378777988 time: 1703258232.545645\n",
      "weight: [ 0.48874746 -0.0399536   0.01230371  0.89855419  0.60642906  0.00919705\n",
      "  0.1199332   0.56958131 -0.30537526  0.16080805  0.54873379  0.6918952\n",
      "  0.25519936  0.10506796  0.58808566  0.23724909  0.3253997   0.74649141\n",
      "  0.67351778  0.85367745  0.56369243  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 212\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192357854155229 R2: 0.018741498518035526 time: 1703258237.4717667\n",
      "batch_idx: 1 loss: 0.01631640281749506 R2: 0.018749625627804245 time: 1703258242.5359838\n",
      "Training [71%] Loss: 0.014119990679523674 time: 1703258242.5359838\n",
      "weight: [ 0.4886138  -0.04024773  0.0119448   0.89855419  0.60642906  0.00919705\n",
      "  0.11967651  0.56970145 -0.30566909  0.16080805  0.54873379  0.6918952\n",
      "  0.25521736  0.10479115  0.58779152  0.23724909  0.3253997   0.74649141\n",
      "  0.67326962  0.85340687  0.56381257  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 213\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011924144009064892 R2: 0.018754877338374998 time: 1703258247.4138584\n",
      "batch_idx: 1 loss: 0.01631542201570391 R2: 0.01876296487165241 time: 1703258252.316951\n",
      "Training [71%] Loss: 0.014119783012384401 time: 1703258252.316951\n",
      "weight: [ 0.48848269 -0.04054183  0.01158659  0.89855419  0.60642906  0.00919705\n",
      "  0.11942007  0.56982372 -0.30595765  0.16080805  0.54873379  0.6918952\n",
      "  0.25523906  0.10451419  0.58749743  0.23724909  0.3253997   0.74649141\n",
      "  0.67302149  0.8531369   0.56393484  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 214\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011924702146843475 R2: 0.01876820738036511 time: 1703258257.545172\n",
      "batch_idx: 1 loss: 0.01631445058590219 R2: 0.018776255691391075 time: 1703258262.7971425\n",
      "Training [71%] Loss: 0.014119576366372833 time: 1703258262.7971425\n",
      "weight: [ 0.48835415 -0.04083588  0.01122909  0.89855419  0.60642906  0.00919705\n",
      "  0.1191639   0.56994807 -0.30624111  0.16080805  0.54873379  0.6918952\n",
      "  0.2552644   0.10423707  0.58720338  0.23724909  0.3253997   0.74649141\n",
      "  0.67277342  0.85286752  0.56405919  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 215\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192525303073745 R2: 0.018781489130861084 time: 1703258267.6548057\n",
      "batch_idx: 1 loss: 0.016313488431075034 R2: 0.018789498555668027 time: 1703258272.8302534\n",
      "Training [72%] Loss: 0.014119370730906241 time: 1703258272.8302534\n",
      "weight: [ 0.48822816 -0.04112988  0.01087229  0.89855419  0.60642906  0.00919705\n",
      "  0.11890799  0.57007445 -0.30651962  0.16080805  0.54873379  0.6918952\n",
      "  0.2552933   0.1039598   0.58690938  0.23724909  0.3253997   0.74649141\n",
      "  0.67252541  0.85259875  0.56418558  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 216\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011925796735451194 R2: 0.018794723060491236 time: 1703258277.794049\n",
      "batch_idx: 1 loss: 0.01631253545620063 R2: 0.01880269392001299 time: 1703258282.808546\n",
      "Training [72%] Loss: 0.014119166095825912 time: 1703258282.808546\n",
      "weight: [ 0.48810475 -0.04142384  0.01051618  0.89855419  0.60642906  0.00919705\n",
      "  0.11865236  0.57020282 -0.30679336  0.16080805  0.54873379  0.6918952\n",
      "  0.2553257   0.10368239  0.58661542  0.23724909  0.3253997   0.74649141\n",
      "  0.67227747  0.85233057  0.56431394  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 217\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011926333333949959 R2: 0.01880790962013623 time: 1703258287.7290418\n",
      "batch_idx: 1 loss: 0.016311591568448355 R2: 0.01881584222925836 time: 1703258292.5213075\n",
      "Training [72%] Loss: 0.014118962451199157 time: 1703258292.5213075\n",
      "weight: [ 0.48798391 -0.04171774  0.01016076  0.89855419  0.60642906  0.00919705\n",
      "  0.11839701  0.57033311 -0.30706247  0.16080805  0.54873379  0.6918952\n",
      "  0.25536152  0.10340483  0.58632152  0.23724909  0.3253997   0.74649141\n",
      "  0.67202962  0.85206299  0.56444423  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 218\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192686289766901 R2: 0.01882104924263195 time: 1703258296.9829037\n",
      "batch_idx: 1 loss: 0.01631065667662084 R2: 0.018828943914990015 time: 1703258301.5274978\n",
      "Training [73%] Loss: 0.014118759787144924 time: 1703258301.5274978\n",
      "weight: [ 0.48786564 -0.04201159  0.00980601  0.89855419  0.60642906  0.00919705\n",
      "  0.11814195  0.57046528 -0.30732713  0.16080805  0.54873379  0.6918952\n",
      "  0.2554007   0.10312713  0.58602767  0.23724909  0.3253997   0.74649141\n",
      "  0.67178186  0.85179601  0.5645764   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 219\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011927385497208917 R2: 0.018834142347647576 time: 1703258305.981139\n",
      "batch_idx: 1 loss: 0.016309730690502042 R2: 0.0188419993923038 time: 1703258310.504651\n",
      "Training [73%] Loss: 0.014118558093855479 time: 1703258310.504651\n",
      "weight: [ 0.48774995 -0.04230539  0.00945195  0.89855419  0.60642906  0.00919705\n",
      "  0.1178872   0.57059928 -0.3075875   0.16080805  0.54873379  0.6918952\n",
      "  0.25544317  0.10284929  0.58573387  0.23724909  0.3253997   0.74649141\n",
      "  0.67153422  0.85152962  0.5647104   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 220\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0119279012027434 R2: 0.018847189344675774 time: 1703258314.9526625\n",
      "batch_idx: 1 loss: 0.01630881352075241 R2: 0.0188550090601034 time: 1703258319.65921\n",
      "Training [73%] Loss: 0.014118357361747904 time: 1703258319.65921\n",
      "weight: [ 0.48763683 -0.04259913  0.00909856  0.89855419  0.60642906  0.00919705\n",
      "  0.11763275  0.57073507 -0.30784373  0.16080805  0.54873379  0.6918952\n",
      "  0.25548886  0.10257133  0.58544013  0.23724909  0.3253997   0.74649141\n",
      "  0.67128669  0.85126382  0.56484619  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 221\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011928410083810206 R2: 0.01886019063198674 time: 1703258324.3638444\n",
      "batch_idx: 1 loss: 0.016307905079298787 R2: 0.018867973304601215 time: 1703258329.226504\n",
      "Training [74%] Loss: 0.014118157581554497 time: 1703258329.226504\n",
      "weight: [ 0.4875263  -0.04289281  0.00874584  0.89855419  0.60642906  0.00919705\n",
      "  0.11737861  0.57087259 -0.30809597  0.16080805  0.54873379  0.6918952\n",
      "  0.25553772  0.10229323  0.58514645  0.23724909  0.3253997   0.74649141\n",
      "  0.6710393   0.85099861  0.56498371  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 222\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011928912208866932 R2: 0.018873146594007517 time: 1703258334.0918882\n",
      "batch_idx: 1 loss: 0.016307005279655794 R2: 0.018880892502338376 time: 1703258339.1185787\n",
      "Training [74%] Loss: 0.014117958744261364 time: 1703258339.1185787\n",
      "weight: [ 0.48741835 -0.04318643  0.00839377  0.89855419  0.60642906  0.00919705\n",
      "  0.1171248   0.57101179 -0.3083444   0.16080805  0.54873379  0.6918952\n",
      "  0.25558968  0.10201501  0.58485283  0.23724909  0.3253997   0.74649141\n",
      "  0.67079204  0.85073398  0.56512292  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 223\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011929407645169459 R2: 0.01888605760079831 time: 1703258343.9484942\n",
      "batch_idx: 1 loss: 0.01630611403679403 R2: 0.018893767020198515 time: 1703258348.734492\n",
      "Training [74%] Loss: 0.014117760840981744 time: 1703258348.734492\n",
      "weight: [ 0.48731298 -0.04347998  0.00804237  0.89855419  0.60642906  0.00919705\n",
      "  0.11687131  0.57115264 -0.30858915  0.16080805  0.54873379  0.6918952\n",
      "  0.25564469  0.10173667  0.58455927  0.23724909  0.3253997   0.74649141\n",
      "  0.67054494  0.85046994  0.56526376  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 224\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011929896459084922 R2: 0.018898924010360862 time: 1703258353.4675817\n",
      "batch_idx: 1 loss: 0.016305231266723758 R2: 0.018906597213514176 time: 1703258357.9069788\n",
      "Training [75%] Loss: 0.01411756386290434 time: 1703258357.9069788\n",
      "weight: [ 0.48721019 -0.04377348  0.00769161  0.89855419  0.60642906  0.00919705\n",
      "  0.11661815  0.57129509 -0.30883039  0.16080805  0.54873379  0.6918952\n",
      "  0.25570267  0.1014582   0.58426578  0.23724909  0.3253997   0.74649141\n",
      "  0.670298    0.85020649  0.56540621  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 225\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011930378716468262 R2: 0.01891174617136082 time: 1703258362.3439007\n",
      "batch_idx: 1 loss: 0.0163043568862349 R2: 0.018919383425150316 time: 1703258366.729886\n",
      "Training [75%] Loss: 0.014117367801351581 time: 1703258366.729886\n",
      "weight: [ 0.48710998 -0.0440669   0.0073415   0.89855419  0.60642906  0.00919705\n",
      "  0.11636534  0.57143908 -0.30906826  0.16080805  0.54873379  0.6918952\n",
      "  0.25576357  0.10117962  0.58397235  0.23724909  0.3253997   0.74649141\n",
      "  0.67005124  0.84994362  0.5655502   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 226\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011930854482731266 R2: 0.01892452442383097 time: 1703258371.40033\n",
      "batch_idx: 1 loss: 0.016303490812989298 R2: 0.01893212598687144 time: 1703258375.9572992\n",
      "Training [75%] Loss: 0.014117172647860282 time: 1703258375.9572992\n",
      "weight: [ 0.48701236 -0.04436026  0.00699203  0.89855419  0.60642906  0.00919705\n",
      "  0.11611287  0.57158458 -0.30930292  0.16080805  0.54873379  0.6918952\n",
      "  0.25582734  0.10090093  0.58367899  0.23724909  0.3253997   0.74649141\n",
      "  0.66980465  0.84968133  0.5656957   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 227\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011931323822625763 R2: 0.018937259097970727 time: 1703258380.53678\n",
      "batch_idx: 1 loss: 0.016302632965753808 R2: 0.018944825221593575 time: 1703258385.027141\n",
      "Training [76%] Loss: 0.014116978394189785 time: 1703258385.027141\n",
      "weight: [ 0.48691731 -0.04465356  0.0066432   0.89855419  0.60642906  0.00919705\n",
      "  0.11586075  0.57173154 -0.3095345   0.16080805  0.54873379  0.6918952\n",
      "  0.25589392  0.10062213  0.5833857   0.23724909  0.3253997   0.74649141\n",
      "  0.66955826  0.84941962  0.56584267  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 228\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01193178680005987 R2: 0.01894995051315651 time: 1703258389.4687543\n",
      "batch_idx: 1 loss: 0.016301783264454178 R2: 0.018957481444452973 time: 1703258394.100407\n",
      "Training [76%] Loss: 0.014116785032257023 time: 1703258394.100407\n",
      "weight: [ 0.48682484 -0.04494678  0.006295    0.89855419  0.60642906  0.00919705\n",
      "  0.115609    0.57187992 -0.30976316  0.16080805  0.54873379  0.6918952\n",
      "  0.25596325  0.10034323  0.58309248  0.23724909  0.3253997   0.74649141\n",
      "  0.66931207  0.84915848  0.56599105  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 229\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011932243478170606 R2: 0.018962598978619516 time: 1703258398.549808\n",
      "batch_idx: 1 loss: 0.01630094162998235 R2: 0.01897009496218449 time: 1703258403.1520934\n",
      "Training [76%] Loss: 0.014116592554076478 time: 1703258403.1520934\n",
      "weight: [ 0.48673494 -0.04523993  0.00594742  0.89855419  0.60642906  0.00919705\n",
      "  0.1153576   0.57202968 -0.30998904  0.16080805  0.54873379  0.6918952\n",
      "  0.25603527  0.10006422  0.58279933  0.23724909  0.3253997   0.74649141\n",
      "  0.66906609  0.84889793  0.56614081  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 230\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01193269391956091 R2: 0.018975204795200762 time: 1703258407.9594314\n",
      "batch_idx: 1 loss: 0.016300107983964786 R2: 0.018982666072231313 time: 1703258412.9049876\n",
      "Training [77%] Loss: 0.014116400951762847 time: 1703258412.9049876\n",
      "weight: [ 0.48664762 -0.045533    0.00560046  0.89855419  0.60642906  0.00919705\n",
      "  0.11510658  0.57218078 -0.31021228  0.16080805  0.54873379  0.6918952\n",
      "  0.25610995  0.09978511  0.58250625  0.23724909  0.3253997   0.74649141\n",
      "  0.66882032  0.84863794  0.5662919   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 231\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011933138186450037 R2: 0.018987768256527815 time: 1703258418.0086184\n",
      "batch_idx: 1 loss: 0.01629928224870541 R2: 0.018995195062961323 time: 1703258422.8700843\n",
      "Training [77%] Loss: 0.014116210217577724 time: 1703258422.8700843\n",
      "weight: [ 0.48656286 -0.045826    0.00525411  0.89855419  0.60642906  0.00919705\n",
      "  0.11485593  0.57233317 -0.31043302  0.16080805  0.54873379  0.6918952\n",
      "  0.25618721  0.0995059   0.58221325  0.23724909  0.3253997   0.74649141\n",
      "  0.66857479  0.84837854  0.56644429  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 232\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011933576340623359 R2: 0.01900028964887046 time: 1703258427.3656614\n",
      "batch_idx: 1 loss: 0.016298464347292818 R2: 0.019007682214944643 time: 1703258431.902813\n",
      "Training [77%] Loss: 0.014116020343958088 time: 1703258431.902813\n",
      "weight: [ 0.48648067 -0.04611893  0.00490837  0.89855419  0.60642906  0.00919705\n",
      "  0.11460566  0.57248682 -0.3106514   0.16080805  0.54873379  0.6918952\n",
      "  0.25626701  0.09922659  0.58192033  0.23724909  0.3253997   0.74649141\n",
      "  0.66832949  0.8481197   0.56659794  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 233\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011934008443300385 R2: 0.019012769250432493 time: 1703258436.4073827\n",
      "batch_idx: 1 loss: 0.016297654203691024 R2: 0.01902012780209139 time: 1703258440.9858563\n",
      "Training [78%] Loss: 0.014115831323495705 time: 1703258440.9858563\n",
      "weight: [ 0.48640103 -0.04641178  0.00456324  0.89855419  0.60642906  0.00919705\n",
      "  0.11435577  0.57264168 -0.31086754  0.16080805  0.54873379  0.6918952\n",
      "  0.25634931  0.0989472   0.58162748  0.23724909  0.3253997   0.74649141\n",
      "  0.66808444  0.84786143  0.5667528   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 234\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011934434555103148 R2: 0.019025207331305083 time: 1703258445.3372142\n",
      "batch_idx: 1 loss: 0.01629685174268593 R2: 0.019032532091812437 time: 1703258449.946474\n",
      "Training [78%] Loss: 0.01411564314889454 time: 1703258449.946474\n",
      "weight: [ 0.48632395 -0.04670455  0.0042187   0.89855419  0.60642906  0.00919705\n",
      "  0.11410628  0.57279772 -0.31108159  0.16080805  0.54873379  0.6918952\n",
      "  0.25643405  0.09866772  0.58133471  0.23724909  0.3253997   0.74649141\n",
      "  0.66783964  0.84760374  0.56690884  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 235\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011934854736164891 R2: 0.019037604154338972 time: 1703258454.407723\n",
      "batch_idx: 1 loss: 0.016296056889738692 R2: 0.01904489534453302 time: 1703258458.9468791\n",
      "Training [78%] Loss: 0.014115455812951792 time: 1703258458.9468791\n",
      "weight: [ 0.48624942 -0.04699724  0.00387475  0.89855419  0.60642906  0.00919705\n",
      "  0.11385718  0.5729549  -0.31129367  0.16080805  0.54873379  0.6918952\n",
      "  0.25652118  0.09838815  0.58104202  0.23724909  0.3253997   0.74649141\n",
      "  0.6675951   0.84734661  0.56706602  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 236\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011935269046261143 R2: 0.019049959976152664 time: 1703258463.865214\n",
      "batch_idx: 1 loss: 0.016295269570888404 R2: 0.019057217813514993 time: 1703258469.052413\n",
      "Training [79%] Loss: 0.014115269308574773 time: 1703258469.052413\n",
      "weight: [ 0.48617743 -0.04728985  0.00353139  0.89855419  0.60642906  0.00919705\n",
      "  0.11360848  0.57311319 -0.3115039   0.16080805  0.54873379  0.6918952\n",
      "  0.25661065  0.0981085   0.58074941  0.23724909  0.3253997   0.74649141\n",
      "  0.66735083  0.84709006  0.56722431  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 237\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011935677544841579 R2: 0.01906227504748792 time: 1703258473.7254372\n",
      "batch_idx: 1 loss: 0.0162944897127697 R2: 0.019069499745393292 time: 1703258478.1845381\n",
      "Training [79%] Loss: 0.01411508362880564 time: 1703258478.1845381\n",
      "weight: [ 0.48610798 -0.04758238  0.00318861  0.89855419  0.60642906  0.00919705\n",
      "  0.11336019  0.57327254 -0.31171241  0.16080805  0.54873379  0.6918952\n",
      "  0.25670242  0.09782876  0.58045688  0.23724909  0.3253997   0.74649141\n",
      "  0.66710684  0.84683407  0.56738367  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 238\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0119360802909683 R2: 0.019074549612937974 time: 1703258482.7735069\n",
      "batch_idx: 1 loss: 0.016293717242677343 R2: 0.019081741381020256 time: 1703258487.2550042\n",
      "Training [79%] Loss: 0.014114898766822821 time: 1703258487.2550042\n",
      "weight: [ 0.48604105 -0.04787483  0.00284641  0.89855419  0.60642906  0.00919705\n",
      "  0.1131123   0.57343294 -0.31191932  0.16080805  0.54873379  0.6918952\n",
      "  0.25679644  0.09754895  0.58016443  0.23724909  0.3253997   0.74649141\n",
      "  0.66686313  0.84657864  0.56754406  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 239\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01193647734326309 R2: 0.01908678391072549 time: 1703258491.6638892\n",
      "batch_idx: 1 loss: 0.01629295208857453 R2: 0.019093942955909937 time: 1703258496.2160323\n",
      "Training [80%] Loss: 0.01411471471591881 time: 1703258496.2160323\n",
      "weight: [ 0.48597665 -0.04816719  0.00250477  0.89855419  0.60642906  0.00919705\n",
      "  0.11286483  0.57359433 -0.31212475  0.16080805  0.54873379  0.6918952\n",
      "  0.25689267  0.09726907  0.57987206  0.23724909  0.3253997   0.74649141\n",
      "  0.66661972  0.84632379  0.56770545  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 240\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011936868759936468 R2: 0.019098978173018755 time: 1703258500.4946806\n",
      "batch_idx: 1 loss: 0.01629219417902269 R2: 0.019106104700132187 time: 1703258505.22799\n",
      "Training [80%] Loss: 0.014114531469479578 time: 1703258505.22799\n",
      "weight: [ 0.48591475 -0.04845947  0.0021637   0.89855419  0.60642906  0.00919705\n",
      "  0.11261778  0.57375668 -0.31232882  0.16080805  0.54873379  0.6918952\n",
      "  0.25699106  0.0969891   0.57957978  0.23724909  0.3253997   0.74649141\n",
      "  0.6663766   0.84606949  0.56786781  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 241\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011937254598869093 R2: 0.019111132626588256 time: 1703258509.8084118\n",
      "batch_idx: 1 loss: 0.016291443443099275 R2: 0.01911822683810982 time: 1703258514.5008926\n",
      "Training [80%] Loss: 0.014114349020984185 time: 1703258514.5008926\n",
      "weight: [ 0.48585537 -0.04875167  0.00182318  0.89855419  0.60642906  0.00919705\n",
      "  0.11237115  0.57391998 -0.31253164  0.16080805  0.54873379  0.6918952\n",
      "  0.25709156  0.09670907  0.57928759  0.23724909  0.3253997   0.74649141\n",
      "  0.66613378  0.84581577  0.5680311   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 242\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01193763491766463 R2: 0.019123247493277873 time: 1703258519.4354148\n",
      "batch_idx: 1 loss: 0.016290699810372428 R2: 0.019130309588777816 time: 1703258524.3905683\n",
      "Training [81%] Loss: 0.01411416736401853 time: 1703258524.3905683\n",
      "weight: [ 0.48579847 -0.04904378  0.00148322  0.89855419  0.60642906  0.00919705\n",
      "  0.11212494  0.57408417 -0.31273331  0.16080805  0.54873379  0.6918952\n",
      "  0.25719415  0.09642897  0.57899547  0.23724909  0.3253997   0.74649141\n",
      "  0.66589128  0.8455626   0.5681953   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 243\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011938009773637605 R2: 0.019135322990029957 time: 1703258529.448135\n",
      "batch_idx: 1 loss: 0.016289963210929438 R2: 0.019142353166065385 time: 1703258533.8972614\n",
      "Training [81%] Loss: 0.014113986492283521 time: 1703258533.8972614\n",
      "weight: [ 0.48574406 -0.04933581  0.00114381  0.89855419  0.60642906  0.00919705\n",
      "  0.11187917  0.57424924 -0.31293394  0.16080805  0.54873379  0.6918952\n",
      "  0.25729876  0.09614881  0.57870345  0.23724909  0.3253997   0.74649141\n",
      "  0.6656491   0.84531     0.56836037  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 244\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011938379223774321 R2: 0.019147359328740232 time: 1703258538.5197988\n",
      "batch_idx: 1 loss: 0.016289233575400002 R2: 0.019154357779341824 time: 1703258542.8534987\n",
      "Training [81%] Loss: 0.014113806399587162 time: 1703258542.8534987\n",
      "weight: [ 4.85692125e-01 -4.96277504e-02  8.04936631e-04  8.98554189e-01\n",
      "  6.06429060e-01  9.19705162e-03  1.11633825e-01  5.74415155e-01\n",
      " -3.13133640e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  2.57405367e-01  9.58685739e-02  5.78411506e-01  2.37249087e-01\n",
      "  3.25399698e-01  7.46491405e-01  6.65407237e-01  8.45057962e-01\n",
      "  5.68526278e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 245\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011938743324725885 R2: 0.019159356716312526 time: 1703258547.5034168\n",
      "batch_idx: 1 loss: 0.0162885108349339 R2: 0.01916632363352977 time: 1703258552.0457373\n",
      "Training [82%] Loss: 0.014113627079829893 time: 1703258552.0457373\n",
      "weight: [ 4.85642652e-01 -4.99196046e-02  4.66599650e-04  8.98554189e-01\n",
      "  6.06429060e-01  9.19705162e-03  1.11388922e-01  5.74581881e-01\n",
      " -3.13332510e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  2.57513925e-01  9.55882803e-02  5.78119652e-01  2.37249087e-01\n",
      "  3.25399698e-01  7.46491405e-01  6.65165710e-01  8.44806486e-01\n",
      "  5.68693004e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 246\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011939102132845558 R2: 0.01917131535501282 time: 1703258556.490835\n",
      "batch_idx: 1 loss: 0.016287794921149717 R2: 0.019178250929012153 time: 1703258561.2299957\n",
      "Training [82%] Loss: 0.014113448526997637 time: 1703258561.2299957\n",
      "weight: [ 4.85595631e-01 -5.02113719e-02  1.28792778e-04  8.98554189e-01\n",
      "  6.06429060e-01  9.19705162e-03  1.11144460e-01  5.74749394e-01\n",
      " -3.13530645e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  2.57624394e-01  9.53079265e-02  5.77827884e-01  2.37249087e-01\n",
      "  3.25399698e-01  7.46491405e-01  6.64924520e-01  8.44555573e-01\n",
      "  5.68860517e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 247\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011939455704232178 R2: 0.019183235442853275 time: 1703258565.6630812\n",
      "batch_idx: 1 loss: 0.016287085766100502 R2: 0.01919013986164364 time: 1703258570.1724632\n",
      "Training [82%] Loss: 0.01411327073516634 time: 1703258570.1724632\n",
      "weight: [ 4.85551050e-01 -5.05030518e-02 -2.08489628e-04  8.98554189e-01\n",
      "  6.06429060e-01  9.19705162e-03  1.10900444e-01  5.74917666e-01\n",
      " -3.13728141e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  2.57736736e-01  9.50275145e-02  5.77536205e-01  2.37249087e-01\n",
      "  3.25399698e-01  7.46491405e-01  6.64683674e-01  8.44305222e-01\n",
      "  5.69028789e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 248\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011939804094740974 R2: 0.019195117173767318 time: 1703258574.5002143\n",
      "batch_idx: 1 loss: 0.016286383302276934 R2: 0.019201990622975895 time: 1703258578.923411\n",
      "Training [83%] Loss: 0.014113093698508954 time: 1703258578.923411\n",
      "weight: [ 4.85508897e-01 -5.07946443e-02 -5.45253219e-04  8.98554189e-01\n",
      "  6.06429060e-01  9.19705162e-03  1.10656878e-01  5.75086670e-01\n",
      " -3.13925089e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  2.57850910e-01  9.47470463e-02  5.77244612e-01  2.37249087e-01\n",
      "  3.25399698e-01  7.46491405e-01  6.64443177e-01  8.44055433e-01\n",
      "  5.69197793e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 249\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011940147359964267 R2: 0.019206960737571444 time: 1703258583.6773236\n",
      "batch_idx: 1 loss: 0.016285687462625473 R2: 0.019213803400590757 time: 1703258588.7140734\n",
      "Training [83%] Loss: 0.014112917411294869 time: 1703258588.7140734\n",
      "weight: [ 4.85469159e-01 -5.10861489e-02 -8.81503645e-04  8.98554189e-01\n",
      "  6.06429060e-01  9.19705162e-03  1.10413763e-01  5.75256382e-01\n",
      " -3.14121578e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  2.57966881e-01  9.44665236e-02  5.76953107e-01  2.37249087e-01\n",
      "  3.25399698e-01  7.46491405e-01  6.64203035e-01  8.43806206e-01\n",
      "  5.69367505e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 250\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011940485555214885 R2: 0.01921876631995778 time: 1703258594.0992348\n",
      "batch_idx: 1 loss: 0.016284998180548852 R2: 0.01922557837828931 time: 1703258599.3974717\n",
      "Training [83%] Loss: 0.014112741867881869 time: 1703258599.3974717\n",
      "weight: [ 0.48543182 -0.05137757 -0.00121725  0.89855419  0.60642906  0.00919705\n",
      "  0.1101711   0.57542677 -0.31431769  0.16080805  0.54873379  0.6918952\n",
      "  0.25808461  0.09418595  0.57666169  0.23724909  0.3253997   0.74649141\n",
      "  0.66396325  0.84355754  0.5695379   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 251\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011940818735535704 R2: 0.019230534102644192 time: 1703258604.0302992\n",
      "batch_idx: 1 loss: 0.016284315389881947 R2: 0.01923731573610088 time: 1703258608.5016732\n",
      "Training [84%] Loss: 0.014112567062708825 time: 1703258608.5016732\n",
      "weight: [ 0.48539688 -0.05166889 -0.00155249  0.89855419  0.60642906  0.00919705\n",
      "  0.1099289   0.57559783 -0.31451352  0.16080805  0.54873379  0.6918952\n",
      "  0.25820406  0.09390532  0.57637036  0.23724909  0.3253997   0.74649141\n",
      "  0.66372384  0.84330944  0.56970895  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 252\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011941146955724869 R2: 0.019242264263637066 time: 1703258612.9313326\n",
      "batch_idx: 1 loss: 0.016283639024864738 R2: 0.01924901565027448 time: 1703258617.504184\n",
      "Training [84%] Loss: 0.014112392990294802 time: 1703258617.504184\n",
      "weight: [ 0.4853643  -0.05196013 -0.00188723  0.89855419  0.60642906  0.00919705\n",
      "  0.10968717  0.57576951 -0.31470913  0.16080805  0.54873379  0.6918952\n",
      "  0.2583252   0.09362465  0.57607912  0.23724909  0.3253997   0.74649141\n",
      "  0.66348479  0.8430619   0.56988064  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 253\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011941470270351251 R2: 0.01925395697742771 time: 1703258621.9224944\n",
      "batch_idx: 1 loss: 0.01628296902013438 R2: 0.01926067829336886 time: 1703258626.37813\n",
      "Training [84%] Loss: 0.014112219645242816 time: 1703258626.37813\n",
      "weight: [ 0.48533409 -0.05225129 -0.00222149  0.89855419  0.60642906  0.00919705\n",
      "  0.10944589  0.57594181 -0.31490462  0.16080805  0.54873379  0.6918952\n",
      "  0.25844799  0.09334392  0.57578797  0.23724909  0.3253997   0.74649141\n",
      "  0.66324613  0.84281492  0.57005293  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 254\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011941788733748617 R2: 0.0192656124150411 time: 1703258630.7654963\n",
      "batch_idx: 1 loss: 0.016282305310734397 R2: 0.019272303834460325 time: 1703258635.2411082\n",
      "Training [85%] Loss: 0.014112047022241506 time: 1703258635.2411082\n",
      "weight: [ 0.48530622 -0.05254235 -0.00255526  0.89855419  0.60642906  0.00919705\n",
      "  0.10920509  0.5761147  -0.31510004  0.16080805  0.54873379  0.6918952\n",
      "  0.2585724   0.09306316  0.57549691  0.23724909  0.3253997   0.74649141\n",
      "  0.66300784  0.8425685   0.57022582  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 255\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011942102400001315 R2: 0.019277230744031648 time: 1703258640.120018\n",
      "batch_idx: 1 loss: 0.016281647832122357 R2: 0.01928389243931994 time: 1703258645.0990846\n",
      "Training [85%] Loss: 0.014111875116061837 time: 1703258645.0990846\n",
      "weight: [ 0.48528067 -0.05283333 -0.00288855  0.89855419  0.60642906  0.00919705\n",
      "  0.10896476  0.57628816 -0.31529546  0.16080805  0.54873379  0.6918952\n",
      "  0.25869838  0.09278234  0.57520593  0.23724909  0.3253997   0.74649141\n",
      "  0.66276994  0.84232265  0.57039928  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 256\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011942411322940048 R2: 0.019288812128547494 time: 1703258650.061603\n",
      "batch_idx: 1 loss: 0.016280996520163286 R2: 0.019295444270486906 time: 1703258654.9717348\n",
      "Training [85%] Loss: 0.014111703921551668 time: 1703258654.9717348\n",
      "weight: [ 0.48525745 -0.05312421 -0.00322136  0.89855419  0.60642906  0.00919705\n",
      "  0.1087249   0.57646217 -0.31549097  0.16080805  0.54873379  0.6918952\n",
      "  0.25882592  0.09250149  0.57491504  0.23724909  0.3253997   0.74649141\n",
      "  0.66253243  0.84207736  0.57057329  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 257\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01194271555615163 R2: 0.019300356729481605 time: 1703258660.0457773\n",
      "batch_idx: 1 loss: 0.016280351311114112 R2: 0.01930695948728356 time: 1703258664.8147435\n",
      "Training [86%] Loss: 0.014111533433632872 time: 1703258664.8147435\n",
      "weight: [ 0.48523651 -0.05341501 -0.00355371  0.89855419  0.60642906  0.00919705\n",
      "  0.10848552  0.57663671 -0.31568662  0.16080805  0.54873379  0.6918952\n",
      "  0.25895498  0.09222059  0.57462424  0.23724909  0.3253997   0.74649141\n",
      "  0.66229532  0.84183263  0.57074783  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 258\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01194301515298981 R2: 0.019311864704637083 time: 1703258669.1985207\n",
      "batch_idx: 1 loss: 0.016279712141614694 R2: 0.01931843824585089 time: 1703258673.7569845\n",
      "Training [86%] Loss: 0.014111363647302252 time: 1703258673.7569845\n",
      "weight: [ 0.48521786 -0.05370573 -0.0038856   0.89855419  0.60642906  0.00919705\n",
      "  0.10824662  0.57681175 -0.31588247  0.16080805  0.54873379  0.6918952\n",
      "  0.25908552  0.09193965  0.57433353  0.23724909  0.3253997   0.74649141\n",
      "  0.66205861  0.84158847  0.57092288  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 259\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01194331016657538 R2: 0.019323336208814212 time: 1703258677.9186883\n",
      "batch_idx: 1 loss: 0.016279078948690663 R2: 0.019329880699264335 time: 1703258682.3891468\n",
      "Training [86%] Loss: 0.014111194557633022 time: 1703258682.3891468\n",
      "weight: [ 0.48520147 -0.05399635 -0.00421703  0.89855419  0.60642906  0.00919705\n",
      "  0.1080082   0.5769873  -0.31607859  0.16080805  0.54873379  0.6918952\n",
      "  0.25921751  0.09165868  0.57404291  0.23724909  0.3253997   0.74649141\n",
      "  0.6618223   0.84134487  0.57109842  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 260\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0119436006497865 R2: 0.019334771393835992 time: 1703258686.8554254\n",
      "batch_idx: 1 loss: 0.016278451669761183 R2: 0.01934128699767368 time: 1703258691.4278588\n",
      "Training [87%] Loss: 0.014111026159773842 time: 1703258691.4278588\n",
      "weight: [ 0.48518733 -0.05428689 -0.004548    0.89855419  0.60642906  0.00919705\n",
      "  0.10777027  0.57716331 -0.31627502  0.16080805  0.54873379  0.6918952\n",
      "  0.25935092  0.09137766  0.57375237  0.23724909  0.3253997   0.74649141\n",
      "  0.66158641  0.84110183  0.57127444  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 261\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011943886655250086 R2: 0.019346170408582886 time: 1703258695.935526\n",
      "batch_idx: 1 loss: 0.016277830242641192 R2: 0.019352657288397412 time: 1703258700.509839\n",
      "Training [87%] Loss: 0.014110858448945638 time: 1703258700.509839\n",
      "weight: [ 0.48517541 -0.05457734 -0.00487854  0.89855419  0.60642906  0.00919705\n",
      "  0.10753283  0.57733979 -0.31647183  0.16080805  0.54873379  0.6918952\n",
      "  0.25948573  0.09109661  0.57346192  0.23724909  0.3253997   0.74649141\n",
      "  0.66135093  0.84085936  0.57145091  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 262\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011944168235341356 R2: 0.019357533399075755 time: 1703258704.9238725\n",
      "batch_idx: 1 loss: 0.01627721460553589 R2: 0.01936399171596026 time: 1703258709.451922\n",
      "Training [87%] Loss: 0.014110691420438623 time: 1703258709.451922\n",
      "weight: [ 0.48516571 -0.0548677  -0.00520863  0.89855419  0.60642906  0.00919705\n",
      "  0.10729588  0.57751671 -0.31666906  0.16080805  0.54873379  0.6918952\n",
      "  0.2596219   0.09081553  0.57317155  0.23724909  0.3253997   0.74649141\n",
      "  0.66111586  0.84061746  0.57162783  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 263\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011944445442187984 R2: 0.019368860508596764 time: 1703258713.978229\n",
      "batch_idx: 1 loss: 0.016276604697034913 R2: 0.019375290422120384 time: 1703258718.397976\n",
      "Training [88%] Loss: 0.014110525069611447 time: 1703258718.397976\n",
      "weight: [ 0.48515819 -0.05515798 -0.00553829  0.89855419  0.60642906  0.00919705\n",
      "  0.10705942  0.57769405 -0.31686676  0.16080805  0.54873379  0.6918952\n",
      "  0.25975939  0.09053441  0.57288127  0.23724909  0.3253997   0.74649141\n",
      "  0.66088122  0.84037612  0.57180518  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 264\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011944718327670854 R2: 0.019380151877780638 time: 1703258723.3610473\n",
      "batch_idx: 1 loss: 0.016276000456112754 R2: 0.01938655354593921 time: 1703258728.2204704\n",
      "Training [88%] Loss: 0.014110359391891803 time: 1703258728.2204704\n",
      "weight: [ 0.48515285 -0.05544817 -0.00586751  0.89855419  0.60642906  0.00919705\n",
      "  0.10682346  0.57787181 -0.31706497  0.16080805  0.54873379  0.6918952\n",
      "  0.2598982   0.09025326  0.57259108  0.23724909  0.3253997   0.74649141\n",
      "  0.660647    0.84013535  0.57198294  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 265\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011944986943417866 R2: 0.019391407644663627 time: 1703258733.025402\n",
      "batch_idx: 1 loss: 0.0162754018221347 R2: 0.019397781223877364 time: 1703258738.079644\n",
      "Training [88%] Loss: 0.014110194382776282 time: 1703258738.079644\n",
      "weight: [ 0.48514966 -0.05573828 -0.00619632  0.89855419  0.60642906  0.00919705\n",
      "  0.10658799  0.57804997 -0.31726374  0.16080805  0.54873379  0.6918952\n",
      "  0.26003827  0.08997207  0.57230097  0.23724909  0.3253997   0.74649141\n",
      "  0.66041321  0.83989515  0.57216109  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 266\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011945251340795063 R2: 0.019402627944719475 time: 1703258742.7827373\n",
      "batch_idx: 1 loss: 0.01627480873486228 R2: 0.019408973589882694 time: 1703258747.3490741\n",
      "Training [89%] Loss: 0.014110030037828672 time: 1703258747.3490741\n",
      "weight: [ 0.4851486  -0.05602831 -0.00652471  0.89855419  0.60642906  0.00919705\n",
      "  0.10635303  0.57822851 -0.3174631   0.16080805  0.54873379  0.6918952\n",
      "  0.2601796   0.08969086  0.57201095  0.23724909  0.3253997   0.74649141\n",
      "  0.66017986  0.83965552  0.57233963  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 267\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011945511570901141 R2: 0.019413812910906714 time: 1703258751.957581\n",
      "batch_idx: 1 loss: 0.01627422113445414 R2: 0.019420130775446687 time: 1703258756.4277954\n",
      "Training [89%] Loss: 0.014109866352677641 time: 1703258756.4277954\n",
      "weight: [ 0.48514966 -0.05631825 -0.00685268  0.89855419  0.60642906  0.00919705\n",
      "  0.10611857  0.57840742 -0.3176631   0.16080805  0.54873379  0.6918952\n",
      "  0.26032214  0.08940961  0.57172101  0.23724909  0.3253997   0.74649141\n",
      "  0.65994694  0.83941645  0.57251854  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 268\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011945767684566209 R2: 0.019424962673752377 time: 1703258760.7460334\n",
      "batch_idx: 1 loss: 0.01627363896146438 R2: 0.019431252909638763 time: 1703258765.3245013\n",
      "Training [89%] Loss: 0.014109703323015295 time: 1703258765.3245013\n",
      "weight: [ 0.48515281 -0.0566081  -0.00718025  0.89855419  0.60642906  0.00919705\n",
      "  0.10588461  0.57858668 -0.31786377  0.16080805  0.54873379  0.6918952\n",
      "  0.26046588  0.08912834  0.57143116  0.23724909  0.3253997   0.74649141\n",
      "  0.65971446  0.83917796  0.57269781  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 269\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011946019732350644 R2: 0.019436077361437376 time: 1703258769.858814\n",
      "batch_idx: 1 loss: 0.016273062156842967 R2: 0.019442340119155355 time: 1703258774.38644\n",
      "Training [90%] Loss: 0.014109540944596806 time: 1703258774.38644\n",
      "weight: [ 0.48515804 -0.05689787 -0.00750741  0.89855419  0.60642906  0.00919705\n",
      "  0.10565116  0.5787663  -0.31806513  0.16080805  0.54873379  0.6918952\n",
      "  0.26061078  0.08884703  0.57114138  0.23724909  0.3253997   0.74649141\n",
      "  0.65948243  0.83894004  0.57287742  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 270\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011946267764540577 R2: 0.01944715709985012 time: 1703258778.8470013\n",
      "batch_idx: 1 loss: 0.016272490661940274 R2: 0.019453392528379077 time: 1703258783.3829103\n",
      "Training [90%] Loss: 0.014109379213240425 time: 1703258783.3829103\n",
      "weight: [ 0.48516532 -0.05718757 -0.00783418  0.89855419  0.60642906  0.00919705\n",
      "  0.10541822  0.57894624 -0.31826723  0.16080805  0.54873379  0.6918952\n",
      "  0.26075682  0.0885657   0.57085169  0.23724909  0.3253997   0.74649141\n",
      "  0.65925085  0.83870268  0.57305737  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 271\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011946511831140212 R2: 0.019458202012632042 time: 1703258787.8632176\n",
      "batch_idx: 1 loss: 0.016271924418512725 R2: 0.019464410259466547 time: 1703258792.3184307\n",
      "Training [90%] Loss: 0.014109218124826468 time: 1703258792.3184307\n",
      "weight: [ 0.48517463 -0.05747718 -0.00816056  0.89855419  0.60642906  0.00919705\n",
      "  0.10518579  0.57912651 -0.31847008  0.16080805  0.54873379  0.6918952\n",
      "  0.26090398  0.08828434  0.57056208  0.23724909  0.3253997   0.74649141\n",
      "  0.65901971  0.8384659   0.57323764  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 272\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011946751981864576 R2: 0.019469212221214116 time: 1703258797.453723\n",
      "batch_idx: 1 loss: 0.0162713633687269 R2: 0.019475393432399235 time: 1703258802.7488503\n",
      "Training [91%] Loss: 0.014109057675295737 time: 1703258802.7488503\n",
      "weight: [ 0.48518595 -0.0577667  -0.00848656  0.89855419  0.60642906  0.00919705\n",
      "  0.10495388  0.5793071  -0.31867371  0.16080805  0.54873379  0.6918952\n",
      "  0.26105222  0.08800296  0.57027255  0.23724909  0.3253997   0.74649141\n",
      "  0.65878903  0.83822969  0.57341822  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 273\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011946988266135012 R2: 0.019480187844879038 time: 1703258807.4832559\n",
      "batch_idx: 1 loss: 0.016270807455161088 R2: 0.01948634216503098 time: 1703258812.2793956\n",
      "Training [91%] Loss: 0.01410889786064805 time: 1703258812.2793956\n",
      "weight: [ 0.48519926 -0.05805615 -0.00881217  0.89855419  0.60642906  0.00919705\n",
      "  0.10472248  0.57948798 -0.31887815  0.16080805  0.54873379  0.6918952\n",
      "  0.26120153  0.08772155  0.5699831   0.23724909  0.3253997   0.74649141\n",
      "  0.65855881  0.83799406  0.5735991   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 274\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011947220733076013 R2: 0.019491129000824836 time: 1703258816.8648236\n",
      "batch_idx: 1 loss: 0.016270256620806844 R2: 0.01949725657312673 time: 1703258821.3461857\n",
      "Training [91%] Loss: 0.01410873867694143 time: 1703258821.3461857\n",
      "weight: [ 0.48521454 -0.05834552 -0.00913741  0.89855419  0.60642906  0.00919705\n",
      "  0.10449159  0.57966916 -0.31908342  0.16080805  0.54873379  0.6918952\n",
      "  0.26135188  0.08744012  0.56969373  0.23724909  0.3253997   0.74649141\n",
      "  0.65832905  0.837759    0.57378028  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 275\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011947449431510927 R2: 0.019502035804231044 time: 1703258825.7733424\n",
      "batch_idx: 1 loss: 0.016269710809072475 R2: 0.019508136770414408 time: 1703258830.3243072\n",
      "Training [92%] Loss: 0.014108580120291701 time: 1703258830.3243072\n",
      "weight: [ 0.48523177 -0.05863481 -0.00946228  0.89855419  0.60642906  0.00919705\n",
      "  0.10426123  0.57985062 -0.31928953  0.16080805  0.54873379  0.6918952\n",
      "  0.26150325  0.08715866  0.56940444  0.23724909  0.3253997   0.74649141\n",
      "  0.65809975  0.83752452  0.57396174  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 276\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01194767440995562 R2: 0.01951290836829933 time: 1703258834.7055762\n",
      "batch_idx: 1 loss: 0.016269169963788212 R2: 0.019518982868647616 time: 1703258839.1983757\n",
      "Training [92%] Loss: 0.014108422186871916 time: 1703258839.1983757\n",
      "weight: [ 0.48525093 -0.05892403 -0.00978678  0.89855419  0.60642906  0.00919705\n",
      "  0.10403139  0.58003235 -0.3194965   0.16080805  0.54873379  0.6918952\n",
      "  0.26165562  0.08687717  0.56911523  0.23724909  0.3253997   0.74649141\n",
      "  0.65787091  0.83729061  0.57414347  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 277\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011947895716611314 R2: 0.019523746804293807 time: 1703258843.6844518\n",
      "batch_idx: 1 loss: 0.0162686340292112 R2: 0.01952979497766305 time: 1703258848.4503076\n",
      "Training [92%] Loss: 0.014108264872911256 time: 1703258848.4503076\n",
      "weight: [ 0.48527199 -0.05921317 -0.01011093  0.89855419  0.60642906  0.00919705\n",
      "  0.10380207  0.58021434 -0.31970435  0.16080805  0.54873379  0.6918952\n",
      "  0.26180895  0.08659567  0.56882609  0.23724909  0.3253997   0.74649141\n",
      "  0.65764255  0.83705728  0.57432547  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 278\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011948113399358671 R2: 0.019534551221590424 time: 1703258853.2942743\n",
      "batch_idx: 1 loss: 0.016268102950028736 R2: 0.019540573205428347 time: 1703258858.1205528\n",
      "Training [93%] Loss: 0.014108108174693703 time: 1703258858.1205528\n",
      "weight: [ 0.48529493 -0.05950223 -0.01043472  0.89855419  0.60642906  0.00919705\n",
      "  0.10357328  0.5803966  -0.31991308  0.16080805  0.54873379  0.6918952\n",
      "  0.26196323  0.08631414  0.56853703  0.23724909  0.3253997   0.74649141\n",
      "  0.65741466  0.83682453  0.57450772  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 279\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011948327505753302 R2: 0.019545321727730824 time: 1703258862.9799821\n",
      "batch_idx: 1 loss: 0.016267576671360952 R2: 0.019551317658082712 time: 1703258867.9634051\n",
      "Training [93%] Loss: 0.014107952088557127 time: 1703258867.9634051\n",
      "weight: [ 0.48531973 -0.05979121 -0.01075816  0.89855419  0.60642906  0.00919705\n",
      "  0.10334501  0.5805791  -0.32012272  0.16080805  0.54873379  0.6918952\n",
      "  0.26211844  0.08603259  0.56824804  0.23724909  0.3253997   0.74649141\n",
      "  0.65718724  0.83659235  0.57469022  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 280\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011948538083021367 R2: 0.019556058428478296 time: 1703258872.6704612\n",
      "batch_idx: 1 loss: 0.016267055138763798 R2: 0.01956202843998167 time: 1703258877.3368423\n",
      "Training [93%] Loss: 0.014107796610892582 time: 1703258877.3368423\n",
      "weight: [ 0.48534637 -0.06008013 -0.01108126  0.89855419  0.60642906  0.00919705\n",
      "  0.10311727  0.58076184 -0.32033326  0.16080805  0.54873379  0.6918952\n",
      "  0.26227456  0.08575102  0.56795913  0.23724909  0.3253997   0.74649141\n",
      "  0.6569603   0.83636076  0.57487296  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 281\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011948745178054125 R2: 0.019566761427864288 time: 1703258881.7459235\n",
      "batch_idx: 1 loss: 0.016266538298233477 R2: 0.019572705653748024 time: 1703258886.4988823\n",
      "Training [94%] Loss: 0.0141076417381438 time: 1703258886.4988823\n",
      "weight: [ 0.48537482 -0.06036896 -0.01140402  0.89855419  0.60642906  0.00919705\n",
      "  0.10289006  0.58094482 -0.32054472  0.16080805  0.54873379  0.6918952\n",
      "  0.26243155  0.08546942  0.56767029  0.23724909  0.3253997   0.74649141\n",
      "  0.65673383  0.83612975  0.57505594  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 282\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011948948837401744 R2: 0.0195774308282316 time: 1703258890.9950643\n",
      "batch_idx: 1 loss: 0.016266026096210934 R2: 0.01958334940032469 time: 1703258895.53298\n",
      "Training [94%] Loss: 0.014107487466806339 time: 1703258895.53298\n",
      "weight: [ 0.48540506 -0.06065773 -0.01172644  0.89855419  0.60642906  0.00919705\n",
      "  0.10266338  0.58112802 -0.32075711  0.16080805  0.54873379  0.6918952\n",
      "  0.26258941  0.08518781  0.56738153  0.23724909  0.3253997   0.74649141\n",
      "  0.65650785  0.83589931  0.57523914  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 283\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011949149107267371 R2: 0.019588066730274467 time: 1703258899.9662805\n",
      "batch_idx: 1 loss: 0.016265518479585784 R2: 0.019593959779021675 time: 1703258904.3720489\n",
      "Training [94%] Loss: 0.014107333793426578 time: 1703258904.3720489\n",
      "weight: [ 0.48543708 -0.06094643 -0.01204854  0.89855419  0.60642906  0.00919705\n",
      "  0.10243723  0.58131144 -0.32097042  0.16080805  0.54873379  0.6918952\n",
      "  0.26274812  0.08490617  0.56709283  0.23724909  0.3253997   0.74649141\n",
      "  0.65628236  0.83566946  0.57542256  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 284\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01194934603350207 R2: 0.019598669233087507 time: 1703258908.9319713\n",
      "batch_idx: 1 loss: 0.016265015395699252 R2: 0.01960453688756192 time: 1703258914.0104704\n",
      "Training [95%] Loss: 0.01410718071460066 time: 1703258914.0104704\n",
      "weight: [ 0.48547084 -0.06123505 -0.01237031  0.89855419  0.60642906  0.00919705\n",
      "  0.10221162  0.58149508 -0.32118466  0.16080805  0.54873379  0.6918952\n",
      "  0.26290764  0.08462451  0.56680421  0.23724909  0.3253997   0.74649141\n",
      "  0.65605735  0.83544019  0.5756062   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 285\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011949539661600492 R2: 0.01960923843421314 time: 1703258918.7739584\n",
      "batch_idx: 1 loss: 0.016264516792347156 R2: 0.01961508082211816 time: 1703258923.7105906\n",
      "Training [95%] Loss: 0.014107028226973824 time: 1703258923.7105906\n",
      "weight: [ 0.48550633 -0.06152361 -0.01269177  0.89855419  0.60642906  0.00919705\n",
      "  0.10198654  0.58167892 -0.32139983  0.16080805  0.54873379  0.6918952\n",
      "  0.26306797  0.08434284  0.56651565  0.23724909  0.3253997   0.74649141\n",
      "  0.65583283  0.83521151  0.57579004  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 286\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011949730036696277 R2: 0.019619774429687986 time: 1703258928.6786149\n",
      "batch_idx: 1 loss: 0.01626402261778314 R2: 0.01962559167736 time: 1703258933.5231793\n",
      "Training [95%] Loss: 0.014106876327239708 time: 1703258933.5231793\n",
      "weight: [ 0.48554351 -0.06181209 -0.01301291  0.89855419  0.60642906  0.00919705\n",
      "  0.101762    0.58186296 -0.32161594  0.16080805  0.54873379  0.6918952\n",
      "  0.26322908  0.08406114  0.56622716  0.23724909  0.3253997   0.74649141\n",
      "  0.6556088   0.83498341  0.57597408  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 287\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011949917203556894 R2: 0.019630277314086397 time: 1703258938.2567668\n",
      "batch_idx: 1 loss: 0.01626353282072257 R2: 0.01963606954649688 time: 1703258942.6138508\n",
      "Training [96%] Loss: 0.014106725012139732 time: 1703258942.6138508\n",
      "weight: [ 0.48558238 -0.06210051 -0.01333374  0.89855419  0.60642906  0.00919705\n",
      "  0.101538    0.5820472  -0.32183298  0.16080805  0.54873379  0.6918952\n",
      "  0.26339095  0.08377943  0.56593874  0.23724909  0.3253997   0.74649141\n",
      "  0.65538526  0.8347559   0.57615832  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 288\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011950101206578401 R2: 0.01964074718055875 time: 1703258947.0783675\n",
      "batch_idx: 1 loss: 0.016263047350345918 R2: 0.019646514521329927 time: 1703258951.5654943\n",
      "Training [96%] Loss: 0.01410657427846216 time: 1703258951.5654943\n",
      "weight: [ 0.4856229  -0.06238887 -0.01365427  0.89855419  0.60642906  0.00919705\n",
      "  0.10131454  0.58223162 -0.32205094  0.16080805  0.54873379  0.6918952\n",
      "  0.26355356  0.08349769  0.56565039  0.23724909  0.3253997   0.74649141\n",
      "  0.65516222  0.83452897  0.57634275  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 289\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011950282089780872 R2: 0.019651184120873633 time: 1703258955.86034\n",
      "batch_idx: 1 loss: 0.01626256615630161 R2: 0.01965692669229191 time: 1703258960.2654996\n",
      "Training [96%] Loss: 0.01410642412304124 time: 1703258960.2654996\n",
      "weight: [ 0.48566506 -0.06267715 -0.0139745   0.89855419  0.60642906  0.00919705\n",
      "  0.10109162  0.58241624 -0.32226983  0.16080805  0.54873379  0.6918952\n",
      "  0.26371691  0.08321593  0.5653621   0.23724909  0.3253997   0.74649141\n",
      "  0.65493968  0.83430262  0.57652736  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 290\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011950459896804296 R2: 0.01966158822545927 time: 1703258964.7570405\n",
      "batch_idx: 1 loss: 0.016262089188708605 R2: 0.019667306148480446 time: 1703258969.5235744\n",
      "Training [97%] Loss: 0.01410627454275645 time: 1703258969.5235744\n",
      "weight: [ 0.48570882 -0.06296538 -0.01429443  0.89855419  0.60642906  0.00919705\n",
      "  0.10086924  0.58260103 -0.32248965  0.16080805  0.54873379  0.6918952\n",
      "  0.26388096  0.08293416  0.56507388  0.23724909  0.3253997   0.74649141\n",
      "  0.65471763  0.83407687  0.57671215  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 291\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011950634670904731 R2: 0.01967195958344936 time: 1703258974.4976287\n",
      "batch_idx: 1 loss: 0.01626161639815877 R2: 0.019677652977706073 time: 1703258979.4267213\n",
      "Training [97%] Loss: 0.01410612553453175 time: 1703258979.4267213\n",
      "weight: [ 0.48575417 -0.06325354 -0.01461407  0.89855419  0.60642906  0.00919705\n",
      "  0.10064741  0.58278599 -0.32271038  0.16080805  0.54873379  0.6918952\n",
      "  0.2640457   0.08265237  0.56478572  0.23724909  0.3253997   0.74649141\n",
      "  0.65449609  0.8338517   0.57689711  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 292\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01195080645495035 R2: 0.019682298282722388 time: 1703258984.3059213\n",
      "batch_idx: 1 loss: 0.016261147735719607 R2: 0.01968796726653088 time: 1703258989.097285\n",
      "Training [97%] Loss: 0.014105977095334978 time: 1703258989.097285\n",
      "weight: [ 0.48580109 -0.06354163 -0.01493343  0.89855419  0.60642906  0.00919705\n",
      "  0.10042612  0.58297112 -0.32293202  0.16080805  0.54873379  0.6918952\n",
      "  0.26421112  0.08237056  0.56449762  0.23724909  0.3253997   0.74649141\n",
      "  0.65427505  0.83362712  0.57708224  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 293\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011950975291417245 R2: 0.019692604409939918 time: 1703258993.9879274\n",
      "batch_idx: 1 loss: 0.016260683152936935 R2: 0.019698249100307375 time: 1703258998.877532\n",
      "Training [98%] Loss: 0.01410582922217709 time: 1703258998.877532\n",
      "weight: [ 0.48584955 -0.06382967 -0.01525251  0.89855419  0.60642906  0.00919705\n",
      "  0.10020537  0.58315642 -0.32315456  0.16080805  0.54873379  0.6918952\n",
      "  0.26437719  0.08208873  0.56420959  0.23724909  0.3253997   0.74649141\n",
      "  0.65405452  0.83340314  0.57726754  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 294\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011951141222385742 R2: 0.01970287805058024 time: 1703259003.7344573\n",
      "batch_idx: 1 loss: 0.016260222601837078 R2: 0.019708498563222543 time: 1703259008.6402228\n",
      "Training [98%] Loss: 0.014105681912111409 time: 1703259008.6402228\n",
      "weight: [ 0.48589953 -0.06411764 -0.01557131  0.89855419  0.60642906  0.00919705\n",
      "  0.09998518  0.58334187 -0.323378    0.16080805  0.54873379  0.6918952\n",
      "  0.2645439   0.08180688  0.56392161  0.23724909  0.3253997   0.74649141\n",
      "  0.65383449  0.83317974  0.57745299  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 295\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011951304289536903 R2: 0.01971311928898467 time: 1703259013.2120397\n",
      "batch_idx: 1 loss: 0.016259766034928628 R2: 0.019718715738335058 time: 1703259017.6206234\n",
      "Training [98%] Loss: 0.014105535162232764 time: 1703259017.6206234\n",
      "weight: [ 0.485951   -0.06440556 -0.01588984  0.89855419  0.60642906  0.00919705\n",
      "  0.09976553  0.58352748 -0.32360232  0.16080805  0.54873379  0.6918952\n",
      "  0.26471123  0.08152501  0.5636337   0.23724909  0.3253997   0.74649141\n",
      "  0.65361497  0.83295693  0.5776386   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 296\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01195146453414967 R2: 0.01972332820839029 time: 1703259021.9783576\n",
      "batch_idx: 1 loss: 0.016259313405204156 R2: 0.0197289007076058 time: 1703259026.485226\n",
      "Training [99%] Loss: 0.014105388969676913 time: 1703259026.485226\n",
      "weight: [ 0.48600394 -0.06469342 -0.0162081   0.89855419  0.60642906  0.00919705\n",
      "  0.09954643  0.58371324 -0.32382753  0.16080805  0.54873379  0.6918952\n",
      "  0.26487917  0.08124312  0.56334584  0.23724909  0.3253997   0.74649141\n",
      "  0.65339596  0.83273471  0.57782436  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 297\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011951621997097862 R2: 0.019733504890969367 time: 1703259030.7219098\n",
      "batch_idx: 1 loss: 0.01625886466614175 R2: 0.019739053551943275 time: 1703259035.0657253\n",
      "Training [99%] Loss: 0.014105243331619806 time: 1703259035.0657253\n",
      "weight: [ 0.48605834 -0.06498122 -0.0165261   0.89855419  0.60642906  0.00919705\n",
      "  0.09932788  0.58389915 -0.32405359  0.16080805  0.54873379  0.6918952\n",
      "  0.2650477   0.08096122  0.56305804  0.23724909  0.3253997   0.74649141\n",
      "  0.65317746  0.83251309  0.57801027  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 298\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011951776718847366 R2: 0.01974364941786433 time: 1703259039.4708133\n",
      "batch_idx: 1 loss: 0.016258419771706577 R2: 0.019749174351234244 time: 1703259044.0599718\n",
      "Training [99%] Loss: 0.014105098245276972 time: 1703259044.0599718\n",
      "weight: [ 0.48611416 -0.06526896 -0.01684384  0.89855419  0.60642906  0.00919705\n",
      "  0.09910988  0.5840852  -0.32428052  0.16080805  0.54873379  0.6918952\n",
      "  0.26521681  0.0806793   0.56277029  0.23724909  0.3253997   0.74649141\n",
      "  0.65295947  0.83229205  0.57819632  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 299\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011951928739453322 R2: 0.019753761869222508 time: 1703259048.691487\n",
      "batch_idx: 1 loss: 0.016257978676352272 R2: 0.019759263184383924 time: 1703259053.7079194\n",
      "Training [100%] Loss: 0.014104953707902797 time: 1703259053.7079194\n",
      "weight: [ 0.48617139 -0.06555665 -0.01716133  0.89855419  0.60642906  0.00919705\n",
      "  0.09889243  0.58427138 -0.32450829  0.16080805  0.54873379  0.6918952\n",
      "  0.26538648  0.08039736  0.5624826   0.23724909  0.3253997   0.74649141\n",
      "  0.652742    0.83207161  0.57838251  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 300\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011952078098557823 R2: 0.01976384232423034 time: 1703259058.6053634\n",
      "batch_idx: 1 loss: 0.016257541335021743 R2: 0.01976932012935073 time: 1703259063.433725\n",
      "Training [100%] Loss: 0.014104809716789783 time: 1703259063.433725\n",
      "weight: [ 0.48623    -0.06584429 -0.01747856  0.89855419  0.60642906  0.00919705\n",
      "  0.09867553  0.5844577  -0.32473689  0.16080805  0.54873379  0.6918952\n",
      "  0.26555669  0.0801154   0.56219497  0.23724909  0.3253997   0.74649141\n",
      "  0.65252504  0.83185176  0.57856883  0.5683086   0.09367477  0.3677158 ]\n",
      "train_MSE: 0.01411574402619809\n",
      "train_RMSE: 0.11880969668422729\n",
      "train_MAE: 0.10310220799018592\n",
      "train_MAPE: 0.22767971001452847\n",
      "train_R2: 0.01976932012935073\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "weights =[0.68756473 , 1.2864305 ,  0.5411694 , -0.01524038 , 0.5852044 ,  0.6762012,\n",
    "  0.35593897 , 0.22628789,  0.38244092,  0.35140917,  0.86482936 , 0.8531242,\n",
    "  0.09241156 , 0.6720707  , 0.38071635,  0.95416117 , 0.63409   ,  0.40179932,\n",
    "  0.7345088  , 0.6243114  , 0.3178202 , -0.2618623  , 0.18122938,  1.0447433,\n",
    "  0.48699683 , 0.7739934  , 0.38703147 , 0.48046085,  0.5525667 ,  0.52838504,\n",
    "  0.28538367 , 0.30099392,  0.74503726 , 0.67772216,  0.3839896 ,  0.417687]\n",
    "weights = npp.array(weights, requires_grad=True)\n",
    "'''\n",
    "weights = params\n",
    "loss_list=[]\n",
    "r2_list=[]\n",
    "\n",
    "n_epochs=300\n",
    "\n",
    "start_time=time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss  = []\n",
    "    total_r2  = []\n",
    "    print(f\"epoch {epoch+1}\\n-------------------------------\")\n",
    "    for it, batch_index in enumerate (chain(*[X_batches])):\n",
    "        # Update the weights by one optimizer step\n",
    "        \n",
    "        X_batch = X[batch_index]\n",
    "        Y_batch = Y[batch_index]\n",
    "        weights, _, _ = opt.step(cost, weights, X_batch, Y_batch)\n",
    "\n",
    "        # Compute accuracy\n",
    "        predictions = [QNN(weights, x) for x in X]\n",
    "        r2 = R2(Y, predictions)\n",
    "        cost_t=cost(weights,X_batch,Y_batch)\n",
    "        total_loss.append(cost_t)\n",
    "        total_r2.append(r2)\n",
    "        end_timet=time.time()\n",
    "        print(\"batch_idx:\",it,\"loss:\",cost_t,\"R2:\",r2,\"time:\",end_timet)\n",
    "    loss_list.append(sum(total_loss)/len(total_loss))\n",
    "    r2_list.append(sum(total_r2)/len(total_r2))\n",
    "    print('Training [{:.0f}%]'.format(100. * (epoch + 1) / n_epochs),\"Loss:\", loss_list[-1], \"time:\",end_timet)\n",
    "    print(\"weight:\",weights)\n",
    "    \n",
    "    \n",
    "predictions = [QNN(weights, x) for x in X]\n",
    "\n",
    "train_R2 = R2(Y, predictions)\n",
    "train_MSE=metrics.mean_squared_error(Y,predictions)\n",
    "train_RMSE=train_MSE**(1/2)\n",
    "train_MAE=metrics.mean_absolute_error(Y,predictions)\n",
    "train_MAPE=metrics.mean_absolute_percentage_error(Y,predictions)\n",
    "\n",
    "print(\"train_MSE:\",train_MSE)\n",
    "print(\"train_RMSE:\",train_RMSE)\n",
    "print(\"train_MAE:\",train_MAE)\n",
    "print(\"train_MAPE:\",train_MAPE)\n",
    "print(\"train_R2:\",train_R2)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "plt.title(\"MSE value against epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot(range(len(loss_list)), np.log(loss_list))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90deb265-6d46-4ecd-882d-03a49eb3e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=xtv_test_log\n",
    "Y_test=ytv_test_log\n",
    "test_predictions = [QNN(weights, x) for x in X_test]\n",
    "\n",
    "test_R2 = R2(Y_test, test_predictions)\n",
    "test_MSE=metrics.mean_squared_error(Y_test,test_predictions)\n",
    "test_RMSE=test_MSE**(1/2)\n",
    "test_MAE=metrics.mean_absolute_error(Y_test,test_predictions)\n",
    "test_MAPE=metrics.mean_absolute_percentage_error(Y_test,test_predictions)\n",
    "\n",
    "print(\"test_MSE:\",test_MSE)\n",
    "print(\"test_RMSE:\",test_RMSE)\n",
    "print(\"test_MAE:\",test_MAE)\n",
    "print(\"test_MAPE:\",test_MAPE)\n",
    "print(\"test_R2:\",test_R2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
