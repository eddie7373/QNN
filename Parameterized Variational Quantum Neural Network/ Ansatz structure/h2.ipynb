{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586edb7a-4b3b-475e-8736-e85f04cf1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as npp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# set the random seed\n",
    "npp.random.seed(42)\n",
    "\n",
    "# create a device to execute the circuit on\n",
    "dev = qml.device(\"default.qubit\", wires=4)\n",
    "\n",
    "@qml.qnode(dev, diff_method=\"parameter-shift\", interface=\"autograd\")\n",
    "def circuit(params,inputs):\n",
    "    qml.RY(inputs[0], wires=0)\n",
    "    qml.RY(inputs[1], wires=1)\n",
    "    qml.RY(inputs[2], wires=2)\n",
    "    qml.RY(inputs[3], wires=3)\n",
    "    \n",
    "\n",
    "    qml.U3(params[0],params[1],params[2], wires=0)\n",
    "    qml.U3(params[3],params[4],params[5], wires=1)\n",
    "    qml.U3(params[6],params[7],params[8], wires=2)\n",
    "    qml.U3(params[9],params[10],params[11], wires=3)\n",
    "    \n",
    "\n",
    "    qml.broadcast(qml.CNOT, wires=[0, 1, 2,3], pattern=\"double_odd\")\n",
    "    '''\n",
    "    qml.RY(inputs[0]*1.5, wires=0)\n",
    "    qml.RY(inputs[1]*1.5, wires=1)\n",
    "    qml.RY(inputs[2]*1.5, wires=2)\n",
    "    qml.RY(inputs[3]*1.5, wires=3)\n",
    "'''\n",
    "    qml.U3(params[12],params[13],params[14], wires=0)\n",
    "    qml.U3(params[15],params[16],params[17], wires=1)\n",
    "    qml.U3(params[18],params[19],params[20], wires=2)\n",
    "    qml.U3(params[21],params[22],params[23], wires=3)\n",
    "\n",
    "    qml.broadcast(qml.CNOT, wires=[0, 1, 2,3], pattern=\"double_odd\")\n",
    "    \n",
    "    '''\n",
    "    qml.RY(inputs[0]*2, wires=0)\n",
    "    qml.RY(inputs[1]*2, wires=1)\n",
    "    qml.RY(inputs[2]*2, wires=2)\n",
    "    qml.RY(inputs[3]*2, wires=3)\n",
    "\n",
    "    qml.U3(params[24],params[25],params[26], wires=0)\n",
    "    qml.U3(params[27],params[28],params[29], wires=1)\n",
    "    qml.U3(params[30],params[31],params[32], wires=2)\n",
    "    qml.U3(params[33],params[34],params[35], wires=3)\n",
    "\n",
    "    qml.broadcast(qml.CNOT, wires=[0, 1, 2, 3], pattern=\"ring\")\n",
    "    '''\n",
    "    #return qml.expval(qml.PauliX(0) @ qml.PauliI(1)@ qml.PauliY(2)@ qml.PauliI(3))\n",
    "    return qml.expval(qml.PauliX(0) @  qml.PauliY(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21f8a3e-a4ab-4c9d-bcf5-73c1b9429ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "def norminv(x):\n",
    "    return ((1.0/math.sqrt(2.0*math.pi)) * math.exp(-x*x*0.5))\n",
    "\n",
    "def d1(S0, K, r, T, sigma, q):\n",
    "    deno = (sigma * math.sqrt(T))\n",
    "    if (deno==0):\n",
    "        return 0\n",
    "    logReturns = math.log(S0/float(K)) if ((S0/float(K)) > 0.0) else 0.0\n",
    "    return (float(logReturns) + (float(r) - float(q) + float(sigma)*float(sigma)*0.5)*float(T)) / float(deno)\n",
    "    \n",
    "def d2(S0, K, r, T, sigma, q):\n",
    "        return d1(S0, K, r, T, sigma, q)-sigma*math.sqrt(T)\n",
    "        \n",
    "def bsformula(callput, S0, K, r, T, sigma, q=0):\n",
    "    N = stats.norm.cdf\n",
    "                \n",
    "    def optionValueOfCall(S0, K, r, T, sigma, q):       \n",
    "        _d1 = d1(S0, K, r, T, sigma, q)\n",
    "        _d2 = d2(S0, K, r, T, sigma, q)\n",
    "        return S0*math.exp(-q*T)*N(_d1)- K*math.exp(-r*T)*N(_d2)\n",
    "      \n",
    "    def optionValueOfPut(S0, K, r, T, sigma, q):\n",
    "        _d1 = d1(S0, K, r, T, sigma, q)\n",
    "        _d2 = d2(S0, K, r, T, sigma, q)\n",
    "        return float(K)*math.exp(-float(r)*float(T))*N(-_d2) - float(S0)*math.exp(-float(q)*float(T))*N(-_d1)\n",
    "        \n",
    "    def delta(callput, S0, K, r, T, sigma, q):\n",
    "        _d1 = d1(S0, K, r, T, sigma, q)        \n",
    "        if callput.lower() == \"call\":            \n",
    "            return N(_d1) * math.exp(-q*T)\n",
    "        else:\n",
    "            return (N(_d1)-1)* math.exp(-q*T)\n",
    "    \n",
    "    def vega(S0, K, r, T, sigma, q):\n",
    "        _d1 = d1(S0, K, r, T, sigma, q)\n",
    "        return S0  * math.sqrt(T) * norminv(_d1)  * math.exp(-q*T)\n",
    "    \n",
    "    if callput.lower()==\"call\":\n",
    "        optionValue = optionValueOfCall(S0, K, r, T, sigma, q)\n",
    "    else:\n",
    "        optionValue = optionValueOfPut(S0, K, r, T, sigma, q)\n",
    "        \n",
    "    _delta = delta(callput, S0, K, r, T, sigma, q)\n",
    "    _vega = vega(S0, K, r, T, sigma, q)\n",
    "    \n",
    "    return (optionValue, _delta, _vega)\n",
    "\n",
    "def bsm_iv_generator(num_sample = 100,tao_bound=[0.01,2.0],  sigma_bound=[0.01,2.0], \n",
    "                     money_bound=[0.3,3.0], rr_bound=[0.01,0.2], callput='call', seed=42):\n",
    "    \n",
    "    # input parameters: when callput is not in 'call' or 'put', randomly generate the option price followed by root-finding methods to\n",
    "    # compute the corresponding implied vol\n",
    "    # return: X_input = [time,stock,rr, dividen, option_price]. Y_outpu  = volatility \n",
    "    np.random.seed(seed)\n",
    "    tao_min,tao_max = tao_bound[0],tao_bound[1]\n",
    "    \n",
    "    sigma_min, sigma_max = sigma_bound[0],sigma_bound[1]\n",
    "    moneyness_min,moneyness_max = money_bound[0],money_bound[1]\n",
    "    rr_min,rr_max = rr_bound[0],rr_bound[1]\n",
    "   \n",
    "    \n",
    "\n",
    "    num_sample = int(num_sample)\n",
    "    xx = np.zeros([num_sample,4],dtype='float')\n",
    "    \n",
    "   \n",
    "    xx[:,0] = np.random.uniform(sigma_min, sigma_max,xx.shape[0])\n",
    "    xx[:,1] = np.random.uniform(tao_min,tao_max,xx.shape[0])\n",
    "    xx[:,2] = np.random.uniform(moneyness_min,moneyness_max,xx.shape[0])\n",
    "    xx[:,3] = np.random.uniform(rr_min,rr_max,xx.shape[0])\n",
    "   \n",
    "    \n",
    "   \n",
    "    strike=1.0 #fixed strike\n",
    "    #callput = 'call' # call option\n",
    "    v = np.zeros(xx.shape[0]) # option value\n",
    "    k = np.ones(xx.shape[0]) # strike price, just in order to match the shape of v\n",
    "    \n",
    "    if callput in ['call','put']:        \n",
    "        for i in range(0,xx.shape[0]):        \n",
    "            sigma, T, S0, interest = xx[i,0],xx[i,1],xx[i,2],xx[i,3]\n",
    "            ## use the Black-Schole function in compfin.py\n",
    "            v[i] = bsformula(callput, S0, strike, interest, T, sigma)[0]              \n",
    "            \n",
    "  \n",
    "    v= v.reshape(xx.shape[0],1)     \n",
    "    xx_sample = np.concatenate((xx,v),axis=1) #sigma, time, s, r, v\n",
    "    \n",
    "    \n",
    "    X_input   = xx_sample[:,1:]   # time,stock,rr, option_price\n",
    "    Y_output  =  xx_sample[:,0] # sigma -implied volatility is the predictive variable.\n",
    "  \n",
    "    return X_input,Y_output\n",
    "#  log-transformation of the option value\n",
    "def logscale_vol(x_train_dat,y_train_dat,otm_lower=0.0000001):\n",
    "   # input data: x_train_dat = [time,stock,rr, option_price], y_train_dat = sigma  \n",
    "   \n",
    "    xtv_train_log=x_train_dat.copy()    \n",
    "    ytv_train_log =y_train_dat.copy()\n",
    "    \n",
    "    \n",
    "    #v_lower[v_lower<0.0]=0.0 # V=max(S-E*exp(-rt),0)  \n",
    "    xintrinsic_train=xtv_train_log[:,1]-1.0*np.exp(-1.0*xtv_train_log[:,2]*xtv_train_log[:,0])\n",
    "    xintrinsic_train[xintrinsic_train<0.0]=0.0 ## \\tilde{V} = max(S-E*exp(-rt),0)\n",
    "    xtv_train_log[:,-1] = xtv_train_log[:,-1] -xintrinsic_train\n",
    "    \n",
    "    ## remove intrisinc values below the threshold (otm_lower \\approx machine pricision)  \n",
    "   \n",
    "    ytv_train_log = ytv_train_log[~np.less(xtv_train_log[:,-1],otm_lower)]\n",
    "    xtv_train_log = xtv_train_log[~np.less(xtv_train_log[:,-1],otm_lower),:]\n",
    "    xtv_train_log[:,-1]=np.log(xtv_train_log[:,-1])\n",
    "\n",
    "    return xtv_train_log,ytv_train_log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce82ab6f-af86-47ed-9c2c-ddf97780cd0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maturity time  range:\n",
      "0.5005522117123602 0.5986886936600517\n",
      "Stock price  range:\n",
      "0.9802780852212476 1.0188712833088385\n",
      "interest rate  range:\n",
      "0.030829391446392806 0.07928252270553005\n",
      "option value  range:\n",
      "0.09746900812834913 0.21773104905666268\n",
      "sigma range:\n",
      "0.308233797718321 0.6879639408647977\n",
      "(50, 4)\n",
      "maturity time  range:\n",
      "0.5005522117123602 0.5986886936600517\n",
      "Stock price  range:\n",
      "0.9802780852212476 1.0188712833088385\n",
      "interest rate  range:\n",
      "0.030829391446392806 0.07928252270553005\n",
      "time option-value  range:\n",
      "-2.614763769983766 -1.672877716398405\n",
      "sigma range:\n",
      "0.308233797718321 0.6879639408647977\n",
      "(40, 4)\n",
      "Parameters: [0.64203165 0.08413996 0.16162871 0.89855419 0.60642906 0.00919705\n",
      " 0.10147154 0.66350177 0.00506158 0.16080805 0.54873379 0.6918952\n",
      " 0.65196126 0.22426931 0.71217922 0.23724909 0.3253997  0.74649141\n",
      " 0.6496329  0.84922341 0.65761289 0.5683086  0.09367477 0.3677158 ]\n",
      "inputs: [0.26520237 0.24398964 0.97301055 0.39309772]\n",
      "Expectation value: 0.3396801896314884\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xin_input,Yin_output=bsm_iv_generator(num_sample = 50,tao_bound=[0.5,0.6],  sigma_bound=[0.3,0.7], \n",
    "                                      money_bound=[0.98,1.02], rr_bound=[0.03,0.08],callput='call')\n",
    "\n",
    "#check the data value range on each dimension\n",
    "## xin = [maturity time, Stock price, interest rate, dividend, option value]\n",
    "\n",
    "xlabel =  ['maturity time', 'Stock price', 'interest rate','option value']\n",
    "for i in range(0, len(xlabel)):\n",
    "    print(xlabel[i]+'  range:')\n",
    "    print(np.min(Xin_input[:,i]),np.max(Xin_input[:,i]))\n",
    "    \n",
    "print('sigma range:')\n",
    "print(np.min(Yin_output),np.max(Yin_output))\n",
    "print(np.shape(Xin_input))\n",
    "\n",
    "# generate and shuffle the data set into training and test part\n",
    "xtv_train_log_all,ytv_train_log_all=logscale_vol(Xin_input,Yin_output,otm_lower=1e-4)\n",
    "'''\n",
    "for i in range(4):\n",
    "    xtv_train_log_all[:,i]= min_max_normalization(xtv_train_log_all[:,i])\n",
    "'''\n",
    "#ytv_train_log_all=ytv_train_log_all/2\n",
    "xtv_train_log,xtv_test_log, ytv_train_log, ytv_test_log   = train_test_split(xtv_train_log_all,ytv_train_log_all,test_size=0.2,random_state=42)\n",
    "\n",
    "xlabel =  ['maturity time', 'Stock price', 'interest rate','time option-value']\n",
    "for i in range(0, len(xlabel)):\n",
    "    print(xlabel[i]+'  range:')\n",
    "    print(np.min(xtv_train_log_all[:,i]),np.max(xtv_train_log_all[:,i]))\n",
    "    \n",
    "print('sigma range:')\n",
    "print(np.min(ytv_train_log),np.max(ytv_train_log))\n",
    "## how many samples after cleaning\n",
    "print(np.shape(xtv_train_log))\n",
    "\n",
    "\n",
    "params = npp.random.random([24], requires_grad=True)\n",
    "inputs = npp.random.random([4], requires_grad=True)\n",
    "print(\"Parameters:\", params)\n",
    "print(\"inputs:\", inputs)\n",
    "print(\"Expectation value:\", circuit(params,inputs))\n",
    "\n",
    "\n",
    "qnode = qml.QNode(circuit, dev)\n",
    "qml.draw_mpl(circuit, decimals=1, style=\"sketch\")(params,inputs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc5c5020-7fea-46b3-85c1-c67a90814673",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.12155535985148175\n",
      "[-1.21555360e-01 -3.05950672e-01 -9.69033591e-02 -3.41854617e-01\n",
      "  1.21564204e-01  3.29344471e-02 -9.78444886e-02 -1.38804798e-02\n",
      " -1.30004951e-02 -2.77555756e-17  8.32667268e-17 -1.11022302e-16\n",
      "  9.18027645e-02 -4.00878481e-01 -3.05950672e-01 -9.34243749e-02\n",
      "  0.00000000e+00  1.21564204e-01  2.42082185e-01 -1.10331211e-01\n",
      " -2.16698108e-01  1.38777878e-16  5.55111512e-17  0.00000000e+00]\n",
      "[-0.12155536 -0.30595067 -0.09690336 -0.34185462  0.1215642   0.03293445\n",
      " -0.09784449 -0.01388048 -0.0130005   0.          0.          0.\n",
      "  0.09180276 -0.40087848 -0.30595067 -0.09342437  0.          0.1215642\n",
      "  0.24208218 -0.11033121 -0.21669811  0.          0.          0.        ]\n",
      "[-0.12155536 -0.30595067 -0.09690336 -0.34185462  0.1215642   0.03293445\n",
      " -0.09784449 -0.01388048 -0.0130005   0.          0.          0.\n",
      "  0.09180276 -0.40087848 -0.30595067 -0.09342437  0.          0.1215642\n",
      "  0.24208218 -0.11033121 -0.21669811  0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "def parameter_shift_term(qnode,params,inputs, i):\n",
    "    shifted = params.copy()\n",
    "    shifted[i] += np.pi/2\n",
    "    forward = qnode(shifted,inputs)  # forward evaluation\n",
    "\n",
    "    shifted[i] -= np.pi\n",
    "    backward = qnode(shifted,inputs) # backward evaluation\n",
    "\n",
    "    return 0.5 * (forward - backward)\n",
    "\n",
    "# gradient with respect to the first parameter\n",
    "print(parameter_shift_term(circuit,params,inputs, 0))\n",
    "\n",
    "\n",
    "def parameter_shift(qnode, params,inputs):\n",
    "    gradients = np.zeros([len(params)])\n",
    "\n",
    "    for i in range(len(params)):\n",
    "        gradients[i] = parameter_shift_term(qnode,params,inputs, i)\n",
    "\n",
    "    return gradients\n",
    "\n",
    "print(parameter_shift(circuit, params,inputs))\n",
    "\n",
    "grad_function = qml.grad(circuit)\n",
    "print(grad_function(params,inputs)[0])\n",
    "\n",
    "\n",
    "print(qml.gradients.param_shift(circuit)(params,inputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9715830-fa60-4d48-bbc4-27dc40dbdf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from itertools import chain\n",
    "import time\n",
    "def QNN(weights, angles):\n",
    "    return circuit(weights, angles)\n",
    "\n",
    "def cost(weights, features, labels):\n",
    "    predictions = [QNN(weights, f) for f in features]\n",
    "    \n",
    "    return square_loss(labels, predictions)\n",
    "\n",
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "def R2(labels, predictions):\n",
    "\n",
    "    r2 = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        r2 = r2 + metrics.r2_score(labels, predictions)\n",
    "    r2 = r2 / len(labels)\n",
    "\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48fdbeb0-8d5d-419e-be05-d99d3936b2fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X=xtv_train_log\n",
    "Y=ytv_train_log\n",
    "weights_init = npp.random.random([24], requires_grad=True)\n",
    "opt = qml.AdamOptimizer(0.01)\n",
    "batch_size = 20\n",
    "batches = len (X) // batch_size\n",
    "X_batches = npp.array_split(npp.arange(len(X)) , batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2218b3c-8997-4af9-84d6-318bffadf9d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.08434648943736736 R2: -5.239856506882225 time: 1703256961.2020638\n",
      "batch_idx: 1 loss: 0.08251056586622843 R2: -4.362112673739405 time: 1703256966.9619806\n",
      "Training [0%] Loss: 0.0834285276517979 time: 1703256966.9619806\n",
      "weight: [0.62204288 0.06414    0.14162627 0.87857848 0.62642314 0.02919039\n",
      " 0.0814822  0.68344618 0.02501439 0.16080805 0.54873379 0.6918952\n",
      " 0.63198525 0.20427085 0.69217926 0.21727969 0.3253997  0.76648549\n",
      " 0.66963305 0.86922618 0.6776207  0.5683086  0.09367477 0.3677158 ]\n",
      "epoch 2\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.06044827648086527 R2: -3.5499535582105457 time: 1703256972.757222\n",
      "batch_idx: 1 loss: 0.05974796448327187 R2: -2.812396427059209 time: 1703256978.516821\n",
      "Training [1%] Loss: 0.06009812048206857 time: 1703256978.516821\n",
      "weight: [0.60226467 0.04424139 0.12169912 0.85880768 0.64625951 0.04902505\n",
      " 0.06160994 0.70295659 0.04477388 0.16080805 0.54873379 0.6918952\n",
      " 0.61235504 0.18438835 0.67228065 0.19772793 0.3253997  0.78632185\n",
      " 0.68954861 0.88914926 0.69754421 0.5683086  0.09367477 0.3677158 ]\n",
      "epoch 3\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.04089174467169903 R2: -2.1604500284894965 time: 1703256984.3442783\n",
      "batch_idx: 1 loss: 0.04179368729080468 R2: -1.597377200122722 time: 1703256990.1119723\n",
      "Training [1%] Loss: 0.04134271598125186 time: 1703256990.1119723\n",
      "weight: [0.58302349 0.02464629 0.10202486 0.83958161 0.66567402 0.06843456\n",
      " 0.04205355 0.72151302 0.06405652 0.16080805 0.54873379 0.6918952\n",
      " 0.59371025 0.16483707 0.65268555 0.17918402 0.3253997  0.80573637\n",
      " 0.7092129  0.90882363 0.71718935 0.5683086  0.09367477 0.3677158 ]\n",
      "epoch 4\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.02648264402324226 R2: -1.127988925055832 time: 1703256995.8071165\n",
      "batch_idx: 1 loss: 0.029121066920218746 R2: -0.7490655515790374 time: 1703257001.5320697\n",
      "Training [1%] Loss: 0.027801855471730502 time: 1703257001.5320697\n",
      "weight: [0.5647858  0.00569917 0.08292685 0.82139949 0.68426495 0.08701386\n",
      " 0.02313167 0.7385388  0.08244919 0.16080805 0.54873379 0.6918952\n",
      " 0.57697101 0.14597174 0.63373843 0.1623905  0.3253997  0.8243273\n",
      " 0.72833262 0.92794775 0.73621173 0.5683086  0.09367477 0.3677158 ]\n",
      "epoch 5\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01729514766939353 R2: -0.4587438423102072 time: 1703257007.1372852\n",
      "batch_idx: 1 loss: 0.021471610934689002 R2: -0.24834583062658777 time: 1703257012.7674131\n",
      "Training [2%] Loss: 0.019383379302041266 time: 1703257012.7674131\n",
      "weight: [ 0.54808572 -0.01210779  0.06489135  0.80483298  0.70152528  0.1042528\n",
      "  0.0052985   0.75356954  0.09945316  0.16080805  0.54873379  0.6918952\n",
      "  0.56292845  0.12828538  0.61593147  0.14797571  0.3253997   0.84158763\n",
      "  0.74645846  0.94606349  0.75412005  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 6\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.012702770179618461 R2: -0.11048187242373686 time: 1703257018.4920025\n",
      "batch_idx: 1 loss: 0.01797263255090293 R2: -0.03328265360519267 time: 1703257024.0468364\n",
      "Training [2%] Loss: 0.015337701365260695 time: 1703257024.0468364\n",
      "weight: [ 0.5333968  -0.02817504  0.0485406   0.7903522   0.71691951  0.11961644\n",
      " -0.01086811  0.76639312  0.11457605  0.16080805  0.54873379  0.6918952\n",
      "  0.55170307  0.11236314  0.59986421  0.13614098  0.3253997   0.85698186\n",
      "  0.76297913  0.96255529  0.77030351  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 7\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011572246269477759 R2: -0.00584795508159508 time: 1703257029.6139739\n",
      "batch_idx: 1 loss: 0.017373457391253358 R2: -0.015731570947501217 time: 1703257035.3070092\n",
      "Training [2%] Loss: 0.01447285183036556 time: 1703257035.3070092\n",
      "weight: [ 0.52100866 -0.0418971   0.03453593  0.77815814  0.72998577  0.1326508\n",
      " -0.02473337  0.77706803  0.12744339  0.16080805  0.54873379  0.6918952\n",
      "  0.54260872  0.0987811   0.58614216  0.12655749  0.3253997   0.87004811\n",
      "  0.77717608  0.97670597  0.78409679  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 8\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.012534044861476454 R2: -0.05092388831350214 time: 1703257041.2571108\n",
      "batch_idx: 1 loss: 0.018324602847922977 R2: -0.10096111757813009 time: 1703257047.0519192\n",
      "Training [3%] Loss: 0.015429323854699716 time: 1703257047.0519192\n",
      "weight: [ 0.51097368 -0.05279941  0.02342602  0.76813965  0.74042755  0.14307485\n",
      " -0.0357347   0.78583497  0.13787558  0.16080805  0.54873379  0.6918952\n",
      "  0.53444469  0.08797804  0.57523984  0.11852732  0.3253997   0.88048989\n",
      "  0.78836325  0.98783431  0.79490138  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 9\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.014273818705912438 R2: -0.15523923498689474 time: 1703257053.042176\n",
      "batch_idx: 1 loss: 0.019656877188673014 R2: -0.20723309588914604 time: 1703257059.0688133\n",
      "Training [3%] Loss: 0.016965347947292726 time: 1703257059.0688133\n",
      "weight: [ 0.50313251 -0.0606475   0.01550961  0.75996908  0.74816507  0.15082656\n",
      " -0.043528    0.79300275  0.14589756  0.16080805  0.54873379  0.6918952\n",
      "  0.52587335  0.0801583   0.56739175  0.11124696  0.3253997   0.88822741\n",
      "  0.79607369  0.99548289  0.80235041  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 10\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.015790873540871435 R2: -0.2495775714089597 time: 1703257064.761927\n",
      "batch_idx: 1 loss: 0.020589401013292166 R2: -0.28031419198748486 time: 1703257070.5028992\n",
      "Training [3%] Loss: 0.0181901372770818 time: 1703257070.5028992\n",
      "weight: [ 0.49718106 -0.06547668  0.01078405  0.7532505   0.75333465  0.15605487\n",
      " -0.04807212  0.79886682  0.15169459  0.16080805  0.54873379  0.6918952\n",
      "  0.51575451  0.07526941  0.56256258  0.10403232  0.3253997   0.89339699\n",
      "  0.80019078  0.99955457  0.80643419  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 11\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.016538882269011644 R2: -0.2957665348566518 time: 1703257076.3487477\n",
      "batch_idx: 1 loss: 0.02079286098335604 R2: -0.2977763331247374 time: 1703257082.2792466\n",
      "Training [4%] Loss: 0.01866587162618384 time: 1703257082.2792466\n",
      "weight: [ 0.49274127 -0.06754736  0.00898726  0.74763723  0.75624636  0.15907273\n",
      " -0.04961249  0.80367238  0.15555088  0.16080805  0.54873379  0.6918952\n",
      "  0.50341189  0.07304685  0.5604919   0.09645056  0.3253997   0.8963087\n",
      "  0.80095139  1.00031681  0.80750332  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 12\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01640821846718014 R2: -0.2855920854505468 time: 1703257088.1521962\n",
      "batch_idx: 1 loss: 0.020310370600347495 R2: -0.2633397823530484 time: 1703257093.9172733\n",
      "Training [4%] Loss: 0.018359294533763816 time: 1703257093.9172733\n",
      "weight: [ 0.48942058 -0.06727022  0.00968308  0.74288677  0.75732199  0.16029507\n",
      " -0.04859858  0.8076093   0.15779846  0.16080805  0.54873379  0.6918952\n",
      "  0.48872191  0.07308531  0.56076904  0.08835136  0.3253997   0.89738434\n",
      "  0.79885022  0.99829933  0.80616254  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 13\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01559595522010374 R2: -0.23192442267624633 time: 1703257099.6669824\n",
      "batch_idx: 1 loss: 0.019405527301560546 R2: -0.19618931288045877 time: 1703257105.5030816\n",
      "Training [4%] Loss: 0.017500741260832144 time: 1703257105.5030816\n",
      "weight: [ 0.48685938 -0.06513957  0.01234012  0.73885872  0.75703244  0.16017931\n",
      " -0.04559349  0.81082321  0.1587842   0.16080805  0.54873379  0.6918952\n",
      "  0.47198524  0.07490221  0.56289968  0.07982454  0.3253997   0.89709479\n",
      "  0.79452496  0.99416892  0.80313307  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 14\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.014442991785228778 R2: -0.15751299299411142 time: 1703257111.2722645\n",
      "batch_idx: 1 loss: 0.018408809807390646 R2: -0.12022582505788075 time: 1703257117.0845013\n",
      "Training [5%] Loss: 0.016425900796309713 time: 1703257117.0845013\n",
      "weight: [ 0.48476706 -0.06168798  0.0163849   0.73547738  0.75584388  0.15917555\n",
      " -0.04120745  0.81343127  0.15885075  0.16080805  0.54873379  0.6918952\n",
      "  0.4537379   0.07798196  0.56635128  0.07112605  0.3253997   0.89590623\n",
      "  0.78867179  0.98863529  0.79913456  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 15\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01329435811960385 R2: -0.08535039027645816 time: 1703257123.1043959\n",
      "batch_idx: 1 loss: 0.017601099106314556 R2: -0.05592548557088195 time: 1703257129.0021641\n",
      "Training [5%] Loss: 0.015447728612959203 time: 1703257129.0021641\n",
      "weight: [ 0.48294457 -0.05745764  0.02123358  0.73268317  0.7541753   0.15768791\n",
      " -0.03605633  0.8155369   0.15832481  0.16080805  0.54873379  0.6918952\n",
      "  0.4346654   0.0818048   0.57058162  0.06260282  0.3253997   0.89423765\n",
      "  0.78199578  0.98239227  0.79479449  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 16\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.012405263923091062 R2: -0.032193189307000214 time: 1703257134.6669438\n",
      "batch_idx: 1 loss: 0.017145357685624803 R2: -0.015555644279626524 time: 1703257140.9520578\n",
      "Training [5%] Loss: 0.014775310804357933 time: 1703257140.9520578\n",
      "weight: [ 0.48129158 -0.05297835  0.0263147   0.730393    0.75236857  0.1560473\n",
      " -0.03073269  0.81723992  0.15750726  0.16080805  0.54873379  0.6918952\n",
      "  0.41564203  0.08586932  0.57506091  0.05462757  0.3253997   0.89243092\n",
      "  0.77517844  0.97607842  0.79058302  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 17\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011895294253138742 R2: -0.005418258529106934 time: 1703257147.1921096\n",
      "batch_idx: 1 loss: 0.017064214133325907 R2: -0.0015819792143567657 time: 1703257153.2370908\n",
      "Training [6%] Loss: 0.014479754193232323 time: 1703257153.2370908\n",
      "weight: [ 0.47979791 -0.04874279  0.03109461  0.7284826   0.75067268  0.15449708\n",
      " -0.02577671  0.81864035  0.15666243  0.16080805  0.54873379  0.6918952\n",
      "  0.3978263   0.08971651  0.57929647  0.04754486  0.3253997   0.89073502\n",
      "  0.76884475  0.97024321  0.78678109  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 18\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011745683425217281 R2: -0.0028715068817692924 time: 1703257159.3177345\n",
      "batch_idx: 1 loss: 0.017257719189984713 R2: -0.007858353333093815 time: 1703257165.0989003\n",
      "Training [6%] Loss: 0.014501701307600997 time: 1703257165.0989003\n",
      "weight: [ 0.47851947 -0.0451744   0.03511215  0.7267945   0.74924314  0.15319221\n",
      " -0.02164044  0.81983571  0.15600623  0.16080805  0.54873379  0.6918952\n",
      "  0.38267848  0.09295856  0.58286486  0.0416279   0.3253997   0.88930548\n",
      "  0.76352092  0.96531549  0.78349028  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 19\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01183348901690669 R2: -0.015292990624987901 time: 1703257171.1327777\n",
      "batch_idx: 1 loss: 0.017555315659611794 R2: -0.023171004076175228 time: 1703257177.0569098\n",
      "Training [6%] Loss: 0.014694402338259242 time: 1703257177.0569098\n",
      "weight: [ 0.4775414  -0.04258988  0.03802096  0.72516609  0.74815435  0.15221014\n",
      " -0.01864767  0.82091324  0.15569469  0.16080805  0.54873379  0.6918952\n",
      "  0.37170348  0.09531034  0.58544938  0.03704745  0.3253997   0.8882167\n",
      "  0.7595872   0.96157879  0.78068006  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 20\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011994808854524901 R2: -0.030785302947531124 time: 1703257182.9469347\n",
      "batch_idx: 1 loss: 0.017789910672754535 R2: -0.03629439216083741 time: 1703257189.0019877\n",
      "Training [7%] Loss: 0.014892359763639718 time: 1703257189.0019877\n",
      "weight: [ 0.47693537 -0.04116613  0.03962796  0.72346509  0.74741968  0.15156868\n",
      " -0.01696146  0.8219405   0.1558162   0.16080805  0.54873379  0.6918952\n",
      "  0.36589148  0.09661527  0.58687313  0.03385544  0.3253997   0.88748203\n",
      "  0.75724073  0.95916058  0.77825332  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 21\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.012097409822426505 R2: -0.039943263616278424 time: 1703257195.1573482\n",
      "batch_idx: 1 loss: 0.017863926795169256 R2: -0.040511151800495915 time: 1703257201.3418505\n",
      "Training [7%] Loss: 0.01498066830879788 time: 1703257201.3418505\n",
      "weight: [ 0.4767249  -0.04092525  0.03991149  0.72161716  0.74701277  0.15124543\n",
      " -0.01657424  0.82295862  0.15639057  0.16080805  0.54873379  0.6918952\n",
      "  0.36528614  0.09685535  0.587114    0.03198589  0.3253997   0.88707512\n",
      "  0.7564846   0.95803971  0.77610598  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 22\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.012087417564539321 R2: -0.03912765467280166 time: 1703257207.4594922\n",
      "batch_idx: 1 loss: 0.0177719169869775 R2: -0.03519564297938271 time: 1703257213.8119748\n",
      "Training [7%] Loss: 0.01492966727575841 time: 1703257213.8119748\n",
      "weight: [ 0.47687211 -0.04174566  0.03900933  0.71961507  0.74688484  0.15119402\n",
      " -0.0173276   0.82398111  0.15737622  0.16080805  0.54873379  0.6918952\n",
      "  0.36908657  0.09613976  0.5862936   0.03127209  0.3253997   0.88694718\n",
      "  0.75714837  0.95806989  0.77416329  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 23\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011987558671253843 R2: -0.03028713418159068 time: 1703257219.8815436\n",
      "batch_idx: 1 loss: 0.017575486364257098 R2: -0.024160013395623015 time: 1703257226.3828855\n",
      "Training [8%] Loss: 0.01478152251775547 time: 1703257226.3828855\n",
      "weight: [ 0.47728835 -0.04339555  0.03718095  0.71750795  0.74697572  0.15135513\n",
      " -0.0189551   0.82499871  0.15868421  0.16080805  0.54873379  0.6918952\n",
      "  0.37605871  0.09467536  0.58464371  0.03147489  0.3253997   0.88703806\n",
      "  0.75893116  0.95901247  0.77238788  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 24\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011860606121875696 R2: -0.01831441078462337 time: 1703257232.946312\n",
      "batch_idx: 1 loss: 0.01735746441759705 R2: -0.012581937830009826 time: 1703257239.527346\n",
      "Training [8%] Loss: 0.014609035269736373 time: 1703257239.527346\n",
      "weight: [ 0.47785916 -0.04557769  0.03475721  0.71537758  0.7472196   0.15166285\n",
      " -0.0211349   0.82598801  0.16019588  0.16080805  0.54873379  0.6918952\n",
      "  0.38486425  0.09272785  0.58246157  0.03231538  0.3253997   0.88728195\n",
      "  0.76145371  0.96057195  0.77076741  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 25\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011767374037669719 R2: -0.008022993597747385 time: 1703257246.092289\n",
      "batch_idx: 1 loss: 0.017182891557912288 R2: -0.004393014109615345 time: 1703257252.0971217\n",
      "Training [8%] Loss: 0.014475132797791003 time: 1703257252.0971217\n",
      "weight: [ 0.47847116 -0.04797535  0.03208941  0.71331182  0.74754825  0.15204876\n",
      " -0.0235405   0.82692092  0.16178158  0.16080805  0.54873379  0.6918952\n",
      "  0.39422955  0.09058167  0.58006391  0.03350725  0.3253997   0.8876106\n",
      "  0.76430994  0.96243101  0.76929466  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 26\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011739509820792906 R2: -0.0021714243920789578 time: 1703257258.12207\n",
      "batch_idx: 1 loss: 0.01708016580544987 R2: -0.0010437902693778423 time: 1703257264.0319667\n",
      "Training [9%] Loss: 0.014409837813121388 time: 1703257264.0319667\n",
      "weight: [ 0.47903226 -0.0502929   0.02950471  0.71138344  0.74789456  0.15244613\n",
      " -0.0258821   0.82777252  0.16331792  0.16080805  0.54873379  0.6918952\n",
      "  0.40304257  0.08850388  0.57774635  0.03478565  0.3253997   0.8879569\n",
      "  0.76711393  0.96428417  0.76795218  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 27\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011773284731252488 R2: -0.0009705819382488379 time: 1703257269.8419154\n",
      "batch_idx: 1 loss: 0.01704282998561577 R2: -0.0016765182699085557 time: 1703257275.789241\n",
      "Training [9%] Loss: 0.014408057358434128 time: 1703257275.789241\n",
      "weight: [ 0.47948109 -0.05228745  0.02727147  0.70963881  0.74819772  0.15279526\n",
      " -0.0279349   0.82852587  0.16470198  0.16080805  0.54873379  0.6918952\n",
      "  0.4104339   0.08671536  0.5757518   0.03593084  0.3253997   0.88826007\n",
      "  0.76953994  0.96586859  0.76670766  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 28\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011840190450929645 R2: -0.00278358918360877 time: 1703257281.7197936\n",
      "batch_idx: 1 loss: 0.017044453014228775 R2: -0.004182494793954827 time: 1703257287.526473\n",
      "Training [9%] Loss: 0.01444232173257921 time: 1703257287.526473\n",
      "weight: [ 0.4797866  -0.05378943  0.02557689  0.70809635  0.74840948  0.15304947\n",
      " -0.02955313  0.82917366  0.16586121  0.16080805  0.54873379  0.6918952\n",
      "  0.41583686  0.08537159  0.57424983  0.03678468  0.3253997   0.88847183\n",
      "  0.77135254  0.96698938  0.76551966  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 29\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011904864580244503 R2: -0.005357554981649804 time: 1703257293.4779058\n",
      "batch_idx: 1 loss: 0.01705638594863804 R2: -0.0064387679416322285 time: 1703257299.4270322\n",
      "Training [10%] Loss: 0.014480625264441271 time: 1703257299.4270322\n",
      "weight: [ 0.47994151 -0.05471149  0.0245174   0.70675147  0.74849972  0.15318024\n",
      " -0.03067144  0.82971764  0.16675834  0.16080805  0.54873379  0.6918952\n",
      "  0.41900362  0.08455364  0.57332777  0.03725879  0.3253997   0.88856206\n",
      "  0.77242349  0.96753566  0.76434871  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 30\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01194087526499212 R2: -0.006923811313891193 time: 1703257305.2068903\n",
      "batch_idx: 1 loss: 0.017060331640950117 R2: -0.007170663945693212 time: 1703257310.853941\n",
      "Training [10%] Loss: 0.01450060345297112 time: 1703257310.853941\n",
      "weight: [ 0.47995424 -0.05504663  0.02410084  0.705584    0.7484595   0.15317985\n",
      " -0.03129639  0.8301668   0.16739146  0.16080805  0.54873379  0.6918952\n",
      "  0.41997325  0.08426917  0.57299263  0.03733389  0.3253997   0.88852184\n",
      "  0.77273383  0.96748495  0.76316753  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 31\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011938681690130317 R2: -0.006766016413086495 time: 1703257316.7919092\n",
      "batch_idx: 1 loss: 0.017052342985425635 R2: -0.0062101390985154215 time: 1703257322.8371568\n",
      "Training [10%] Loss: 0.014495512337777976 time: 1703257322.8371568\n",
      "weight: [ 0.47984262 -0.05485768  0.02425821  0.70456469  0.74830048  0.15306054\n",
      " -0.03149192  0.83053533  0.16778994  0.16080805  0.54873379  0.6918952\n",
      "  0.41901038  0.08446154  0.57318157  0.03705112  0.3253997   0.88836283\n",
      "  0.77236204  0.96689546  0.76196604  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 32\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011904883131194397 R2: -0.005190576688972159 time: 1703257328.8470538\n",
      "batch_idx: 1 loss: 0.01703930926972514 R2: -0.00422638581898771 time: 1703257334.737328\n",
      "Training [11%] Loss: 0.014472096200459769 time: 1703257334.737328\n",
      "weight: [ 0.47963041 -0.05426079  0.02486193  0.70365966  0.7480508   0.15285039\n",
      " -0.0313617   0.83084076  0.16800723  0.16080805  0.54873379  0.6918952\n",
      "  0.41653658  0.08502483  0.57377847  0.03649715  0.3253997   0.88811315\n",
      "  0.7714617   0.96588844  0.76074985  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 33\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011855454025067682 R2: -0.0030846832287270143 time: 1703257340.4421034\n",
      "batch_idx: 1 loss: 0.017031610137055907 R2: -0.0021857686735871074 time: 1703257346.2739196\n",
      "Training [11%] Loss: 0.014443532081061795 time: 1703257346.2739196\n",
      "weight: [ 0.47934585 -0.05340529  0.02574796  0.70283299  0.74774854  0.15258706\n",
      " -0.03103058  0.83110237  0.16811171  0.16080805  0.54873379  0.6918952\n",
      "  0.41306666  0.08582251  0.57463397  0.03578582  0.3253997   0.88781088\n",
      "  0.77023284  0.96462438  0.75953431  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 34\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011807187065117214 R2: -0.0013537593337304976 time: 1703257351.8869207\n",
      "batch_idx: 1 loss: 0.01703612843014489 R2: -0.0008336449175405658 time: 1703257357.6319\n",
      "Training [11%] Loss: 0.014421657747631052 time: 1703257357.6319\n",
      "weight: [ 0.47902047 -0.05245214  0.02673945  0.70204877  0.74743447  0.15231073\n",
      " -0.03062666  0.83133955  0.16817667  0.16080805  0.54873379  0.6918952\n",
      "  0.40915038  0.08670743  0.57558711  0.03503883  0.3253997   0.88749682\n",
      "  0.76889105  0.96327687  0.75833713  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 35\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011771175881711792 R2: -0.0005026044837328492 time: 1703257363.417167\n",
      "batch_idx: 1 loss: 0.017052655609677468 R2: -0.00041094851177714276 time: 1703257369.0470703\n",
      "Training [12%] Loss: 0.01441191574569463 time: 1703257369.0470703\n",
      "weight: [ 0.47868695 -0.05155297  0.02766979  0.70127333  0.74714562  0.15205791\n",
      " -0.03026532  0.83157029  0.16827072  0.16080805  0.54873379  0.6918952\n",
      "  0.4053152   0.08754124  0.57648628  0.03436805  0.3253997   0.88720797\n",
      "  0.76763807  0.96200798  0.75717257  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 36\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011750469609822407 R2: -0.0005055871355512664 time: 1703257374.7541559\n",
      "batch_idx: 1 loss: 0.017074635061328187 R2: -0.0006744375334131192 time: 1703257380.4868865\n",
      "Training [12%] Loss: 0.014412552335575297 time: 1703257380.4868865\n",
      "weight: [ 0.47837551 -0.0508317   0.02840276  0.70047822  0.74691057  0.15185688\n",
      " -0.03003646  0.8318095   0.16844954  0.16080805  0.54873379  0.6918952\n",
      "  0.40200842  0.08821123  0.57720756  0.03386155  0.3253997   0.88697292\n",
      "  0.76663704  0.96094798  0.75604871  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 37\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011741832133656902 R2: -0.0009574514402286294 time: 1703257386.2471063\n",
      "batch_idx: 1 loss: 0.017093049513815257 R2: -0.0011418614598979193 time: 1703257392.626442\n",
      "Training [12%] Loss: 0.01441744082373608 time: 1703257392.626442\n",
      "weight: [ 0.47810951 -0.05037101  0.02884744  0.69964306  0.74674695  0.15172518\n",
      " -0.02999637  0.83206767  0.16875006  0.16080805  0.54873379  0.6918952\n",
      "  0.39954445  0.08864249  0.57766824  0.03357459  0.3253997   0.8868093\n",
      "  0.76599504  0.96018168  0.75496747  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 38\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011739730569143823 R2: -0.0013701641671888254 time: 1703257398.3969946\n",
      "batch_idx: 1 loss: 0.017100953808565463 R2: -0.0013927232760893027 time: 1703257404.1121833\n",
      "Training [13%] Loss: 0.014420342188854643 time: 1703257404.1121833\n",
      "weight: [ 0.47790175 -0.05020554  0.0289658   0.69875756  0.74666086  0.15166898\n",
      " -0.03016495  0.8323501   0.16918772  0.16080805  0.54873379  0.6918952\n",
      "  0.3980693   0.08880396  0.57783372  0.03352631  0.3253997   0.8867232\n",
      "  0.76575504  0.95974233  0.75392618  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 39\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011740019402947542 R2: -0.0014361934331927895 time: 1703257409.9222305\n",
      "batch_idx: 1 loss: 0.01709625108986353 R2: -0.001260560806330746 time: 1703257415.6819506\n",
      "Training [13%] Loss: 0.014418135246405537 time: 1703257415.6819506\n",
      "weight: [ 0.47775269 -0.05032242  0.02877208  0.69782165  0.74664777  0.15168396\n",
      " -0.0305286   0.83265693  0.16975696  0.16080805  0.54873379  0.6918952\n",
      "  0.39755358  0.08870765  0.57771683  0.03370178  0.3253997   0.88671012\n",
      "  0.76589761  0.95961342  0.75291915  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 40\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011741515879129387 R2: -0.0011315054919236456 time: 1703257421.3971586\n",
      "batch_idx: 1 loss: 0.017081654120443154 R2: -0.000843111915501682 time: 1703257427.4243286\n",
      "Training [13%] Loss: 0.01441158499978627 time: 1703257427.4243286\n",
      "weight: [ 0.47765143 -0.0506691   0.02832424  0.69684365  0.74669451  0.15175728\n",
      " -0.03104775  0.83298406  0.17043475  0.16080805  0.54873379  0.6918952\n",
      "  0.3978169   0.08840158  0.57737016  0.03405873  0.3253997   0.88675686\n",
      "  0.76635159  0.9597371   0.75193858  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 41\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011745230662352927 R2: -0.0006436633381960988 time: 1703257433.1971045\n",
      "batch_idx: 1 loss: 0.017062463218976233 R2: -0.00037095274821696833 time: 1703257438.8323348\n",
      "Training [14%] Loss: 0.014403846940664579 time: 1703257438.8323348\n",
      "weight: [ 0.47757895 -0.05116626  0.02770967  0.69583708  0.74678195  0.15187021\n",
      " -0.03166742  0.8333246   0.17118628  0.16080805  0.54873379  0.6918952\n",
      "  0.39857634  0.08795798  0.576873    0.0345372   0.3253997   0.8868443\n",
      "  0.7670111   0.96002758  0.75097467  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 42\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011752390995389089 R2: -0.00020847234873122567 time: 1703257444.7222476\n",
      "batch_idx: 1 loss: 0.017043790397949454 R2: -4.250168511532593e-05 time: 1703257450.2920773\n",
      "Training [14%] Loss: 0.014398090696669271 time: 1703257450.2920773\n",
      "weight: [ 0.47751269 -0.05172351  0.02702793  0.69481689  0.74688817  0.15200132\n",
      " -0.03232845  0.83367068  0.17197183  0.16080805  0.54873379  0.6918952\n",
      "  0.39950618  0.08745909  0.57631575  0.03507071  0.3253997   0.88695051\n",
      "  0.76775578  0.96038705  0.75001586  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 43\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011762796032775948 R2: 3.2280356727532045e-05 time: 1703257456.0270267\n",
      "batch_idx: 1 loss: 0.017028729501218733 R2: 7.601578137073073e-05 time: 1703257461.787254\n",
      "Training [14%] Loss: 0.01439576276699734 time: 1703257461.787254\n",
      "weight: [ 0.47743105 -0.0522548   0.02637376  0.69379628  0.74699173  0.15212976\n",
      " -0.03297769  0.83401503  0.17275346  0.16080805  0.54873379  0.6918952\n",
      "  0.40029628  0.08698323  0.57578445  0.03559691  0.3253997   0.88705407\n",
      "  0.76847091  0.96072176  0.74904957  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 44\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011774457556538748 R2: 8.06724416195781e-05 time: 1703257467.4070244\n",
      "batch_idx: 1 loss: 0.017018061598431992 R2: 4.9160038208651e-05 time: 1703257473.1222494\n",
      "Training [15%] Loss: 0.01439625957748537 time: 1703257473.1222494\n",
      "weight: [ 0.47731688 -0.05269127  0.02582298  0.69278456  0.74707482  0.15223821\n",
      " -0.03357524  0.8343522   0.17350074  0.16080805  0.54873379  0.6918952\n",
      "  0.40069932  0.08659316  0.57534799  0.03606637  0.3253997   0.88713717\n",
      "  0.76906424  0.96095575  0.74806384  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 45\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01178449086406283 R2: 3.894270930460131e-05 time: 1703257478.8568983\n",
      "batch_idx: 1 loss: 0.01701111083419559 R2: -1.1921188349806044e-06 time: 1703257484.5172591\n",
      "Training [15%] Loss: 0.01439780084912921 time: 1703257484.5172591\n",
      "weight: [ 0.47715957 -0.05298962  0.02542322  0.69178632  0.74712561  0.15231525\n",
      " -0.0340984   0.83467915  0.1741945   0.16080805  0.54873379  0.6918952\n",
      "  0.40056116  0.08632834  0.57504964  0.03644827  0.3253997   0.88718796\n",
      "  0.76947743  0.9610404   0.74704923  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 46\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011790500576905146 R2: 2.2949842343411575e-05 time: 1703257490.4221842\n",
      "batch_idx: 1 loss: 0.0170068838875203 R2: 1.9694191118913018e-05 time: 1703257496.2477045\n",
      "Training [15%] Loss: 0.014398692232212722 time: 1703257496.2477045\n",
      "weight: [ 0.47695569 -0.05313551  0.02519027  0.69080168  0.7471395   0.15235651\n",
      " -0.03454203  0.8349954   0.17482837  0.16080805  0.54873379  0.6918952\n",
      "  0.3998322   0.08620191  0.57490374  0.03673244  0.3253997   0.88720185\n",
      "  0.76969074  0.96095865  0.74600053  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 47\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011791590621161656 R2: 9.314487164147067e-05 time: 1703257501.8370981\n",
      "batch_idx: 1 loss: 0.01700476042036498 R2: 0.0001343109427219824 time: 1703257507.6870725\n",
      "Training [16%] Loss: 0.014398175520763317 time: 1703257507.6870725\n",
      "weight: [ 0.47670867 -0.05314186  0.02510992  0.68982715  0.74711907  0.15236455\n",
      " -0.03491595  0.8353027   0.17540807  0.16080805  0.54873379  0.6918952\n",
      "  0.39856106  0.08620217  0.5748974   0.03692785  0.3253997   0.88718141\n",
      "  0.76972092  0.96072361  0.74491752  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 48\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011788524575683206 R2: 0.00023701834579281655 time: 1703257513.412147\n",
      "batch_idx: 1 loss: 0.017004502884544588 R2: 0.00030176196738562755 time: 1703257519.092347\n",
      "Training [16%] Loss: 0.014396513730113896 time: 1703257519.092347\n",
      "weight: [ 0.47642753 -0.05304304  0.02514436  0.68885689  0.74707271  0.1523476\n",
      " -0.03524044  0.8356044   0.17594883  0.16080805  0.54873379  0.6918952\n",
      "  0.39687423  0.08629784  0.57499621  0.03705812  0.3253997   0.88713506\n",
      "  0.76961353  0.96037233  0.74380451  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 49\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011783123934846635 R2: 0.0003987679944913447 time: 1703257524.8019812\n",
      "batch_idx: 1 loss: 0.017005841412555275 R2: 0.0004607154945789338 time: 1703257530.4423397\n",
      "Training [16%] Loss: 0.014394482673700956 time: 1703257530.4423397\n",
      "weight: [ 0.47612511 -0.05288635  0.02524157  0.68788405  0.74701248  0.15231734\n",
      " -0.03554071  0.83590474  0.17647159  0.16080805  0.54873379  0.6918952\n",
      "  0.39494633  0.08644594  0.57515291  0.03715521  0.3253997   0.88707483\n",
      "  0.76943132  0.95995636  0.74266896  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 50\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011777371406160192 R2: 0.0005260646440053973 time: 1703257536.4369307\n",
      "batch_idx: 1 loss: 0.017008091417153513 R2: 0.0005721873429291868 time: 1703257542.4093091\n",
      "Training [17%] Loss: 0.014392731411656853 time: 1703257542.4093091\n",
      "weight: [ 0.47581577 -0.05272229  0.025346    0.68690196  0.74695141  0.15228642\n",
      " -0.03584156  0.83620798  0.17699867  0.16080805  0.54873379  0.6918952\n",
      "  0.39296613  0.0866006   0.57531697  0.03725256  0.3253997   0.88701375\n",
      "  0.76924117  0.95953094  0.7415196   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 51\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011772721781191998 R2: 0.0006016282628996672 time: 1703257548.372097\n",
      "batch_idx: 1 loss: 0.017010130897599375 R2: 0.0006367733479746285 time: 1703257554.4219744\n",
      "Training [17%] Loss: 0.014391426339395687 time: 1703257554.4219744\n",
      "weight: [ 0.47551289 -0.05259546  0.02540857  0.68590522  0.74690107  0.15226604\n",
      " -0.03616312  0.83651776  0.17754969  0.16080805  0.54873379  0.6918952\n",
      "  0.3911032   0.08672136  0.57544379  0.03737898  0.3253997   0.88696342\n",
      "  0.76910194  0.95914479  0.74036453  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 52\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01176987594654651 R2: 0.0006445309442592029 time: 1703257560.6490083\n",
      "batch_idx: 1 loss: 0.017010752930231375 R2: 0.0006826953117626822 time: 1703257567.2242382\n",
      "Training [17%] Loss: 0.014390314438388943 time: 1703257567.2242382\n",
      "weight: [ 0.47522667 -0.05253749  0.0253945   0.68489028  0.7468697   0.15226414\n",
      " -0.03651815  0.83683657  0.17813851  0.16080805  0.54873379  0.6918952\n",
      "  0.38948064  0.08677956  0.57550177  0.03755418  0.3253997   0.88693205\n",
      "  0.76905511  0.95883242  0.73920972  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 53\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01176894919437985 R2: 0.0006883583107640812 time: 1703257574.5969222\n",
      "batch_idx: 1 loss: 0.017009144718031128 R2: 0.0007399064814777034 time: 1703257581.5619445\n",
      "Training [18%] Loss: 0.014389046956205489 time: 1703257581.5619445\n",
      "weight: [ 0.47496247 -0.05256304  0.02528766  0.68385576  0.74686114  0.15228443\n",
      " -0.03691117  0.83716552  0.17877151  0.16080805  0.54873379  0.6918952\n",
      "  0.38815832  0.08676197  0.57547622  0.03778666  0.3253997   0.88692349\n",
      "  0.76911973  0.95860975  0.73805808  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 54\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011769770924019794 R2: 0.0007560584686987504 time: 1703257588.915871\n",
      "batch_idx: 1 loss: 0.017005187357608316 R2: 0.0008204767071143815 time: 1703257595.3719375\n",
      "Training [18%] Loss: 0.014387479140814055 time: 1703257595.3719375\n",
      "weight: [ 0.47472024 -0.05266944  0.02509103  0.6828022   0.74687468  0.1523262\n",
      " -0.03733943  0.83750435  0.17944741  0.16080805  0.54873379  0.6918952\n",
      "  0.38712934  0.08667107  0.57536982  0.03807384  0.3253997   0.88693703\n",
      "  0.76929195  0.95847367  0.73690916  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 55\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011772076334288673 R2: 0.0008484948973843665 time: 1703257602.2071717\n",
      "batch_idx: 1 loss: 0.01699942133373224 R2: 0.0009162898846735157 time: 1703257609.0268946\n",
      "Training [18%] Loss: 0.014385748834010456 time: 1703257609.0268946\n",
      "weight: [ 0.47449511 -0.0528395   0.02482353  0.68173151  0.74690576  0.15238504\n",
      " -0.037795    0.83785182  0.18015859  0.16080805  0.54873379  0.6918952\n",
      "  0.38632979  0.08652247  0.57519975  0.03840431  0.3253997   0.88696811\n",
      "  0.76954895  0.95840521  0.73575949  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 56\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011775522261470934 R2: 0.0009504983407971901 time: 1703257616.5722976\n",
      "batch_idx: 1 loss: 0.01699274569749244 R2: 0.0010108325650917127 time: 1703257624.076214\n",
      "Training [19%] Loss: 0.014384133979481687 time: 1703257624.076214\n",
      "weight: [ 0.47427888 -0.05304686  0.02451423  0.68064626  0.7469473   0.15245413\n",
      " -0.03826752  0.83820605  0.18089341  0.16080805  0.54873379  0.6918952\n",
      "  0.38565857  0.08634006  0.5749924   0.03876128  0.3253997   0.88700965\n",
      "  0.76985586  0.95837522  0.73460333  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 57\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011779624903909804 R2: 0.0010456573185740892 time: 1703257630.6771045\n",
      "batch_idx: 1 loss: 0.016986054352040748 R2: 0.001093253514201753 time: 1703257637.695257\n",
      "Training [19%] Loss: 0.014382839627975276 time: 1703257637.695257\n",
      "weight: [ 0.47406212 -0.05326232  0.02419528  0.6795489   0.74699132  0.1525258\n",
      " -0.03874682  0.83856508  0.18163901  0.16080805  0.54873379  0.6918952\n",
      "  0.38500228  0.08615031  0.57477694  0.0391267   0.3253997   0.88705367\n",
      "  0.77017418  0.95835134  0.73343389  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 58\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011783760851443167 R2: 0.001127544352355847 time: 1703257644.167993\n",
      "batch_idx: 1 loss: 0.016979992625682023 R2: 0.0011644710441643946 time: 1703257650.8287008\n",
      "Training [19%] Loss: 0.014381876738562595 time: 1703257650.8287008\n",
      "weight: [ 0.4738362  -0.05346     0.02389516  0.67844131  0.74703062  0.15259312\n",
      " -0.03922498  0.83892727  0.182384    0.16080805  0.54873379  0.6918952\n",
      "  0.3842595   0.08597665  0.57457926  0.03948489  0.3253997   0.88709296\n",
      "  0.77046979  0.95830472  0.73224473  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 59\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011787291955888662 R2: 0.0012008102191893855 time: 1703257657.49081\n",
      "batch_idx: 1 loss: 0.01697489346094581 R2: 0.0012333122902628002 time: 1703257663.9628563\n",
      "Training [20%] Loss: 0.014381092708417236 time: 1703257663.9628563\n",
      "weight: [ 0.473595   -0.05362203  0.02363357  0.67732451  0.74706007  0.1526512\n",
      " -0.03969743  0.83929159  0.18312049  0.16080805  0.54873379  0.6918952\n",
      "  0.38335962  0.08583525  0.57441723  0.03982529  0.3253997   0.88712242\n",
      "  0.77071913  0.95821534  0.73103098  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 60\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011789757473797697 R2: 0.001274327799680619 time: 1703257670.5936093\n",
      "batch_idx: 1 loss: 0.01697083379151678 R2: 0.0013081356021055202 time: 1703257677.8665595\n",
      "Training [20%] Loss: 0.014380295632657239 time: 1703257677.8665595\n",
      "weight: [ 0.47333584 -0.05374102  0.02341865  0.6761987   0.74707745  0.1526979\n",
      " -0.04016305  0.83965769  0.18384518  0.16080805  0.54873379  0.6918952\n",
      "  0.38227303  0.0857328   0.57429824  0.04014373  0.3253997   0.88713979\n",
      "  0.77091244  0.95807474  0.72979022  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 61\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01179101875704439 R2: 0.0013534853866032925 time: 1703257685.043827\n",
      "batch_idx: 1 loss: 0.016967713898975317 R2: 0.001391089132582235 time: 1703257692.2685516\n",
      "Training [20%] Loss: 0.014379366328009853 time: 1703257692.2685516\n",
      "weight: [ 0.47305969 -0.05382007  0.02324698  0.67506343  0.74708348  0.15273393\n",
      " -0.0406235   0.84002587  0.18455938  0.16080805  0.54873379  0.6918952\n",
      "  0.38101187  0.08566649  0.57421918  0.04044225  0.3253997   0.88714583\n",
      "  0.77105379  0.95788624  0.7285228   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 62\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011791274617651986 R2: 0.0014373602080078873 time: 1703257699.5705085\n",
      "batch_idx: 1 loss: 0.016965304678115422 R2: 0.0014783704109638807 time: 1703257706.262878\n",
      "Training [21%] Loss: 0.014378289647883704 time: 1703257706.262878\n",
      "weight: [ 0.47277058 -0.05387068  0.02310593  0.67391794  0.74708136  0.15276235\n",
      " -0.04108202  0.84039687  0.18526803  0.16080805  0.54873379  0.6918952\n",
      "  0.3796221   0.08562593  0.57416857  0.04072766  0.3253997   0.8871437\n",
      "  0.77115826  0.95766257  0.72723149  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 63\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011790949811882782 R2: 0.001521427281992449 time: 1703257713.0486224\n",
      "batch_idx: 1 loss: 0.01696327291924154 R2: 0.001564652005203726 time: 1703257719.568707\n",
      "Training [21%] Loss: 0.01437711136556216 time: 1703257719.568707\n",
      "weight: [ 0.47247441 -0.05390912  0.02297758  0.67276145  0.7470757   0.15278756\n",
      " -0.04154223  0.84077166  0.18597817  0.16080805  0.54873379  0.6918952\n",
      "  0.37816959  0.08559641  0.57413014  0.04100923  0.3253997   0.88713805\n",
      "  0.7712472   0.9574219   0.72592066  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 64\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011790527712364914 R2: 0.001602241970258933 time: 1703257726.267185\n",
      "batch_idx: 1 loss: 0.016961225256404165 R2: 0.0016473084287257356 time: 1703257732.936979\n",
      "Training [21%] Loss: 0.014375876484384539 time: 1703257732.936979\n",
      "weight: [ 0.47217758 -0.05395226  0.02284338  0.67159337  0.74707141  0.15281424\n",
      " -0.04200707  0.84115114  0.18669707  0.16080805  0.54873379  0.6918952\n",
      "  0.37672356  0.08556267  0.57408699  0.04129636  0.3253997   0.88713376\n",
      "  0.77134272  0.95718315  0.72459513  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 65\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011790409428004644 R2: 0.00168001588146105 time: 1703257739.4752758\n",
      "batch_idx: 1 loss: 0.016958791041277548 R2: 0.0017273975450530221 time: 1703257745.898958\n",
      "Training [22%] Loss: 0.014374600234641096 time: 1703257745.898958\n",
      "weight: [ 0.4718855  -0.05401386  0.0226882   0.67041343  0.74707253  0.15284623\n",
      " -0.04247826  0.841536    0.18743063  0.16080805  0.54873379  0.6918952\n",
      "  0.37534123  0.08551232  0.57402539  0.0415964   0.3253997   0.88713487\n",
      "  0.77146276  0.95696175  0.72325902  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 66\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011790842417864754 R2: 0.0017576615953699415 time: 1703257752.6690495\n",
      "batch_idx: 1 loss: 0.016955720957398217 R2: 0.0018075996867025212 time: 1703257760.0007305\n",
      "Training [22%] Loss: 0.014373281687631485 time: 1703257760.0007305\n",
      "weight: [ 0.47160153 -0.05410191  0.02250327  0.66922164  0.74708145  0.15288579\n",
      " -0.04295619  0.84192657  0.18818221  0.16080805  0.54873379  0.6918952\n",
      "  0.37405669  0.08543815  0.57393735  0.04191345  0.3253997   0.88714379\n",
      "  0.77161771  0.95676671  0.72191493  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 67\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01179191176968175 R2: 0.0018379550907625664 time: 1703257767.3241959\n",
      "batch_idx: 1 loss: 0.01695195325733396 R2: 0.0018895789436598198 time: 1703257774.7355776\n",
      "Training [22%] Loss: 0.014371932513507855 time: 1703257774.7355776\n",
      "weight: [ 0.47132642 -0.05421766  0.02228721  0.66801821  0.7470986   0.15293333\n",
      " -0.0434403   0.84232286  0.18895227  0.16080805  0.54873379  0.6918952\n",
      "  0.37287628  0.08533904  0.5738216   0.04224791  0.3253997   0.88716095\n",
      "  0.77180916  0.95659948  0.72056339  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 68\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011793563098474106 R2: 0.0019215049069797319 time: 1703257781.503765\n",
      "batch_idx: 1 loss: 0.01694761309767093 R2: 0.0019730016871041256 time: 1703257787.8761678\n",
      "Training [23%] Loss: 0.014370588098072519 time: 1703257787.8761678\n",
      "weight: [ 0.47105847 -0.05435632  0.02204532  0.66680341  0.74712261  0.15298751\n",
      " -0.0439296   0.84272461  0.18973863  0.16080805  0.54873379  0.6918952\n",
      "  0.3717809   0.08521931  0.57368294  0.04259704  0.3253997   0.88718495\n",
      "  0.77203079  0.95645467  0.71920302  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 69\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011795634353386332 R2: 0.002006896180315576 time: 1703257794.7655566\n",
      "batch_idx: 1 loss: 0.01694294865425669 R2: 0.0020565135776569043 time: 1703257801.4321823\n",
      "Training [23%] Loss: 0.014369291503821512 time: 1703257801.4321823\n",
      "weight: [ 0.47079424 -0.05450903  0.02178738  0.6655774   0.74715082  0.15304581\n",
      " -0.04442315  0.8431314   0.19053737  0.16080805  0.54873379  0.6918952\n",
      "  0.37073393  0.08508697  0.57353023  0.04295614  0.3253997   0.88721316\n",
      "  0.77227101  0.95632225  0.71783095  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 70\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011797894310061645 R2: 0.002092306672300648 time: 1703257808.135524\n",
      "batch_idx: 1 loss: 0.01693823945343375 R2: 0.002139304054399882 time: 1703257814.843853\n",
      "Training [23%] Loss: 0.014368066881747698 time: 1703257814.843853\n",
      "weight: [ 0.47052956 -0.05466555  0.02152467  0.66434026  0.74718012  0.15310528\n",
      " -0.04492046  0.8435428   0.19134397  0.16080805  0.54873379  0.6918952\n",
      "  0.36969243  0.08495124  0.57337371  0.04331996  0.3253997   0.88724246\n",
      "  0.77251644  0.95619059  0.71644368  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 71\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011800094058471416 R2: 0.0021769152376894807 time: 1703257821.926934\n",
      "batch_idx: 1 loss: 0.016933717095958867 R2: 0.002221772349565554 time: 1703257829.2223353\n",
      "Training [24%] Loss: 0.014366905577215141 time: 1703257829.2223353\n",
      "weight: [ 0.4702607  -0.05481691  0.02126707  0.66309191  0.74720773  0.15316327\n",
      " -0.04542152  0.84395848  0.19215446  0.16080805  0.54873379  0.6918952\n",
      "  0.36861847  0.08482021  0.57322235  0.04368403  0.3253997   0.88727007\n",
      "  0.77275539  0.95604948  0.71503798  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 72\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01180202806391796 R2: 0.002261061282599397 time: 1703257836.5387447\n",
      "batch_idx: 1 loss: 0.016929520107924305 R2: 0.0023049578305407437 time: 1703257843.7925098\n",
      "Training [24%] Loss: 0.014365774085921133 time: 1703257843.7925098\n",
      "weight: [ 0.4699852  -0.05495743  0.02102084  0.66183224  0.74723185  0.15321807\n",
      " -0.04592665  0.84437827  0.19296633  0.16080805  0.54873379  0.6918952\n",
      "  0.36748783  0.08469897  0.57308182  0.04404566  0.3253997   0.8872942\n",
      "  0.77298046  0.95589239  0.71361166  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 73\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011803586478636175 R2: 0.0023454312788389053 time: 1703257850.827153\n",
      "batch_idx: 1 loss: 0.01692568244311218 R2: 0.0023895341346081844 time: 1703257857.5350895\n",
      "Training [24%] Loss: 0.014364634460874177 time: 1703257857.5350895\n",
      "weight: [ 0.46970242 -0.05508566  0.02078761  0.66056121  0.74725198  0.15326922\n",
      " -0.04643616  0.84480214  0.19377883  0.16080805  0.54873379  0.6918952\n",
      "  0.36629423  0.08458886  0.5729536   0.04440422  0.3253997   0.88731433\n",
      "  0.77318972  0.95571759  0.71216401  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 74\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011804777061569832 R2: 0.0024302735038992385 time: 1703257864.1864126\n",
      "batch_idx: 1 loss: 0.01692214486556864 R2: 0.002475334834290077 time: 1703257870.4505305\n",
      "Training [25%] Loss: 0.014363460963569236 time: 1703257870.4505305\n",
      "weight: [ 0.46941344 -0.05520408  0.02056464  0.65927887  0.74726889  0.15331741\n",
      " -0.0469501   0.84523018  0.19459295  0.16080805  0.54873379  0.6918952\n",
      "  0.36504862  0.08448762  0.57283518  0.04476096  0.3253997   0.88733123\n",
      "  0.77338644  0.95552792  0.71069585  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 75\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011805707823065566 R2: 0.002515318022399904 time: 1703257877.2068906\n",
      "batch_idx: 1 loss: 0.0169187807336171 R2: 0.0025616771183685216 time: 1703257883.9912996\n",
      "Training [25%] Loss: 0.014362244278341332 time: 1703257883.9912996\n",
      "weight: [ 0.46912066 -0.05531797  0.02034614  0.65798548  0.74728423  0.15336422\n",
      " -0.04746803  0.84566254  0.19541085  0.16080805  0.54873379  0.6918952\n",
      "  0.36377415  0.08439051  0.57272129  0.04511835  0.3253997   0.88734657\n",
      "  0.77357746  0.95532938  0.70920913  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 76\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01180654040894738 R2: 0.0026002575761253643 time: 1703257891.3269486\n",
      "batch_idx: 1 loss: 0.016915432569352806 R2: 0.0026479995728001526 time: 1703257898.4739947\n",
      "Training [25%] Loss: 0.014360986489150093 time: 1703257898.4739947\n",
      "weight: [ 0.46882699 -0.05543359  0.02012522  0.65668141  0.74730003  0.15341156\n",
      " -0.04798909  0.84609936  0.19623511  0.16080805  0.54873379  0.6918952\n",
      "  0.36249865  0.08429189  0.57260566  0.0454792   0.3253997   0.88736237\n",
      "  0.77377089  0.95512918  0.70770632  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 77\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011807435560791463 R2: 0.0026851721161659237 time: 1703257905.7970634\n",
      "batch_idx: 1 loss: 0.01691195490483472 R2: 0.002734194281002744 time: 1703257913.189269\n",
      "Training [26%] Loss: 0.014359695232813092 time: 1703257913.189269\n",
      "weight: [ 0.46853507 -0.05555644  0.01989586  0.65536707  0.74731809  0.1534611\n",
      " -0.04851222  0.84654075  0.19706797  0.16080805  0.54873379  0.6918952\n",
      "  0.3612468   0.08418683  0.57248282  0.04584588  0.3253997   0.88738044\n",
      "  0.77397384  0.95493365  0.70618968  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 78\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011808510053940142 R2: 0.0027704880676980848 time: 1703257920.3128407\n",
      "batch_idx: 1 loss: 0.016908253282722323 R2: 0.0028204555080046934 time: 1703257927.5536995\n",
      "Training [26%] Loss: 0.014358381668331232 time: 1703257927.5536995\n",
      "weight: [ 0.46824662 -0.05568985  0.01965439  0.65404282  0.74733956  0.15351391\n",
      " -0.04903648  0.84698672  0.19791074  0.16080805  0.54873379  0.6918952\n",
      "  0.36003408  0.08407233  0.5723494   0.04621974  0.3253997   0.88740191\n",
      "  0.77419063  0.95474667  0.70466068  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 79\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011809814347411546 R2: 0.0028565973170284753 time: 1703257934.2012718\n",
      "batch_idx: 1 loss: 0.016904306422816377 R2: 0.002906944960403557 time: 1703257941.1795344\n",
      "Training [26%] Loss: 0.014357060385113961 time: 1703257941.1795344\n",
      "weight: [ 0.46796207 -0.05583448  0.01940013  0.65270886  0.74736467  0.15357021\n",
      " -0.04956139  0.84743724  0.19876357  0.16080805  0.54873379  0.6918952\n",
      "  0.35886389  0.08394781  0.57220478  0.04660092  0.3253997   0.88742701\n",
      "  0.77442207  0.954569    0.70311965  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 80\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011811330734165633 R2: 0.0029435526969036063 time: 1703257948.2803361\n",
      "batch_idx: 1 loss: 0.01690016362003107 R2: 0.0029936448259996196 time: 1703257955.5521932\n",
      "Training [27%] Loss: 0.014355747177098352 time: 1703257955.5521932\n",
      "weight: [ 0.46768063 -0.05598845  0.01913514  0.65136516  0.74739283  0.15362943\n",
      " -0.05008707  0.84789225  0.19962555  0.16080805  0.54873379  0.6918952\n",
      "  0.35772841  0.08381495  0.57205081  0.04698855  0.3253997   0.88745517\n",
      "  0.77466571  0.95439849  0.70156583  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 81\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011812987665936175 R2: 0.00303109363692311 time: 1703257962.8709517\n",
      "batch_idx: 1 loss: 0.016895920032896142 R2: 0.0030804754230224773 time: 1703257970.1665757\n",
      "Training [27%] Loss: 0.014354453849416159 time: 1703257970.1665757\n",
      "weight: [ 0.46740069 -0.05614829  0.01886326  0.65001157  0.74742289  0.15369048\n",
      " -0.0506142   0.84835167  0.20049509  0.16080805  0.54873379  0.6918952\n",
      "  0.35661241  0.08367687  0.57189096  0.04738117  0.3253997   0.88748523\n",
      "  0.77491706  0.95423111  0.69999769  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 82\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011814685297080737 R2: 0.0031188979857110777 time: 1703257977.1494129\n",
      "batch_idx: 1 loss: 0.016891681413980578 R2: 0.0031674685932501223 time: 1703257983.7782772\n",
      "Training [27%] Loss: 0.014353183355530657 time: 1703257983.7782772\n",
      "weight: [ 0.4671204  -0.05631016  0.01858874  0.64864781  0.74745354  0.15375213\n",
      " -0.05114379  0.84881546  0.20137042  0.16080805  0.54873379  0.6918952\n",
      "  0.35549876  0.08353702  0.5717291   0.0477772   0.3253997   0.88751588\n",
      "  0.7751711   0.95406235  0.69841343  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 83\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011816326293013825 R2: 0.0032067855874382545 time: 1703257991.0399542\n",
      "batch_idx: 1 loss: 0.016887531783894374 R2: 0.003254785072996147 time: 1703257998.2652562\n",
      "Training [28%] Loss: 0.0143519290384541 time: 1703257998.2652562\n",
      "weight: [ 0.46683824 -0.05647096  0.01831498  0.64727363  0.74748371  0.15381337\n",
      " -0.05167679  0.84928357  0.20225013  0.16080805  0.54873379  0.6918952\n",
      "  0.35437368  0.08339817  0.57156829  0.04817547  0.3253997   0.88754606\n",
      "  0.77542385  0.9538886   0.69681155  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 84\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011817843811699773 R2: 0.003294727736258274 time: 1703258004.9201467\n",
      "batch_idx: 1 loss: 0.01688351320779987 R2: 0.003342576105126449 time: 1703258011.6429148\n",
      "Training [28%] Loss: 0.014350678509749821 time: 1703258011.6429148\n",
      "weight: [ 0.46655347 -0.05662919  0.01804367  0.64588894  0.74751289  0.1538737\n",
      " -0.05221374  0.84975599  0.20313345  0.16080805  0.54873379  0.6918952\n",
      "  0.35323051  0.08326168  0.57141006  0.04857541  0.3253997   0.88757523\n",
      "  0.77567333  0.95370805  0.69519126  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 85\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011819218019285292 R2: 0.0033827416184697423 time: 1703258018.963065\n",
      "batch_idx: 1 loss: 0.016879620659477606 R2: 0.0034308526929456296 time: 1703258026.3827603\n",
      "Training [28%] Loss: 0.014349419339381449 time: 1703258026.3827603\n",
      "weight: [ 0.46626625 -0.05678511  0.01777452  0.64449381  0.74754114  0.15393318\n",
      " -0.05275454  0.85023271  0.20402036  0.16080805  0.54873379  0.6918952\n",
      "  0.35207073  0.0831273   0.57125415  0.04897713  0.3253997   0.88760349\n",
      "  0.77591988  0.95352101  0.69355263  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 86\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011820475999069149 R2: 0.0034708263706525466 time: 1703258033.4598012\n",
      "batch_idx: 1 loss: 0.016875810494613078 R2: 0.0035194875380103463 time: 1703258040.2796254\n",
      "Training [29%] Loss: 0.014348143246841114 time: 1703258040.2796254\n",
      "weight: [ 0.46597754 -0.05694042  0.01750568  0.64308852  0.74756908  0.15399238\n",
      " -0.05329846  0.8507137   0.20491145  0.16080805  0.54873379  0.6918952\n",
      "  0.35090257  0.08299352  0.57109884  0.04938123  0.3253997   0.88763142\n",
      "  0.77616569  0.95332946  0.69189646  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 87\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011821676437335912 R2: 0.0035589931590718216 time: 1703258046.7283711\n",
      "batch_idx: 1 loss: 0.01687201862924905 R2: 0.003608320547294097 time: 1703258053.37371\n",
      "Training [29%] Loss: 0.014346847533292482 time: 1703258053.37371\n",
      "weight: [ 0.46568869 -0.05709753  0.01723451  0.64167348  0.74759756  0.15405209\n",
      " -0.05384431  0.85119893  0.20580759  0.16080805  0.54873379  0.6918952\n",
      "  0.34973764  0.08285818  0.57094173  0.04978851  0.3253997   0.88765991\n",
      "  0.77641385  0.95313626  0.69022397  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 88\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011822886244574265 R2: 0.0036473139761884488 time: 1703258059.754528\n",
      "batch_idx: 1 loss: 0.016868182837884944 R2: 0.0036972510010424564 time: 1703258066.3414161\n",
      "Training [29%] Loss: 0.014345534541229605 time: 1703258066.3414161\n",
      "weight: [ 0.46540102 -0.05725871  0.01695852  0.64024911  0.74762744  0.1541131\n",
      " -0.05439084  0.85168833  0.2067096   0.16080805  0.54873379  0.6918952\n",
      "  0.34858701  0.08271922  0.57078054  0.05019968  0.3253997   0.88768978\n",
      "  0.77666727  0.95294411  0.68853636  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 89\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0118241581713207 R2: 0.0037359047713972915 time: 1703258073.0484118\n",
      "batch_idx: 1 loss: 0.016864262388117588 R2: 0.0037862486342845614 time: 1703258080.3127315\n",
      "Training [30%] Loss: 0.014344210279719143 time: 1703258080.3127315\n",
      "weight: [ 0.4651154  -0.05742542  0.01667615  0.63881574  0.74765925  0.15417589\n",
      " -0.05493711  0.85218185  0.20761794  0.16080805  0.54873379  0.6918952\n",
      "  0.34745791  0.08257538  0.57061384  0.05061515  0.3253997   0.88772159\n",
      "  0.7769278   0.95275471  0.68683441  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 90\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01182551637961408 R2: 0.0038248498146365995 time: 1703258087.483587\n",
      "batch_idx: 1 loss: 0.016860248758592718 R2: 0.003875315022173198 time: 1703258094.734169\n",
      "Training [30%] Loss: 0.014342882569103398 time: 1703258094.734169\n",
      "weight: [ 0.4648321  -0.05759792  0.01638709  0.63737352  0.74769311  0.15424059\n",
      " -0.05548271  0.85267944  0.20853258  0.16080805  0.54873379  0.6918952\n",
      "  0.3463521   0.08242638  0.57044133  0.05103493  0.3253997   0.88775546\n",
      "  0.77719577  0.95256841  0.68511831  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 91\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011826952999146888 R2: 0.003914143419364469 time: 1703258102.0997922\n",
      "batch_idx: 1 loss: 0.01685616434450329 R2: 0.0039644600062472835 time: 1703258108.7512896\n",
      "Training [30%] Loss: 0.01434155867182509 time: 1703258108.7512896\n",
      "weight: [ 0.46455072 -0.05777542  0.01609227  0.63592244  0.74772876  0.1543069\n",
      " -0.05602788  0.85318108  0.20945303  0.16080805  0.54873379  0.6918952\n",
      "  0.34526616  0.08227297  0.57026384  0.05145872  0.3253997   0.8877911\n",
      "  0.77747014  0.95238426  0.68338761  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 92\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011828435036102931 R2: 0.0040037004642167195 time: 1703258115.241185\n",
      "batch_idx: 1 loss: 0.016852050773360796 R2: 0.004053710509924979 time: 1703258121.6478932\n",
      "Training [31%] Loss: 0.014340242904731864 time: 1703258121.6478932\n",
      "weight: [ 0.46427053 -0.05795644  0.01579329  0.63446235  0.74776565  0.15437434\n",
      " -0.05657331  0.85368675  0.21037854  0.16080805  0.54873379  0.6918952\n",
      "  0.34419351  0.08211644  0.57008282  0.05188603  0.3253997   0.88782799\n",
      "  0.777749    0.95220051  0.68164149  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 93\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011829918342345529 R2: 0.004093413961794967 time: 1703258128.6329055\n",
      "batch_idx: 1 loss: 0.016847952266376427 R2: 0.004143117845028854 time: 1703258135.8327627\n",
      "Training [31%] Loss: 0.014338935304360978 time: 1703258135.8327627\n",
      "weight: [ 0.46399069 -0.05813946  0.01549186  0.63299306  0.74780321  0.15444236\n",
      " -0.05711982  0.85419647  0.21130831  0.16080805  0.54873379  0.6918952\n",
      "  0.34312714  0.08195817  0.5698998   0.05231641  0.3253997   0.88786555\n",
      "  0.77803038  0.9520153   0.67987904  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 94\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011831363651064354 R2: 0.004183203569982363 time: 1703258142.9615107\n",
      "batch_idx: 1 loss: 0.016843900548320755 R2: 0.0042327355329279115 time: 1703258150.334806\n",
      "Training [31%] Loss: 0.014337632099692556 time: 1703258150.334806\n",
      "weight: [ 0.46371062 -0.05832339  0.01518918  0.63151446  0.74784103  0.15451057\n",
      " -0.05766802  0.85471023  0.21224174  0.16080805  0.54873379  0.6918952\n",
      "  0.34206217  0.08179912  0.56971587  0.05274954  0.3253997   0.88790337\n",
      "  0.77831285  0.95182733  0.67809958  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 95\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011832749213071212 R2: 0.004273028017257707 time: 1703258157.658994\n",
      "batch_idx: 1 loss: 0.016839906180445553 R2: 0.00432258267419261 time: 1703258164.656789\n",
      "Training [32%] Loss: 0.014336327696758383 time: 1703258164.656789\n",
      "weight: [ 0.46343018 -0.05850791  0.01488562  0.63002655  0.74787898  0.15457884\n",
      " -0.05821804  0.855228    0.21317854  0.16080805  0.54873379  0.6918952\n",
      "  0.34099735  0.08163959  0.56953135  0.05318532  0.3253997   0.88794133\n",
      "  0.77859599  0.95163619  0.67630289  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 96\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011834075886786366 R2: 0.004362877295818968 time: 1703258171.255273\n",
      "batch_idx: 1 loss: 0.016835958243630145 R2: 0.004412628465710444 time: 1703258178.0402005\n",
      "Training [32%] Loss: 0.014335017065208256 time: 1703258178.0402005\n",
      "weight: [ 0.46314968 -0.05869343  0.01458074  0.62852951  0.74791725  0.15464733\n",
      " -0.05876947  0.85574974  0.21411872  0.16080805  0.54873379  0.6918952\n",
      "  0.3399351   0.0814792   0.56934583  0.05362382  0.3253997   0.88797959\n",
      "  0.77888033  0.9514424   0.6744892   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 97\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011835363537129101 R2: 0.004452767368914867 time: 1703258184.4499464\n",
      "batch_idx: 1 loss: 0.01683203137967975 R2: 0.004502810513641875 time: 1703258191.204415\n",
      "Training [32%] Loss: 0.014333697458404425 time: 1703258191.204415\n",
      "weight: [ 0.46286975 -0.05888088  0.01427354  0.62702362  0.74795619  0.15471638\n",
      " -0.05932146  0.85627538  0.21506249  0.16080805  0.54873379  0.6918952\n",
      "  0.33888026  0.08131713  0.56915838  0.05406525  0.3253997   0.88801854\n",
      "  0.77916702  0.95124705  0.67265907  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 98\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011836641032613242 R2: 0.004542737170451283 time: 1703258199.1234813\n",
      "batch_idx: 1 loss: 0.016828096996032958 R2: 0.004593066853383676 time: 1703258206.2346528\n",
      "Training [33%] Loss: 0.0143323690143231 time: 1703258206.2346528\n",
      "weight: [ 0.46259106 -0.05907125  0.01396295  0.6255092   0.74799622  0.15478634\n",
      " -0.05987305  0.85680482  0.2160101   0.16080805  0.54873379  0.6918952\n",
      "  0.33783811  0.08115248  0.56896801  0.05450976  0.3253997   0.88805857\n",
      "  0.77945734  0.95105137  0.67081314  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 99\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011837934342003252 R2: 0.00463283450903218 time: 1703258213.4753556\n",
      "batch_idx: 1 loss: 0.016824134310086152 R2: 0.004683356495937541 time: 1703258221.265781\n",
      "Training [33%] Loss: 0.014331034326044701 time: 1703258221.265781\n",
      "weight: [ 0.46231413 -0.05926526  0.01364818  0.62398651  0.74803764  0.1548575\n",
      " -0.06042345  0.85733799  0.21696166  0.16080805  0.54873379  0.6918952\n",
      "  0.33681255  0.08098462  0.568774    0.05495748  0.3253997   0.88809999\n",
      "  0.77975218  0.95085624  0.66895188  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 100\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01183925734651753 R2: 0.004723090714342293 time: 1703258229.1820738\n",
      "batch_idx: 1 loss: 0.0168201372168809 R2: 0.004773663066565015 time: 1703258236.586043\n",
      "Training [33%] Loss: 0.014329697281699215 time: 1703258236.586043\n",
      "weight: [ 0.46203918 -0.05946311  0.01332904  0.62245571  0.74808055  0.15492994\n",
      " -0.06097228  0.85787482  0.21791708  0.16080805  0.54873379  0.6918952\n",
      "  0.33580495  0.08081336  0.56857615  0.05540838  0.3253997   0.88814289\n",
      "  0.78005179  0.95066192  0.66707544  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 101\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011840608329687943 R2: 0.004813501557627942 time: 1703258243.97869\n",
      "batch_idx: 1 loss: 0.016816114788041518 R2: 0.004863991245764976 time: 1703258251.301694\n",
      "Training [34%] Loss: 0.01432836155886473 time: 1703258251.301694\n",
      "weight: [ 0.46176606 -0.05966449  0.01300588  0.62091682  0.74812484  0.15500354\n",
      " -0.06151959  0.85841528  0.21887606  0.16080805  0.54873379  0.6918952\n",
      "  0.33481414  0.08063899  0.56837477  0.05586235  0.3253997   0.88818718\n",
      "  0.78035575  0.95046802  0.66518361  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 102\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011841972678598633 R2: 0.004904029320258463 time: 1703258258.9377851\n",
      "batch_idx: 1 loss: 0.016812085893136323 R2: 0.004954361974432109 time: 1703258266.1699731\n",
      "Training [34%] Loss: 0.014327029285867477 time: 1703258266.1699731\n",
      "weight: [ 0.46149445 -0.05986877  0.01267941  0.61936977  0.74817025  0.15507807\n",
      " -0.06206581  0.85895935  0.21983815  0.16080805  0.54873379  0.6918952\n",
      "  0.33383733  0.08046206  0.56817049  0.05631924  0.3253997   0.8882326\n",
      "  0.78066323  0.95027378  0.66327595  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 103\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011843330067733264 R2: 0.004994622214050226 time: 1703258273.4969952\n",
      "batch_idx: 1 loss: 0.016808070743635658 R2: 0.005044803300864431 time: 1703258280.8895369\n",
      "Training [34%] Loss: 0.014325700405684462 time: 1703258280.8895369\n",
      "weight: [ 0.46122396 -0.06007529  0.01235036  0.61781449  0.74821653  0.15515327\n",
      " -0.06261138  0.85950703  0.22080292  0.16080805  0.54873379  0.6918952\n",
      "  0.33287155  0.08028317  0.56796397  0.05677889  0.3253997   0.88827888\n",
      "  0.78097338  0.95007836  0.66135201  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 104\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011844662942144178 R2: 0.005085235485315587 time: 1703258288.3770545\n",
      "batch_idx: 1 loss: 0.016804083058686 R2: 0.005135335497026494 time: 1703258295.200584\n",
      "Training [35%] Loss: 0.01432437300041509 time: 1703258295.200584\n",
      "weight: [ 0.46095438 -0.06028361  0.01201924  0.61625095  0.74826349  0.15522896\n",
      " -0.06315661  0.8600583   0.22177     0.16080805  0.54873379  0.6918952\n",
      "  0.33191493  0.0801027   0.56775565  0.05724122  0.3253997   0.88832584\n",
      "  0.78128561  0.94988123  0.65941144  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 105\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011845962885318542 R2: 0.005175843453624873 time: 1703258301.888399\n",
      "batch_idx: 1 loss: 0.01680012583053779 R2: 0.005225958080054549 time: 1703258308.5629532\n",
      "Training [35%] Loss: 0.014323044357928166 time: 1703258308.5629532\n",
      "weight: [ 0.4606857  -0.06049365  0.01168613  0.61467921  0.74831111  0.15530513\n",
      " -0.06370142  0.86061311  0.22273916  0.16080805  0.54873379  0.6918952\n",
      "  0.33096741  0.07992072  0.5675456   0.05770617  0.3253997   0.88837346\n",
      "  0.78159981  0.94968228  0.65745418  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 106\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0118472325581962 R2: 0.005266442218263112 time: 1703258315.37831\n",
      "batch_idx: 1 loss: 0.01679619188657814 R2: 0.005316648202071561 time: 1703258322.2921438\n",
      "Training [35%] Loss: 0.01432171222238717 time: 1703258322.2921438\n",
      "weight: [ 0.46041814 -0.06070567  0.01135079  0.61309943  0.74835951  0.15538186\n",
      " -0.06424537  0.86117139  0.22371033  0.16080805  0.54873379  0.6918952\n",
      "  0.33003058  0.07973701  0.56733359  0.05817375  0.3253997   0.88842186\n",
      "  0.78191628  0.94948183  0.65548042  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 107\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011848482905353503 R2: 0.005357046051176373 time: 1703258329.116757\n",
      "batch_idx: 1 loss: 0.016792268445204636 R2: 0.005407371057354338 time: 1703258336.5473652\n",
      "Training [36%] Loss: 0.014320375675279069 time: 1703258336.5473652\n",
      "weight: [ 0.46015208 -0.06092009  0.01101276  0.61151183  0.74840888  0.15545932\n",
      " -0.0647878   0.86173306  0.22468346  0.16080805  0.54873379  0.6918952\n",
      "  0.32910695  0.07955118  0.56711917  0.05864396  0.3253997   0.88847123\n",
      "  0.78223555  0.94928042  0.65349048  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 108\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01184972733151812 R2: 0.005447678212822882 time: 1703258343.7406259\n",
      "batch_idx: 1 loss: 0.016788343377200494 R2: 0.005498094173288615 time: 1703258351.1405158\n",
      "Training [36%] Loss: 0.014319035354359307 time: 1703258351.1405158\n",
      "weight: [ 0.45988782 -0.0611373   0.01067161  0.60991662  0.74845941  0.1555377\n",
      " -0.06532807  0.86229802  0.22565854  0.16080805  0.54873379  0.6918952\n",
      "  0.32819883  0.07936289  0.56690195  0.0591168   0.3253997   0.88852176\n",
      "  0.78255812  0.94907856  0.65148466  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 109\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011850975637661586 R2: 0.00553835765820887 time: 1703258357.9346132\n",
      "batch_idx: 1 loss: 0.016784410406052986 R2: 0.00558879746587182 time: 1703258364.5018654\n",
      "Training [36%] Loss: 0.014317693021857286 time: 1703258364.5018654\n",
      "weight: [ 0.45962559 -0.06135752  0.01032714  0.60831397  0.74851121  0.15561706\n",
      " -0.06586573  0.8628662   0.22663543  0.16080805  0.54873379  0.6918952\n",
      "  0.32730756  0.07917194  0.56668174  0.05959224  0.3253997   0.88857356\n",
      "  0.78288421  0.94887649  0.64946314  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 110\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011852230276744658 R2: 0.005629087510305664 time: 1703258371.1902208\n",
      "batch_idx: 1 loss: 0.016780471168363634 R2: 0.005679476505416892 time: 1703258377.85395\n",
      "Training [37%] Loss: 0.014316350722554147 time: 1703258377.85395\n",
      "weight: [ 0.4593654  -0.06158067  0.00997944  0.60670395  0.74856426  0.15569739\n",
      " -0.06640064  0.86343753  0.22761391  0.16080805  0.54873379  0.6918952\n",
      "  0.32643314  0.07897839  0.56645859  0.06007022  0.3253997   0.8886266\n",
      "  0.78321375  0.94867416  0.64742588  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 111\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011853486213145338 R2: 0.005719852125276148 time: 1703258384.5374005\n",
      "batch_idx: 1 loss: 0.016776533599378782 R2: 0.005770139951504438 time: 1703258390.9209855\n",
      "Training [37%] Loss: 0.01431500990626206 time: 1703258390.9209855\n",
      "weight: [ 0.45910713 -0.06180651  0.00962878  0.60508656  0.74861845  0.15577858\n",
      " -0.06693291  0.86401199  0.22859368  0.16080805  0.54873379  0.6918952\n",
      "  0.32557456  0.07878247  0.56623274  0.0605507   0.3253997   0.8886808\n",
      "  0.78354639  0.94847124  0.64537269  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 112\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011854734011605587 R2: 0.005810624027786981 time: 1703258397.63026\n",
      "batch_idx: 1 loss: 0.016772607778184626 R2: 0.0058608024047225316 time: 1703258404.2616775\n",
      "Training [37%] Loss: 0.014313670894895106 time: 1703258404.2616775\n",
      "weight: [ 0.45885061 -0.06203473  0.00927554  0.6034618   0.74867366  0.15586051\n",
      " -0.06746278  0.86458953  0.2295744   0.16080805  0.54873379  0.6918952\n",
      "  0.32473047  0.07858445  0.56600453  0.0610336   0.3253997   0.88873601\n",
      "  0.7838817   0.94826733  0.64330331  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 113\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01185596444911827 R2: 0.0059013755116815725 time: 1703258411.60464\n",
      "batch_idx: 1 loss: 0.016768701372209385 R2: 0.0059514748277710305 time: 1703258419.0336845\n",
      "Training [38%] Loss: 0.014312332910663828 time: 1703258419.0336845\n",
      "weight: [ 0.45859571 -0.06226507  0.00891999  0.60182966  0.74872979  0.15594308\n",
      " -0.06799035  0.86517012  0.23055577  0.16080805  0.54873379  0.6918952\n",
      "  0.32389993  0.07838455  0.56577418  0.06151888  0.3253997   0.88879213\n",
      "  0.78421935  0.94806213  0.64121756  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 114\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011857172450330317 R2: 0.005992088301273291 time: 1703258426.1094065\n",
      "batch_idx: 1 loss: 0.0167648167541686 R2: 0.00604215689153198 time: 1703258432.8231387\n",
      "Training [38%] Loss: 0.014310994602249459 time: 1703258432.8231387\n",
      "weight: [ 0.45834245 -0.06249748  0.00856221  0.60019022  0.7487868   0.15602624\n",
      " -0.06851554  0.86575371  0.23153755  0.16080805  0.54873379  0.6918952\n",
      "  0.32308285  0.07818283  0.56554177  0.0620065   0.3253997   0.88884915\n",
      "  0.78455926  0.94785556  0.63911538  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 115\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011858358658943817 R2: 0.006082757656267868 time: 1703258439.6802511\n",
      "batch_idx: 1 loss: 0.01676095087765845 R2: 0.00613283554119759 time: 1703258446.5553222\n",
      "Training [38%] Loss: 0.014309654768301133 time: 1703258446.5553222\n",
      "weight: [ 0.45809094 -0.06273205  0.00820211  0.59854358  0.74884476  0.15611005\n",
      " -0.06903803  0.8663402   0.23251956  0.16080805  0.54873379  0.6918952\n",
      "  0.32227998  0.07797919  0.5653072   0.06249643  0.3253997   0.88890711\n",
      "  0.78490151  0.94764774  0.63699687  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 116\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01185952820015927 R2: 0.006173390422826164 time: 1703258453.3952348\n",
      "batch_idx: 1 loss: 0.016757097635378637 R2: 0.00622349028169189 time: 1703258460.0235376\n",
      "Training [39%] Loss: 0.014308312917768955 time: 1703258460.0235376\n",
      "weight: [ 0.45784138 -0.06296897  0.00783951  0.59688992  0.74890377  0.15619459\n",
      " -0.06955732  0.86692953  0.23350166  0.16080805  0.54873379  0.6918952\n",
      "  0.32149254  0.07777348  0.56507028  0.06298861  0.3253997   0.88896611\n",
      "  0.78524633  0.94743893  0.63486221  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 117\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011860687537955444 R2: 0.0062639983189204385 time: 1703258467.5095942\n",
      "batch_idx: 1 loss: 0.016753251322565783 R2: 0.006314101504807712 time: 1703258474.2305892\n",
      "Training [39%] Loss: 0.014306969430260615 time: 1703258474.2305892\n",
      "weight: [ 0.45759395 -0.06320841  0.00747423  0.59522938  0.74896392  0.15627994\n",
      " -0.07007297  0.86752159  0.2344837   0.16080805  0.54873379  0.6918952\n",
      "  0.32072163  0.07756552  0.56483084  0.063483    0.3253997   0.88902626\n",
      "  0.78559393  0.94722935  0.63271159  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 118\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011861841126768028 R2: 0.006354589405670352 time: 1703258480.7676928\n",
      "batch_idx: 1 loss: 0.01674940945571495 R2: 0.006404657205396624 time: 1703258487.619089\n",
      "Training [39%] Loss: 0.01430562529124149 time: 1703258487.619089\n",
      "weight: [ 0.45734875 -0.06345044  0.00710621  0.59356207  0.74902525  0.15636612\n",
      " -0.07058465  0.8681163   0.23546547  0.16080805  0.54873379  0.6918952\n",
      "  0.31996784  0.07735528  0.56458881  0.06397956  0.3253997   0.8890876\n",
      "  0.78594436  0.9470191   0.63054507  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 119\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01186298946399583 R2: 0.006445161985004999 time: 1703258494.2928536\n",
      "batch_idx: 1 loss: 0.016745573678111685 R2: 0.006495155331155966 time: 1703258501.0256395\n",
      "Training [40%] Loss: 0.014304281571053758 time: 1703258501.0256395\n",
      "weight: [ 0.45710579 -0.063695    0.00673553  0.59188807  0.74908775  0.15645313\n",
      " -0.07109224  0.86871359  0.23644674  0.16080805  0.54873379  0.6918952\n",
      "  0.31923107  0.07714278  0.56434426  0.06447823  0.3253997   0.8891501\n",
      "  0.78629755  0.94680811  0.62836264  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 120\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0118641293026665 R2: 0.006535704132793585 time: 1703258507.6700778\n",
      "batch_idx: 1 loss: 0.016741748538419737 R2: 0.006585601376892458 time: 1703258514.6579022\n",
      "Training [40%] Loss: 0.014302938920543119 time: 1703258514.6579022\n",
      "weight: [ 0.45686501 -0.06394195  0.00636235  0.59020739  0.74915137  0.15654088\n",
      " -0.07159575  0.86931342  0.23742722  0.16080805  0.54873379  0.6918952\n",
      "  0.31851078  0.07692816  0.56409731  0.06497899  0.3253997   0.88921371\n",
      "  0.78665327  0.94659619  0.6261642   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 121\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011865255662246341 R2: 0.0066261986581573895 time: 1703258521.8914616\n",
      "batch_idx: 1 loss: 0.016737938995717575 R2: 0.006676002660202518 time: 1703258529.433129\n",
      "Training [40%] Loss: 0.014301597328981959 time: 1703258529.433129\n",
      "weight: [ 0.45662633 -0.06419112  0.00598686  0.58852007  0.74921602  0.15662932\n",
      " -0.07209523  0.86991571  0.23840662  0.16080805  0.54873379  0.6918952\n",
      "  0.31780636  0.07671155  0.56384813  0.0654818   0.3253997   0.88927837\n",
      "  0.78701132  0.94638316  0.62394963  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 122\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01186636443854284 R2: 0.006716630162108439 time: 1703258536.0198047\n",
      "batch_idx: 1 loss: 0.01673414804324854 R2: 0.006766362148318605 time: 1703258542.4267335\n",
      "Training [41%] Loss: 0.01430025624089569 time: 1703258542.4267335\n",
      "weight: [ 0.45638973 -0.06444245  0.00560917  0.58682613  0.74928168  0.1567184\n",
      " -0.07259063  0.87052042  0.23938469  0.16080805  0.54873379  0.6918952\n",
      "  0.3171175   0.07649304  0.56359681  0.06598663  0.3253997   0.88934403\n",
      "  0.78737156  0.9461689   0.62171887  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 123\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0118674542420829 R2: 0.006806990293750537 time: 1703258549.210499\n",
      "batch_idx: 1 loss: 0.016730375627918286 R2: 0.0068566750948200195 time: 1703258555.656897\n",
      "Training [41%] Loss: 0.014298914935000594 time: 1703258555.656897\n",
      "weight: [ 0.45615526 -0.0646959   0.0052293   0.58512568  0.74934836  0.15680812\n",
      " -0.07308174  0.87112746  0.24036118  0.16080805  0.54873379  0.6918952\n",
      "  0.31644436  0.07627262  0.56334335  0.06649343  0.3253997   0.8894107\n",
      "  0.78773396  0.94595341  0.61947196  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 124\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0118685266260124 R2: 0.006897278899548831 time: 1703258562.3874435\n",
      "batch_idx: 1 loss: 0.016726619255864234 R2: 0.006946930185799838 time: 1703258569.0431476\n",
      "Training [41%] Loss: 0.014297572940938318 time: 1703258569.0431476\n",
      "weight: [ 0.455923   -0.06495157  0.00484719  0.5834188   0.74941609  0.15689851\n",
      " -0.07356824  0.87173674  0.24133588  0.16080805  0.54873379  0.6918952\n",
      "  0.31578742  0.07605024  0.56308769  0.06700216  0.3253997   0.88947843\n",
      "  0.78809859  0.94573678  0.61720901  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 125\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011869584778785313 R2: 0.006987500981467054 time: 1703258575.5633135\n",
      "batch_idx: 1 loss: 0.016722875769324557 R2: 0.00703711414836139 time: 1703258582.3402941\n",
      "Training [42%] Loss: 0.014296230274054935 time: 1703258582.3402941\n",
      "weight: [ 0.45569306 -0.06520951  0.00446276  0.58170563  0.74948492  0.15698961\n",
      " -0.07404977  0.87234817  0.2423086   0.16080805  0.54873379  0.6918952\n",
      "  0.31514726  0.07582581  0.56282974  0.06751275  0.3253997   0.88954727\n",
      "  0.78846554  0.94551915  0.61493013  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 126\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011870631546582683 R2: 0.0070776613226415375 time: 1703258589.7746391\n",
      "batch_idx: 1 loss: 0.01671914322815459 R2: 0.007127216896041011 time: 1703258597.689246\n",
      "Training [42%] Loss: 0.014294887387368636 time: 1703258597.689246\n",
      "weight: [ 0.45546554 -0.06546979  0.00407599  0.57998626  0.74955491  0.15708144\n",
      " -0.07452603  0.87296165  0.24327911  0.16080805  0.54873379  0.6918952\n",
      "  0.31452422  0.0755993   0.56256947  0.06802515  0.3253997   0.88961725\n",
      "  0.78883483  0.94530057  0.61263542  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 127\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011871667907046392 R2: 0.0071677597689797645 time: 1703258604.877403\n",
      "batch_idx: 1 loss: 0.016715421888100778 R2: 0.007217234408424922 time: 1703258611.620954\n",
      "Training [42%] Loss: 0.014293544897573585 time: 1703258611.620954\n",
      "weight: [ 0.45524045 -0.06573237  0.00368689  0.57826077  0.74962604  0.15717401\n",
      " -0.07499684  0.87357711  0.24424717  0.16080805  0.54873379  0.6918952\n",
      "  0.31391834  0.07537072  0.56230688  0.06853933  0.3253997   0.88968838\n",
      "  0.78920642  0.94508103  0.61032493  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 128\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011872692651123895 R2: 0.007257789741308307 time: 1703258618.0890887\n",
      "batch_idx: 1 loss: 0.016711713859605064 R2: 0.007307168104184747 time: 1703258624.7763588\n",
      "Training [43%] Loss: 0.01429220325536448 time: 1703258624.7763588\n",
      "weight: [ 0.45501777 -0.0659972   0.00329557  0.57652919  0.74969829  0.15726727\n",
      " -0.07546211  0.87419446  0.2452125   0.16080805  0.54873379  0.6918952\n",
      "  0.31332932  0.07514014  0.56204206  0.06905524  0.3253997   0.88976063\n",
      "  0.78958021  0.94486045  0.60799864  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 129\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011873703297155008 R2: 0.0073477405458980405 time: 1703258631.5376947\n",
      "batch_idx: 1 loss: 0.016708021793023258 R2: 0.0073970213718626 time: 1703258638.373135\n",
      "Training [43%] Loss: 0.014290862545089133 time: 1703258638.373135\n",
      "weight: [ 0.45479748 -0.06626418  0.00290213  0.57479156  0.74977161  0.15736118\n",
      " -0.07592182  0.87481363  0.24617484  0.16080805  0.54873379  0.6918952\n",
      "  0.31275683  0.07490762  0.56177508  0.06957285  0.3253997   0.88983396\n",
      "  0.78995605  0.94463871  0.6056565   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 130\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011874697588896857 R2: 0.007437601759595958 time: 1703258645.6193483\n",
      "batch_idx: 1 loss: 0.016704347433408852 R2: 0.007486795369044619 time: 1703258652.8990476\n",
      "Training [43%] Loss: 0.014289522511152855 time: 1703258652.8990476\n",
      "weight: [ 0.45457955 -0.06653325  0.00250665  0.57304794  0.749846    0.15745571\n",
      " -0.07637587  0.87543455  0.24713391  0.16080805  0.54873379  0.6918952\n",
      "  0.31220063  0.07467322  0.561506    0.07009214  0.3253997   0.88990834\n",
      "  0.79033385  0.94441577  0.60329854  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 131\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011875674677984319 R2: 0.007527366965476512 time: 1703258660.3011916\n",
      "batch_idx: 1 loss: 0.016700690866016644 R2: 0.007576486527606008 time: 1703258667.1396787\n",
      "Training [44%] Loss: 0.014288182772000481 time: 1703258667.1396787\n",
      "weight: [ 0.45436403 -0.0668044   0.00210915  0.57129838  0.74992144  0.15755086\n",
      " -0.07682407  0.87605712  0.24808948  0.16080805  0.54873379  0.6918952\n",
      "  0.31166072  0.07443694  0.56123485  0.07061305  0.3253997   0.88998378\n",
      "  0.79071358  0.94419159  0.60092479  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 132\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011876635365813525 R2: 0.007617034823398927 time: 1703258673.8979752\n",
      "batch_idx: 1 loss: 0.016697050792868963 R2: 0.007666087086784623 time: 1703258680.5183082\n",
      "Training [44%] Loss: 0.014286843079341244 time: 1703258680.5183082\n",
      "weight: [ 0.45415095 -0.06707766  0.00170962  0.56954297  0.74999796  0.15764663\n",
      " -0.07726619  0.87668125  0.2490413   0.16080805  0.54873379  0.6918952\n",
      "  0.31113727  0.07419877  0.5609616   0.07113554  0.3253997   0.8900603\n",
      "  0.79109524  0.94396624  0.59853537  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 133\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011877581382106263 R2: 0.007706607157458412 time: 1703258687.1846704\n",
      "batch_idx: 1 loss: 0.016693425555944323 R2: 0.007755588050685103 time: 1703258693.916528\n",
      "Training [44%] Loss: 0.014285503469025292 time: 1703258693.916528\n",
      "weight: [ 0.45394039 -0.06735305  0.00130805  0.5677818   0.75007559  0.15774306\n",
      " -0.07770194  0.87730685  0.24998914  0.16080805  0.54873379  0.6918952\n",
      "  0.31063049  0.07395868  0.56068621  0.07165955  0.3253997   0.89013793\n",
      "  0.79147884  0.94373976  0.59613039  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 134\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011878514233646947 R2: 0.007796085432246969 time: 1703258700.283116\n",
      "batch_idx: 1 loss: 0.016689814223899098 R2: 0.007844982556282742 time: 1703258706.8371663\n",
      "Training [45%] Loss: 0.014284164228773023 time: 1703258706.8371663\n",
      "weight: [ 4.53732396e-01 -6.76305760e-02  9.04421488e-04  5.66014948e-01\n",
      "  7.50154339e-01  1.57840135e-01 -7.81310927e-02  8.77933797e-01\n",
      "  2.50932755e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  3.10140469e-01  7.37166443e-02  5.60408680e-01  7.21850296e-02\n",
      "  3.25399698e-01  8.90216684e-01  7.91864380e-01  9.43512189e-01\n",
      "  5.93709956e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 135\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011879434346681934 R2: 0.0078854678287692 time: 1703258713.4732497\n",
      "batch_idx: 1 loss: 0.016686217098041217 R2: 0.007934267612042523 time: 1703258720.272234\n",
      "Training [45%] Loss: 0.014282825722361576 time: 1703258720.272234\n",
      "weight: [ 4.53526973e-01 -6.79102272e-02  4.98786401e-04  5.64242466e-01\n",
      "  7.50234212e-01  1.57937853e-01 -7.85535111e-02  8.78562016e-01\n",
      "  2.51871888e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  3.09667099e-01  7.34726920e-02  5.60129029e-01  7.27119295e-02\n",
      "  3.25399698e-01  8.90296557e-01  7.92251808e-01  9.43283508e-01\n",
      "  5.91274128e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 136\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011880340971822464 R2: 0.007974748629484218 time: 1703258727.6672018\n",
      "batch_idx: 1 loss: 0.016682635409262535 R2: 0.008023443344427617 time: 1703258734.9623396\n",
      "Training [45%] Loss: 0.0142814881905425 time: 1703258734.9623396\n",
      "weight: [ 4.53324114e-01 -6.81919502e-02  9.12048674e-05  5.62464398e-01\n",
      "  7.50315185e-01  1.58036189e-01 -7.89691015e-02  8.79191415e-01\n",
      "  2.52806275e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  3.09210144e-01  7.32268605e-02  5.59847306e-01  7.32402172e-02\n",
      "  3.25399698e-01  8.90377531e-01  7.92641038e-01  9.43053667e-01\n",
      "  5.88822951e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 137\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011881232813401522 R2: 0.008063920062522323 time: 1703258742.18133\n",
      "batch_idx: 1 loss: 0.01667907048893711 R2: 0.008112510442030207 time: 1703258748.9156709\n",
      "Training [46%] Loss: 0.014280151651169316 time: 1703258748.9156709\n",
      "weight: [ 4.53123807e-01 -6.84756945e-02 -3.18256468e-04  5.60680784e-01\n",
      "  7.50397239e-01  1.58135116e-01 -7.93777870e-02  8.79821904e-01\n",
      "  2.53735649e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  3.08769344e-01  7.29791921e-02  5.59563562e-01  7.37698586e-02\n",
      "  3.25399698e-01  8.90459585e-01  7.93031982e-01  9.42822610e-01\n",
      "  5.86356473e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 138\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011882108913051276 R2: 0.008152975201169421 time: 1703258755.7390134\n",
      "batch_idx: 1 loss: 0.016675522984133065 R2: 0.008201467523501038 time: 1703258762.4439437\n",
      "Training [46%] Loss: 0.01427881594859217 time: 1703258762.4439437\n",
      "weight: [ 4.52926055e-01 -6.87614273e-02 -7.29550762e-04  5.58891670e-01\n",
      "  7.50480363e-01  1.58234617e-01 -7.97794558e-02  8.80453390e-01\n",
      "  2.54659747e-01  1.60808051e-01  5.48733789e-01  6.91895198e-01\n",
      "  3.08344526e-01  7.27297129e-02  5.59277829e-01  7.43008140e-02\n",
      "  3.25399698e-01  8.90542709e-01  7.93424575e-01  9.42590310e-01\n",
      "  5.83874765e-01  5.68308603e-01  9.36747678e-02  3.67715803e-01]\n",
      "epoch 139\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011882969215180283 R2: 0.008241910001121178 time: 1703258769.0515451\n",
      "batch_idx: 1 loss: 0.016671992594886074 R2: 0.008290310081948138 time: 1703258775.5091467\n",
      "Training [46%] Loss: 0.014277480905033179 time: 1703258775.5091467\n",
      "weight: [ 0.45273088 -0.06904914 -0.00114266  0.55709711  0.75056456  0.15833469\n",
      " -0.08017394  0.88108577  0.25557832  0.16080805  0.54873379  0.6918952\n",
      "  0.30793563  0.07247843  0.55899012  0.07483303  0.3253997   0.89062691\n",
      "  0.79381878  0.94235677  0.58137794  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 140\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011883814503474841 R2: 0.008330723296866904 time: 1703258781.9693384\n",
      "batch_idx: 1 loss: 0.016668478431912066 R2: 0.008379031525742042 time: 1703258788.5007672\n",
      "Training [47%] Loss: 0.014276146467693453 time: 1703258788.5007672\n",
      "weight: [ 0.45253832 -0.06933884 -0.00155757  0.55529718  0.75064984  0.15843533\n",
      " -0.08056106  0.88171894  0.25649111  0.16080805  0.54873379  0.6918952\n",
      "  0.30754265  0.07222533  0.55870042  0.07536647  0.3253997   0.89071219\n",
      "  0.79421459  0.94212203  0.57886612  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 141\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01188464582677716 R2: 0.008419414967612004 time: 1703258795.5833008\n",
      "batch_idx: 1 loss: 0.01666497969166087 R2: 0.008467625430069514 time: 1703258803.4388144\n",
      "Training [47%] Loss: 0.014274812759219015 time: 1703258803.4388144\n",
      "weight: [ 0.45234841 -0.06963053 -0.00197429  0.55349194  0.75073623  0.15853656\n",
      " -0.08094063  0.8823528   0.25739789  0.16080805  0.54873379  0.6918952\n",
      "  0.30716558  0.0719704   0.55840872  0.07590105  0.3253997   0.89079857\n",
      "  0.79461199  0.9418861   0.57633946  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 142\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011885463842624188 R2: 0.008507983623868398 time: 1703258810.8049865\n",
      "batch_idx: 1 loss: 0.016661496178465837 R2: 0.008556087409399948 time: 1703258817.6610308\n",
      "Training [47%] Loss: 0.014273480010545013 time: 1703258817.6610308\n",
      "weight: [ 0.45216116 -0.0699242  -0.00239278  0.55168143  0.75082371  0.15863836\n",
      " -0.08131251  0.88298724  0.25829838  0.16080805  0.54873379  0.6918952\n",
      "  0.30680429  0.07171367  0.55811506  0.07643674  0.3253997   0.89088606\n",
      "  0.79501093  0.941649    0.57379807  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 143\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011886268509901547 R2: 0.008596425364765237 time: 1703258824.3490465\n",
      "batch_idx: 1 loss: 0.016658028356377672 R2: 0.0086444154263573 time: 1703258830.7533894\n",
      "Training [48%] Loss: 0.01427214843313961 time: 1703258830.7533894\n",
      "weight: [ 0.45197658 -0.07021982 -0.002813    0.54986569  0.75091228  0.15874071\n",
      " -0.08167661  0.88362215  0.25919233  0.16080805  0.54873379  0.6918952\n",
      "  0.30645859  0.07145514  0.55781944  0.07697349  0.3253997   0.89097463\n",
      "  0.79541134  0.94141069  0.57124206  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 144\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011887059285550798 R2: 0.008684734316477316 time: 1703258837.5744364\n",
      "batch_idx: 1 loss: 0.016654576957017827 R2: 0.008732608490647165 time: 1703258844.187573\n",
      "Training [48%] Loss: 0.014270818121284314 time: 1703258844.187573\n",
      "weight: [ 0.45179465 -0.07051735 -0.00323491  0.54804476  0.75100193  0.15884361\n",
      " -0.08203289  0.88425745  0.26007949  0.16080805  0.54873379  0.6918952\n",
      "  0.30612822  0.07119486  0.55752191  0.07751127  0.3253997   0.89106428\n",
      "  0.79581317  0.94117116  0.56867155  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 145\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011887835631645558 R2: 0.008772904400461079 time: 1703258851.256482\n",
      "batch_idx: 1 loss: 0.016651142459839247 R2: 0.008820664773545484 time: 1703258858.4811099\n",
      "Training [48%] Loss: 0.014269489045742403 time: 1703258858.4811099\n",
      "weight: [ 0.45161539 -0.07081676 -0.00365846  0.54621868  0.75109264  0.15894702\n",
      " -0.08238126  0.88489302  0.26095958  0.16080805  0.54873379  0.6918952\n",
      "  0.30581296  0.07093283  0.5572225   0.07805002  0.3253997   0.89115499\n",
      "  0.79621636  0.94093038  0.56608666  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 146\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011888597455812092 R2: 0.008860931001685834 time: 1703258865.7253673\n",
      "batch_idx: 1 loss: 0.01664772480912205 R2: 0.008908580470170735 time: 1703258872.8521967\n",
      "Training [49%] Loss: 0.01426816113246707 time: 1703258872.8521967\n",
      "weight: [ 0.45143879 -0.07111804 -0.00408361  0.54438748  0.75118442  0.15905095\n",
      " -0.08272164  0.88552876  0.26183236  0.16080805  0.54873379  0.6918952\n",
      "  0.30551267  0.07066908  0.55692122  0.0785897   0.3253997   0.89124676\n",
      "  0.79662084  0.94068835  0.56348753  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 147\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011889345196906211 R2: 0.008948811409947965 time: 1703258879.764047\n",
      "batch_idx: 1 loss: 0.01664432352905878 R2: 0.008996350112619234 time: 1703258886.4758558\n",
      "Training [49%] Loss: 0.014266834362982494 time: 1703258886.4758558\n",
      "weight: [ 0.45126488 -0.07142118 -0.00451037  0.5425512   0.75127726  0.15915539\n",
      " -0.0830539   0.88616457  0.26269757  0.16080805  0.54873379  0.6918952\n",
      "  0.3052272   0.0704036   0.55661808  0.07913025  0.3253997   0.89133961\n",
      "  0.7970266   0.94044509  0.56087435  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 148\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011890079542823142 R2: 0.009036543859685997 time: 1703258893.2491102\n",
      "batch_idx: 1 loss: 0.01664093810729699 R2: 0.009083967961433737 time: 1703258899.9194114\n",
      "Training [49%] Loss: 0.014265508825060067 time: 1703258899.9194114\n",
      "weight: [ 0.45109368 -0.07172617 -0.00493869  0.5407099   0.75137118  0.15926034\n",
      " -0.08337796  0.88680032  0.26355497  0.16080805  0.54873379  0.6918952\n",
      "  0.30495645  0.07013641  0.55631309  0.07967162  0.3253997   0.89143352\n",
      "  0.7974336   0.94020061  0.55824728  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 149\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011890801021669553 R2: 0.009124125980398246 time: 1703258906.6008842\n",
      "batch_idx: 1 loss: 0.01663756834733774 R2: 0.009171429380081308 time: 1703258913.493064\n",
      "Training [50%] Loss: 0.014264184684503647 time: 1703258913.493064\n",
      "weight: [ 0.4509252  -0.072033   -0.00536858  0.5388636   0.75146616  0.15936579\n",
      " -0.08369373  0.88743591  0.26440429  0.16080805  0.54873379  0.6918952\n",
      "  0.30470025  0.0698675   0.55600625  0.08021375  0.3253997   0.89152851\n",
      "  0.79784179  0.93995493  0.55560652  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 150\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011891509766710074 R2: 0.00921155380805494 time: 1703258920.8912227\n",
      "batch_idx: 1 loss: 0.016634214447764285 R2: 0.009258731217632987 time: 1703258928.1311266\n",
      "Training [50%] Loss: 0.014262862107237179 time: 1703258928.1311266\n",
      "weight: [ 0.45075944 -0.07234166 -0.00579997  0.53701233  0.75156221  0.15947173\n",
      " -0.08400118  0.88807123  0.2652453   0.16080805  0.54873379  0.6918952\n",
      "  0.30445837  0.06959689  0.5556976   0.08075659  0.3253997   0.89162456\n",
      "  0.79825114  0.93970804  0.55295222  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 151\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011892205597360334 R2: 0.009298821999807805 time: 1703258935.1842446\n",
      "batch_idx: 1 loss: 0.016630876788355016 R2: 0.009345871028797914 time: 1703258941.812216\n",
      "Training [50%] Loss: 0.014261541192857675 time: 1703258941.812216\n",
      "weight: [ 0.4505964  -0.0726521  -0.00623285  0.53515612  0.75165932  0.15957815\n",
      " -0.08430028  0.88870617  0.26607773  0.16080805  0.54873379  0.6918952\n",
      "  0.30423058  0.06932461  0.55538716  0.08130009  0.3253997   0.89172166\n",
      "  0.79866156  0.93945993  0.55028458  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 152\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01189288831690501 R2: 0.009385924962565628 time: 1703258948.2382288\n",
      "batch_idx: 1 loss: 0.01662755561490523 R2: 0.009432845815616875 time: 1703258954.8215234\n",
      "Training [51%] Loss: 0.01426022196590512 time: 1703258954.8215234\n",
      "weight: [ 0.45043608 -0.0729643  -0.00666716  0.53329498  0.75175747  0.15968503\n",
      " -0.08459102  0.88934062  0.26690133  0.16080805  0.54873379  0.6918952\n",
      "  0.30401663  0.06905067  0.55507495  0.0818442   0.3253997   0.89181981\n",
      "  0.79907302  0.93921058  0.54760377  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 153\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011893557984045292 R2: 0.009472857968554127 time: 1703258961.4508579\n",
      "batch_idx: 1 loss: 0.016624250864333397 R2: 0.009519651259800477 time: 1703258968.2521513\n",
      "Training [51%] Loss: 0.014258904424189344 time: 1703258968.2521513\n",
      "weight: [ 0.45027848 -0.07327826 -0.00710289  0.53142895  0.75185666  0.15979236\n",
      " -0.08487337  0.88997447  0.26771586  0.16080805  0.54873379  0.6918952\n",
      "  0.30381634  0.06877509  0.554761    0.08238886  0.3253997   0.891919\n",
      "  0.79948546  0.93896002  0.54491001  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 154\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011894214963924515 R2: 0.009559617436038725 time: 1703258975.1655273\n",
      "batch_idx: 1 loss: 0.016620962239680785 R2: 0.009606281958686513 time: 1703258982.2333703\n",
      "Training [51%] Loss: 0.014257588601802651 time: 1703258982.2333703\n",
      "weight: [ 0.45012363 -0.07359395 -0.00754     0.52955804  0.75195689  0.15990014\n",
      " -0.0851473   0.89060761  0.26852106  0.16080805  0.54873379  0.6918952\n",
      "  0.30362952  0.06849787  0.55444531  0.08293401  0.3253997   0.89201923\n",
      "  0.79989884  0.93870824  0.54220353  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 155\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011894859749873461 R2: 0.00964620025696239 time: 1703258989.4417665\n",
      "batch_idx: 1 loss: 0.016617689446836444 R2: 0.009692732375456647 time: 1703258996.679088\n",
      "Training [52%] Loss: 0.014256274598354952 time: 1703258996.679088\n",
      "weight: [ 0.44997152 -0.07391137 -0.00797849  0.52768227  0.75205816  0.16000836\n",
      " -0.08541279  0.89123991  0.26931669  0.16080805  0.54873379  0.6918952\n",
      "  0.30345599  0.06821901  0.55412789  0.0834796   0.3253997   0.89212051\n",
      "  0.80031313  0.93845528  0.53948456  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 156\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011895492718237356 R2: 0.00973260277953003 time: 1703259003.7813246\n",
      "batch_idx: 1 loss: 0.016614432395175887 R2: 0.009778997703314807 time: 1703259010.9461174\n",
      "Training [52%] Loss: 0.014254962556706621 time: 1703259010.9461174\n",
      "weight: [ 0.44982216 -0.0742305  -0.00841832  0.52580167  0.75216047  0.16011701\n",
      " -0.08566984  0.89187126  0.27010252  0.16080805  0.54873379  0.6918952\n",
      "  0.30329554  0.06793854  0.55380876  0.08402555  0.3253997   0.89222282\n",
      "  0.80072827  0.93820113  0.53675335  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 157\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011896114008698772 R2: 0.009818820254296923 time: 1703259017.5581336\n",
      "batch_idx: 1 loss: 0.016611191216026402 R2: 0.009865073991526474 time: 1703259024.2834494\n",
      "Training [52%] Loss: 0.014253652612362586 time: 1703259024.2834494\n",
      "weight: [ 0.44967556 -0.07455132 -0.00885945  0.52391625  0.75226381  0.16022609\n",
      " -0.08591848  0.89250155  0.27087828  0.16080805  0.54873379  0.6918952\n",
      "  0.30314794  0.06765646  0.55348794  0.08457182  0.3253997   0.89232615\n",
      "  0.80114422  0.93794581  0.53401012  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 158\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011896723603567404 R2: 0.009904847114623139 time: 1703259030.7592354\n",
      "batch_idx: 1 loss: 0.016607966109719915 R2: 0.009950957516850156 time: 1703259037.0830386\n",
      "Training [53%] Loss: 0.014252344856643659 time: 1703259037.0830386\n",
      "weight: [ 0.44953172 -0.0748738  -0.00930186  0.522026    0.75236817  0.16033558\n",
      " -0.08615873  0.89313067  0.27164374  0.16080805  0.54873379  0.6918952\n",
      "  0.30301295  0.06737279  0.55316546  0.08511833  0.3253997   0.89243051\n",
      "  0.80156093  0.93768931  0.53125514  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 159\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011897321518173283 R2: 0.009990677776948842 time: 1703259043.7538223\n",
      "batch_idx: 1 loss: 0.01660475716238319 R2: 0.010036643972629933 time: 1703259050.6567726\n",
      "Training [53%] Loss: 0.014251039340278236 time: 1703259050.6567726\n",
      "weight: [ 0.44939063 -0.07519793 -0.00974552  0.52013095  0.75247354  0.16044547\n",
      " -0.08639065  0.89375849  0.27239867  0.16080805  0.54873379  0.6918952\n",
      "  0.30289034  0.06708755  0.55284133  0.08566503  0.3253997   0.89253589\n",
      "  0.80197833  0.93743164  0.52848865  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 160\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011897907939739601 R2: 0.010076307272963492 time: 1703259057.8367107\n",
      "batch_idx: 1 loss: 0.0166015642792384 R2: 0.010122128132801 time: 1703259064.889717\n",
      "Training [53%] Loss: 0.014249736109489 time: 1703259064.889717\n",
      "weight: [ 0.4492523  -0.07552369 -0.01019039  0.51823108  0.75257992  0.16055575\n",
      " -0.08661427  0.89438491  0.27314281  0.16080805  0.54873379  0.6918952\n",
      "  0.30277989  0.06680075  0.55251557  0.08621186  0.3253997   0.89264227\n",
      "  0.80239639  0.93717282  0.52571095  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 161\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011898483212897163 R2: 0.010161731232601334 time: 1703259071.4723237\n",
      "batch_idx: 1 loss: 0.016598387271148082 R2: 0.010207404202748904 time: 1703259078.7971241\n",
      "Training [54%] Loss: 0.014248435242022623 time: 1703259078.7971241\n",
      "weight: [ 0.44911673 -0.07585106 -0.01063646  0.51632641  0.75268731  0.16066641\n",
      " -0.08682964  0.8950098   0.27387594  0.16080805  0.54873379  0.6918952\n",
      "  0.3026814   0.06651239  0.55218819  0.08675873  0.3253997   0.89274966\n",
      "  0.80281506  0.93691286  0.52292233  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 162\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011899047703838513 R2: 0.010246945306132682 time: 1703259085.136223\n",
      "batch_idx: 1 loss: 0.0165952260050823 R2: 0.010292466495668506 time: 1703259091.3262472\n",
      "Training [54%] Loss: 0.014247136854460406 time: 1703259091.3262472\n",
      "weight: [ 0.44898394 -0.07618004 -0.01108369  0.51441695  0.7527957   0.16077745\n",
      " -0.0870368   0.89563306  0.27459781  0.16080805  0.54873379  0.6918952\n",
      "  0.30259466  0.06622249  0.55185921  0.08730559  0.3253997   0.89285805\n",
      "  0.8032343   0.93665178  0.52012307  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 163\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011899601667620115 R2: 0.010331944555985961 time: 1703259099.4235966\n",
      "batch_idx: 1 loss: 0.016592080489527518 R2: 0.010377309848668914 time: 1703259107.0722916\n",
      "Training [54%] Loss: 0.014245841078573816 time: 1703259107.0722916\n",
      "weight: [ 0.44885393 -0.0765106  -0.01153207  0.51250267  0.75290509  0.16088886\n",
      " -0.08723583  0.89625455  0.27530821  0.16080805  0.54873379  0.6918952\n",
      "  0.30251944  0.06593105  0.55152865  0.08785235  0.3253997   0.89296743\n",
      "  0.80365406  0.9363896   0.5173135   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 164\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01190014522459647 R2: 0.010416723316794885 time: 1703259113.9621594\n",
      "batch_idx: 1 loss: 0.01658895083500513 R2: 0.010461929469456677 time: 1703259120.4424667\n",
      "Training [55%] Loss: 0.0142445480298008 time: 1703259120.4424667\n",
      "weight: [ 0.44872669 -0.07684273 -0.01198156  0.51058358  0.75301546  0.16100062\n",
      " -0.08742682  0.89687417  0.27600689  0.16080805  0.54873379  0.6918952\n",
      "  0.30245552  0.0656381   0.55119653  0.08839896  0.3253997   0.8930778\n",
      "  0.80407429  0.93612633  0.51449391  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 165\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011900678449766883 R2: 0.010501275583783332 time: 1703259127.0020595\n",
      "batch_idx: 1 loss: 0.016585837139389718 R2: 0.010546320408004739 time: 1703259133.4270942\n",
      "Training [55%] Loss: 0.014243257794578301 time: 1703259133.4270942\n",
      "weight: [ 0.44860222 -0.07717639 -0.01243212  0.50865967  0.75312681  0.16111273\n",
      " -0.08760985  0.8974918   0.27669362  0.16080805  0.54873379  0.6918952\n",
      "  0.30240265  0.06534364  0.55086286  0.08894533  0.3253997   0.89318915\n",
      "  0.80449494  0.93586198  0.51166464  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 166\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011901201485901696 R2: 0.010585595545539173 time: 1703259140.087484\n",
      "batch_idx: 1 loss: 0.016582739403056178 R2: 0.010630477138984018 time: 1703259146.733526\n",
      "Training [55%] Loss: 0.014241970444478936 time: 1703259146.733526\n",
      "weight: [ 0.44848053 -0.07751158 -0.01288375  0.50673092  0.75323912  0.16122518\n",
      " -0.08778503  0.89810732  0.27736819  0.16080805  0.54873379  0.6918952\n",
      "  0.30236062  0.06504769  0.55052767  0.0894914   0.3253997   0.89330147\n",
      "  0.80491596  0.93559656  0.50882601  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 167\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011901714583340165 R2: 0.010669677799918675 time: 1703259153.6170447\n",
      "batch_idx: 1 loss: 0.016579657540303615 R2: 0.010714393594800531 time: 1703259160.6822681\n",
      "Training [56%] Loss: 0.01424068606182189 time: 1703259160.6822681\n",
      "weight: [ 0.44836162 -0.07784828 -0.01333639  0.50479731  0.75335241  0.16133795\n",
      " -0.08795244  0.89872061  0.27803035  0.16080805  0.54873379  0.6918952\n",
      "  0.30232921  0.06475026  0.55019097  0.09003708  0.3253997   0.89341475\n",
      "  0.80533731  0.93533011  0.50597838  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 168\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01190221804329531 R2: 0.01075351711160577 time: 1703259167.4133465\n",
      "batch_idx: 1 loss: 0.016576591464056788 R2: 0.010798063569763539 time: 1703259174.04213\n",
      "Training [56%] Loss: 0.01423940475367605 time: 1703259174.04213\n",
      "weight: [ 0.4482455  -0.07818648 -0.01379005  0.50285884  0.75346665  0.16145105\n",
      " -0.08811219  0.89933157  0.2786799   0.16080805  0.54873379  0.6918952\n",
      "  0.30230821  0.06445137  0.54985278  0.09058229  0.3253997   0.893529\n",
      "  0.80575893  0.93506264  0.5031221   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 169\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011902712125792313 R2: 0.010837107976395921 time: 1703259180.3470519\n",
      "batch_idx: 1 loss: 0.016573541161938772 R2: 0.010881481107949287 time: 1703259187.969225\n",
      "Training [56%] Loss: 0.014238126643865543 time: 1703259187.969225\n",
      "weight: [ 0.44813216 -0.07852614 -0.01424467  0.50091547  0.75358184  0.16156445\n",
      " -0.0882644   0.89994006  0.27931661  0.16080805  0.54873379  0.6918952\n",
      "  0.3022974   0.06415101  0.54951311  0.09112695  0.3253997   0.89364419\n",
      "  0.80618079  0.93479416  0.50025753  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 170\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011903197005296157 R2: 0.010920444393640483 time: 1703259195.922054\n",
      "batch_idx: 1 loss: 0.016570506700070212 R2: 0.010964640546075266 time: 1703259202.677297\n",
      "Training [57%] Loss: 0.014236851852683184 time: 1703259202.677297\n",
      "weight: [ 0.4480216  -0.07886727 -0.01470024  0.49896719  0.75369797  0.16167816\n",
      " -0.08840918  0.90054598  0.27994026  0.16080805  0.54873379  0.6918952\n",
      "  0.30229655  0.06384922  0.54917199  0.09167098  0.3253997   0.89376032\n",
      "  0.80660284  0.93452472  0.49738505  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 171\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011903672805158054 R2: 0.011003520022601854 time: 1703259209.1224105\n",
      "batch_idx: 1 loss: 0.016567488161638434 R2: 0.011047536221088783 time: 1703259215.5022047\n",
      "Training [57%] Loss: 0.014235580483398243 time: 1703259215.5022047\n",
      "weight: [ 0.44791382 -0.07920983 -0.01515673  0.49701395  0.75381503  0.16179215\n",
      " -0.08854667  0.90114921  0.28055064  0.16080805  0.54873379  0.6918952\n",
      "  0.30230544  0.06354599  0.54882943  0.0922143   0.3253997   0.89387738\n",
      "  0.80702501  0.93425431  0.49450503  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 172\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011904139669747214 R2: 0.011086328543206236 time: 1703259221.932119\n",
      "batch_idx: 1 loss: 0.01656448558291001 R2: 0.011130162145778888 time: 1703259228.357278\n",
      "Training [57%] Loss: 0.014234312626328611 time: 1703259228.357278\n",
      "weight: [ 0.44780883 -0.0795538  -0.01561411  0.49505574  0.75393302  0.16190643\n",
      " -0.088677    0.90174964  0.28114753  0.16080805  0.54873379  0.6918952\n",
      "  0.30232385  0.06324135  0.54848545  0.09275682  0.3253997   0.89399536\n",
      "  0.80744728  0.93398296  0.49161787  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 173\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01190459780586139 R2: 0.0111688638831835 time: 1703259234.7508411\n",
      "batch_idx: 1 loss: 0.016561498943081225 R2: 0.011212511949522885 time: 1703259241.2324264\n",
      "Training [58%] Loss: 0.014233048374471306 time: 1703259241.2324264\n",
      "weight: [ 0.44770662 -0.07989918 -0.01607235  0.49309251  0.75405191  0.16202097\n",
      " -0.0888003   0.90234714  0.28173072  0.16080805  0.54873379  0.6918952\n",
      "  0.30235157  0.06293531  0.54814008  0.09329844  0.3253997   0.89411425\n",
      "  0.80786958  0.9337107   0.48872395  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 174\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01190504746130743 R2: 0.011251120132702797 time: 1703259247.777258\n",
      "batch_idx: 1 loss: 0.016558528209145617 R2: 0.011294579110477598 time: 1703259254.5941849\n",
      "Training [58%] Loss: 0.014231787835226523 time: 1703259254.5941849\n",
      "weight: [ 0.4476072  -0.08024593 -0.01653143  0.49112423  0.7541717   0.16213577\n",
      " -0.08891672  0.90294161  0.2823      0.16080805  0.54873379  0.6918952\n",
      "  0.30238838  0.06262788  0.54779333  0.09383909  0.3253997   0.89423404\n",
      "  0.80829188  0.93343754  0.4858237   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 175\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011905488868621875 R2: 0.011333091269280948 time: 1703259261.3424048\n",
      "batch_idx: 1 loss: 0.016555573387818254 R2: 0.011376357240833634 time: 1703259267.7120433\n",
      "Training [58%] Loss: 0.014230531128220064 time: 1703259267.7120433\n",
      "weight: [ 0.44751056 -0.08059404 -0.01699131  0.48915085  0.75429238  0.16225082\n",
      " -0.08902638  0.90353294  0.28285517  0.16080805  0.54873379  0.6918952\n",
      "  0.30243408  0.06231908  0.54744521  0.09437867  0.3253997   0.89435472\n",
      "  0.80871411  0.93316353  0.48291752  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 176\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011905922209233479 R2: 0.011414770973002586 time: 1703259274.092441\n",
      "batch_idx: 1 loss: 0.016552634536803033 R2: 0.011457840167557909 time: 1703259281.1595685\n",
      "Training [59%] Loss: 0.014229278373018257 time: 1703259281.1595685\n",
      "weight: [ 0.4474167  -0.0809435  -0.01745198  0.48717234  0.75441393  0.1623661\n",
      " -0.08912945  0.904121    0.28339603  0.16080805  0.54873379  0.6918952\n",
      "  0.30248844  0.06200891  0.54709576  0.09491709  0.3253997   0.89447628\n",
      "  0.80913625  0.93288867  0.48000582  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 177\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011906347627349477 R2: 0.011496152696817807 time: 1703259287.864208\n",
      "batch_idx: 1 loss: 0.016549711731037052 R2: 0.01153902176946453 time: 1703259294.132228\n",
      "Training [59%] Loss: 0.014228029679193264 time: 1703259294.132228\n",
      "weight: [ 0.44732561 -0.08129427 -0.01791339  0.48518863  0.75453635  0.16248161\n",
      " -0.08922607  0.90470569  0.28392236  0.16080805  0.54873379  0.6918952\n",
      "  0.30255126  0.0616974   0.54674499  0.09545425  0.3253997   0.8945987\n",
      "  0.80955822  0.93261299  0.47708905  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 178\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011906765273649023 R2: 0.011577229903207531 time: 1703259300.8274481\n",
      "batch_idx: 1 loss: 0.01654680502118251 R2: 0.011619895761115107 time: 1703259307.332449\n",
      "Training [59%] Loss: 0.014226785147415766 time: 1703259307.332449\n",
      "weight: [ 0.44723731 -0.08164634 -0.01837553  0.48319968  0.75465962  0.16259732\n",
      " -0.0893164   0.90528691  0.28443397  0.16080805  0.54873379  0.6918952\n",
      "  0.30262233  0.06138456  0.54639292  0.09599006  0.3253997   0.89472197\n",
      "  0.80998     0.93233652  0.47416762  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 179\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011907175334360913 R2: 0.011657996235983847 time: 1703259314.1533546\n",
      "batch_idx: 1 loss: 0.016543914423539734 R2: 0.011700455640796026 time: 1703259320.5573666\n",
      "Training [60%] Loss: 0.014225544878950324 time: 1703259320.5573666\n",
      "weight: [ 0.44715177 -0.08199969 -0.01883836  0.48120543  0.75478373  0.16271324\n",
      " -0.0894006   0.90586452  0.28493067  0.16080805  0.54873379  0.6918952\n",
      "  0.30270142  0.06107041  0.54603957  0.09652442  0.3253997   0.89484607\n",
      "  0.81040152  0.93205928  0.47124198  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 180\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011907578020999322 R2: 0.011738445485321991 time: 1703259326.9783509\n",
      "batch_idx: 1 loss: 0.016541039945948133 R2: 0.011780694840136663 time: 1703259333.7372766\n",
      "Training [60%] Loss: 0.014224308983473728 time: 1703259333.7372766\n",
      "weight: [ 0.447069   -0.08235429 -0.01930185  0.47920582  0.75490865  0.16282935\n",
      " -0.08947883  0.90643844  0.28541226  0.16080805  0.54873379  0.6918952\n",
      "  0.30278836  0.06075496  0.54568496  0.09705724  0.3253997   0.894971\n",
      "  0.81082274  0.93178129  0.46831258  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 181\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011907973536083154 R2: 0.01181857141624243 time: 1703259340.24283\n",
      "batch_idx: 1 loss: 0.016538181619999537 R2: 0.011860606918644323 time: 1703259347.4474597\n",
      "Training [60%] Loss: 0.014223077578041345 time: 1703259347.4474597\n",
      "weight: [ 0.446989   -0.08271013 -0.01976598  0.47720079  0.75503438  0.16294562\n",
      " -0.08955123  0.90700855  0.28587856  0.16080805  0.54873379  0.6918952\n",
      "  0.30288292  0.06043822  0.54532912  0.09758841  0.3253997   0.89509672\n",
      "  0.81124361  0.93150259  0.46537985  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 182\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011908362050506615 R2: 0.011898367651564934 time: 1703259354.2324781\n",
      "batch_idx: 1 loss: 0.016535339508136855 R2: 0.011940185623597777 time: 1703259361.287277\n",
      "Training [61%] Loss: 0.014221850779321735 time: 1703259361.287277\n",
      "weight: [ 0.44691176 -0.08306719 -0.02023072  0.47519027  0.75516089  0.16306206\n",
      " -0.08961798  0.90757475  0.28632936  0.16080805  0.54873379  0.6918952\n",
      "  0.3029849   0.06012022  0.54497207  0.09811784  0.3253997   0.89522324\n",
      "  0.81166407  0.9312232   0.46244426  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 183\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011908743712182656 R2: 0.011977827724880763 time: 1703259368.8240309\n",
      "batch_idx: 1 loss: 0.016532513682453114 R2: 0.01201942478731688 time: 1703259376.1824522\n",
      "Training [61%] Loss: 0.014220628697317885 time: 1703259376.1824522\n",
      "weight: [ 0.44683727 -0.08342543 -0.02069603  0.47317418  0.75528818  0.16317864\n",
      " -0.08967923  0.90813693  0.28676449  0.16080805  0.54873379  0.6918952\n",
      "  0.30309411  0.05980097  0.54461382  0.09864543  0.3253997   0.89535052\n",
      "  0.81208409  0.93094314  0.45950626  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 184\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011909118673055899 R2: 0.012056945241575412 time: 1703259382.7376525\n",
      "batch_idx: 1 loss: 0.01652970419945165 R2: 0.012098318194275981 time: 1703259389.398147\n",
      "Training [61%] Loss: 0.014219411436253774 time: 1703259389.398147\n",
      "weight: [ 0.44676553 -0.08378485 -0.0211619   0.47115246  0.75541621  0.16329535\n",
      " -0.08973516  0.90869499  0.28718377  0.16080805  0.54873379  0.6918952\n",
      "  0.30321033  0.05948048  0.54425441  0.09917108  0.3253997   0.89547856\n",
      "  0.81250361  0.93066244  0.45656631  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 185\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011909487105832136 R2: 0.012135713989567809 time: 1703259396.0831637\n",
      "batch_idx: 1 loss: 0.01652691109541852 R2: 0.012176859563218456 time: 1703259402.7721357\n",
      "Training [62%] Loss: 0.014218199100625328 time: 1703259402.7721357\n",
      "weight: [ 0.44669653 -0.08414541 -0.02162828  0.46912501  0.75554498  0.16341217\n",
      " -0.08978591  0.90924882  0.28758701  0.16080805  0.54873379  0.6918952\n",
      "  0.30333339  0.05915879  0.54389385  0.09969469  0.3253997   0.89560733\n",
      "  0.81292258  0.93038113  0.45362487  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 186\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011909849195804099 R2: 0.012214127910798789 time: 1703259409.2921586\n",
      "batch_idx: 1 loss: 0.016524134403541724 R2: 0.012255042658632642 time: 1703259415.8804014\n",
      "Training [62%] Loss: 0.014216991799672911 time: 1703259415.8804014\n",
      "weight: [ 0.44663027 -0.08450708 -0.02209515  0.46709176  0.75567446  0.16352908\n",
      " -0.08983167  0.90979833  0.28797404  0.16080805  0.54873379  0.6918952\n",
      "  0.30346307  0.05883589  0.54353217  0.10021616  0.3253997   0.8957368\n",
      "  0.81334095  0.93009922  0.45068241  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 187\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011910205119455794 R2: 0.01229218099261109 time: 1703259422.5353708\n",
      "batch_idx: 1 loss: 0.016521374172534884 R2: 0.012332861416893937 time: 1703259429.307169\n",
      "Training [62%] Loss: 0.01421578964599534 time: 1703259429.307169\n",
      "weight: [ 0.44656673 -0.08486986 -0.02256247  0.46505263  0.75580462  0.16364608\n",
      " -0.08987258  0.91034341  0.28834468  0.16080805  0.54873379  0.6918952\n",
      "  0.30359919  0.05851182  0.5431694   0.10073539  0.3253997   0.89586697\n",
      "  0.81375867  0.92981675  0.44773939  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 188\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01191055503277116 R2: 0.012369867209647167 time: 1703259435.822495\n",
      "batch_idx: 1 loss: 0.016518630467975956 R2: 0.012410309972658351 time: 1703259442.4122825\n",
      "Training [63%] Loss: 0.014214592750373558 time: 1703259442.4122825\n",
      "weight: [ 0.44650591 -0.08523371 -0.02303023  0.46300751  0.75593545  0.16376314\n",
      " -0.08990882  0.91088398  0.28869876  0.16080805  0.54873379  0.6918952\n",
      "  0.30374156  0.05818659  0.54280555  0.10125227  0.3253997   0.8959978\n",
      "  0.81417569  0.92953374  0.44479628  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 189\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011910899079277098 R2: 0.012447180579665429 time: 1703259449.2254739\n",
      "batch_idx: 1 loss: 0.016515903357388527 R2: 0.012487382585400275 time: 1703259456.012465\n",
      "Training [63%] Loss: 0.014213401218332812 time: 1703259456.012465\n",
      "weight: [ 0.4464478  -0.08559861 -0.02349838  0.46095631  0.75606693  0.16388024\n",
      " -0.08994054  0.91141993  0.28903612  0.16080805  0.54873379  0.6918952\n",
      "  0.30388998  0.05786022  0.54244065  0.10176671  0.3253997   0.89612927\n",
      "  0.81459196  0.92925022  0.44185355  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 190\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011911237406809213 R2: 0.012524115274292313 time: 1703259463.122433\n",
      "batch_idx: 1 loss: 0.01651319289611413 R2: 0.01256407356615985 time: 1703259469.5121403\n",
      "Training [63%] Loss: 0.014212215151461672 time: 1703259469.5121403\n",
      "weight: [ 0.44639239 -0.08596453 -0.02396689  0.45889894  0.75619902  0.16399736\n",
      " -0.0899679   0.91195117  0.28935659  0.16080805  0.54873379  0.6918952\n",
      "  0.30404428  0.05753273  0.54207473  0.10227861  0.3253997   0.89626136\n",
      "  0.81500742  0.9289662   0.43891166  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 191\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011911570174889075 R2: 0.01260066567799989 time: 1703259476.057489\n",
      "batch_idx: 1 loss: 0.016510499127717827 R2: 0.012640377290964455 time: 1703259482.6571915\n",
      "Training [64%] Loss: 0.014211034651303451 time: 1703259482.6571915\n",
      "weight: [ 0.44633967 -0.08633145 -0.02443575  0.45683528  0.7563317   0.16411448\n",
      " -0.08999107  0.91247761  0.28966     0.16080805  0.54873379  0.6918952\n",
      "  0.30420426  0.05720414  0.54170781  0.10278788  0.3253997   0.89639404\n",
      "  0.81542204  0.92868173  0.43597108  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 192\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011911897546701012 R2: 0.012676826355041526 time: 1703259489.5072708\n",
      "batch_idx: 1 loss: 0.016507822095979695 R2: 0.012716288287520916 time: 1703259496.0273807\n",
      "Training [64%] Loss: 0.014209859821340353 time: 1703259496.0273807\n",
      "weight: [ 0.44628962 -0.08669934 -0.0249049   0.45476523  0.75646494  0.16423159\n",
      " -0.0900102   0.91299917  0.2899462   0.16080805  0.54873379  0.6918952\n",
      "  0.30436975  0.05687448  0.54133992  0.10329441  0.3253997   0.89652729\n",
      "  0.81583576  0.92839681  0.43303227  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 193\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011912219676122963 R2: 0.012752591983088202 time: 1703259502.7322726\n",
      "batch_idx: 1 loss: 0.016505161854198175 R2: 0.012791801308198347 time: 1703259509.5121465\n",
      "Training [64%] Loss: 0.01420869076516057 time: 1703259509.5121465\n",
      "weight: [ 0.44624224 -0.08706817 -0.02537434  0.45268867  0.75659872  0.16434865\n",
      " -0.09002546  0.91351575  0.29021503  0.16080805  0.54873379  0.6918952\n",
      "  0.30454057  0.05654375  0.54097108  0.10379812  0.3253997   0.89666106\n",
      "  0.81624853  0.92811148  0.43009569  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 194\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011912536703748318 R2: 0.01282795733827935 time: 1703259515.9733212\n",
      "batch_idx: 1 loss: 0.01650251846239832 R2: 0.01286691132771478 time: 1703259522.5330374\n",
      "Training [65%] Loss: 0.014207527583073318 time: 1703259522.5330374\n",
      "weight: [ 0.44619751 -0.08743793 -0.02584401  0.4506055   0.756733    0.16446566\n",
      " -0.09003698  0.91402727  0.29046634  0.16080805  0.54873379  0.6918952\n",
      "  0.30471654  0.05621198  0.54060133  0.1042989   0.3253997   0.89679535\n",
      "  0.8166603   0.92782575  0.4271618   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 195\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011912848764542768 R2: 0.012902917351461651 time: 1703259529.2273927\n",
      "batch_idx: 1 loss: 0.016499891976873855 R2: 0.012941613489838466 time: 1703259535.8524144\n",
      "Training [65%] Loss: 0.01420637037070831 time: 1703259535.8524144\n",
      "weight: [ 0.44615541 -0.08780858 -0.0263139   0.44851558  0.75686776  0.16458257\n",
      " -0.09004494  0.91453365  0.29069998  0.16080805  0.54873379  0.6918952\n",
      "  0.30489748  0.0558792   0.54023067  0.10479668  0.3253997   0.89693011\n",
      "  0.81707102  0.92753966  0.42423106  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 196\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011913155997384902 R2: 0.012977467178209467 time: 1703259542.4122305\n",
      "batch_idx: 1 loss: 0.016497282443756565 R2: 0.013015903076762592 time: 1703259549.208212\n",
      "Training [65%] Loss: 0.014205219220570733 time: 1703259549.208212\n",
      "weight: [ 0.44611595 -0.0881801  -0.02678397  0.44641879  0.75700297  0.16469938\n",
      " -0.09004947  0.9150348   0.29091581  0.16080805  0.54873379  0.6918952\n",
      "  0.30508323  0.05554542  0.53985915  0.10529135  0.3253997   0.89706531\n",
      "  0.81748065  0.92725322  0.42130392  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 197\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01191345854624558 R2: 0.013051602218130731 time: 1703259556.0121396\n",
      "batch_idx: 1 loss: 0.016494689902356148 R2: 0.013089775540423787 time: 1703259562.4424582\n",
      "Training [66%] Loss: 0.014204074224300863 time: 1703259562.4424582\n",
      "weight: [ 0.4460791  -0.08855246 -0.02725419  0.444315    0.75713858  0.16481606\n",
      " -0.09005073  0.91553066  0.29111367  0.16080805  0.54873379  0.6918952\n",
      "  0.30527362  0.05521067  0.53948679  0.10578283  0.3253997   0.89720093\n",
      "  0.81788912  0.92696646  0.41838083  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 198\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01191375655303014 R2: 0.013125318080580484 time: 1703259568.9774318\n",
      "batch_idx: 1 loss: 0.016492114392921496 R2: 0.013163226563636554 time: 1703259575.282365\n",
      "Training [66%] Loss: 0.014202935472975818 time: 1703259575.282365\n",
      "weight: [ 0.44604484 -0.08892564 -0.02772454  0.44220407  0.75727458  0.16493257\n",
      " -0.09004886  0.91602113  0.29129344  0.16080805  0.54873379  0.6918952\n",
      "  0.30546849  0.05487497  0.53911362  0.10627103  0.3253997   0.89733692\n",
      "  0.8182964   0.9266794   0.41546222  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 199\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011914050150855686 R2: 0.013198610550386847 time: 1703259581.6974525\n",
      "batch_idx: 1 loss: 0.016489555959634136 R2: 0.01323625209103585 time: 1703259588.25269\n",
      "Training [66%] Loss: 0.014201803055244912 time: 1703259588.25269\n",
      "weight: [ 0.44601316 -0.0892996  -0.02819497  0.44008587  0.75741091  0.1650489\n",
      " -0.09004401  0.91650616  0.29145497  0.16080805  0.54873379  0.6918952\n",
      "  0.30566767  0.05453834  0.53873966  0.10675588  0.3253997   0.89747326\n",
      "  0.81870244  0.92639207  0.41254853  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 200\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011914339464743165 R2: 0.013271475599076044 time: 1703259594.597083\n",
      "batch_idx: 1 loss: 0.01648701464609937 R2: 0.01330884830965906 time: 1703259601.057435\n",
      "Training [67%] Loss: 0.014200677055421267 time: 1703259601.057435\n",
      "weight: [ 0.44598405 -0.08967432 -0.02866547  0.43796026  0.75754756  0.16516502\n",
      " -0.09003631  0.91698567  0.29159814  0.16080805  0.54873379  0.6918952\n",
      "  0.305871    0.0542008   0.53836494  0.1072373   0.3253997   0.8976099\n",
      "  0.81910718  0.92610448  0.4096402   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 201\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01191462461776419 R2: 0.01334390943130237 time: 1703259607.3642175\n",
      "batch_idx: 1 loss: 0.01648449048901716 R2: 0.013381011615099636 time: 1703259613.652229\n",
      "Training [67%] Loss: 0.014199557553390675 time: 1703259613.652229\n",
      "weight: [ 0.44595749 -0.09004977 -0.02913599  0.43582707  0.75768447  0.16528089\n",
      " -0.0900259   0.91745959  0.29172282  0.16080805  0.54873379  0.6918952\n",
      "  0.30607833  0.05386237  0.53798948  0.1077152   0.3253997   0.89774682\n",
      "  0.81951059  0.92581666  0.40673764  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 202\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011914905735155714 R2: 0.013415908519001984 time: 1703259619.9525378\n",
      "batch_idx: 1 loss: 0.01648198351680069 R2: 0.013452738608928372 time: 1703259626.4923007\n",
      "Training [67%] Loss: 0.014198444625978203 time: 1703259626.4923007\n",
      "weight: [ 0.44593347 -0.09042593 -0.02960651  0.43368617  0.75782162  0.1653965\n",
      " -0.09001293  0.91792786  0.29182887  0.16080805  0.54873379  0.6918952\n",
      "  0.3062895   0.05352309  0.53761332  0.10818953  0.3253997   0.89788396\n",
      "  0.81991261  0.92552863  0.40384128  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 203\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011915182942250194 R2: 0.01348746959496161 time: 1703259633.1224647\n",
      "batch_idx: 1 loss: 0.016479493753550106 R2: 0.01352402613188275 time: 1703259639.9052148\n",
      "Training [68%] Loss: 0.01419733834790015 time: 1703259639.9052148\n",
      "weight: [ 0.44591196 -0.09080277 -0.03007701  0.43153739  0.75795896  0.16551181\n",
      " -0.08999751  0.91839041  0.29191619  0.16080805  0.54873379  0.6918952\n",
      "  0.30650438  0.05318296  0.53723649  0.10866019  0.3253997   0.8980213\n",
      "  0.8203132   0.92524041  0.40095151  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 204\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011915456359390328 R2: 0.013558589624860895 time: 1703259646.6925774\n",
      "batch_idx: 1 loss: 0.016477021222938936 R2: 0.01359487129679915 time: 1703259653.3671017\n",
      "Training [68%] Loss: 0.014196238791164632 time: 1703259653.3671017\n",
      "weight: [ 0.44589294 -0.09118026 -0.03054745  0.42938057  0.75809645  0.16562679\n",
      " -0.08997978  0.91884718  0.29198464  0.16080805  0.54873379  0.6918952\n",
      "  0.3067228   0.05284201  0.53685899  0.10912714  0.3253997   0.8981588\n",
      "  0.82071231  0.92495202  0.39806875  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 205\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011915726099784144 R2: 0.013629265796835432 time: 1703259660.1475182\n",
      "batch_idx: 1 loss: 0.016474565947522703 R2: 0.013665271489880615 time: 1703259666.842389\n",
      "Training [68%] Loss: 0.014195146023653423 time: 1703259666.842389\n",
      "weight: [ 0.44587641 -0.09155838 -0.0310178   0.42721554  0.75823407  0.16574142\n",
      " -0.08995987  0.91929813  0.29203413  0.16080805  0.54873379  0.6918952\n",
      "  0.30694463  0.05250027  0.53648088  0.10959031  0.3253997   0.89829641\n",
      "  0.8211099   0.92466349  0.39519339  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 206\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011915992272126131 R2: 0.013699495541891538 time: 1703259673.1625204\n",
      "batch_idx: 1 loss: 0.01647212794470314 R2: 0.013735224347377062 time: 1703259679.5073855\n",
      "Training [69%] Loss: 0.014194060108414636 time: 1703259679.5073855\n",
      "weight: [ 0.44586234 -0.09193709 -0.03148804  0.42504214  0.75837175  0.16585566\n",
      " -0.08993789  0.91974319  0.29206453  0.16080805  0.54873379  0.6918952\n",
      "  0.30716974  0.05215776  0.53610216  0.11004964  0.3253997   0.8984341\n",
      "  0.82150592  0.92437483  0.39232581  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 207\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011916254984300805 R2: 0.013769276561644173 time: 1703259685.896118\n",
      "batch_idx: 1 loss: 0.016469707224081826 R2: 0.01380472774016206 time: 1703259692.1255755\n",
      "Training [69%] Loss: 0.014192981104191316 time: 1703259692.1255755\n",
      "weight: [ 0.44585072 -0.09231638 -0.03195813  0.42286019  0.75850947  0.16596948\n",
      " -0.08991396  0.92018231  0.29207574  0.16080805  0.54873379  0.6918952\n",
      "  0.30739798  0.05181449  0.53572287  0.11050508  0.3253997   0.89857181\n",
      "  0.82190034  0.92408607  0.38946638  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 208\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011916514343873764 R2: 0.013838606834242095 time: 1703259698.517655\n",
      "batch_idx: 1 loss: 0.016467303788676137 R2: 0.01387377978428106 time: 1703259705.042371\n",
      "Training [69%] Loss: 0.01419190906627495 time: 1703259705.042371\n",
      "weight: [ 0.44584152 -0.09269622 -0.03242806  0.42066951  0.75864718  0.16608285\n",
      " -0.08988821  0.92061544  0.29206767  0.16080805  0.54873379  0.6918952\n",
      "  0.30762922  0.0514705   0.53534304  0.11095657  0.3253997   0.89870952\n",
      "  0.8222931   0.92379723  0.38661547  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 209\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011916770455316139 R2: 0.013907484597092123 time: 1703259711.2821321\n",
      "batch_idx: 1 loss: 0.01646491763780787 R2: 0.013942378863033733 time: 1703259717.5860367\n",
      "Training [70%] Loss: 0.014190844046562005 time: 1703259717.5860367\n",
      "weight: [ 0.44583473 -0.09307657 -0.03289778  0.41846991  0.75878483  0.16619574\n",
      " -0.08986074  0.92104254  0.2920402   0.16080805  0.54873379  0.6918952\n",
      "  0.30786333  0.0511258   0.53496268  0.11140407  0.3253997   0.89884718\n",
      "  0.82268417  0.92350832  0.38377344  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 210\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011917023417562168 R2: 0.013975908330334796 time: 1703259724.3524883\n",
      "batch_idx: 1 loss: 0.016462548767972828 R2: 0.014010523634455652 time: 1703259730.8722003\n",
      "Training [70%] Loss: 0.014189786092767498 time: 1703259730.8722003\n",
      "weight: [ 0.44583033 -0.09345742 -0.03336729  0.41626121  0.7589224   0.16630811\n",
      " -0.08983166  0.92146357  0.29199325  0.16080805  0.54873379  0.6918952\n",
      "  0.3081002   0.05078041  0.53458184  0.11184754  0.3253997   0.89898474\n",
      "  0.82307351  0.92321937  0.38094063  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 211\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011917273324565079 R2: 0.0140438767598281 time: 1703259737.3074677\n",
      "batch_idx: 1 loss: 0.01646019717086916 R2: 0.014078213017565622 time: 1703259743.532281\n",
      "Training [70%] Loss: 0.014188735247717119 time: 1703259743.532281\n",
      "weight: [ 0.44582829 -0.09383874 -0.03383654  0.41404321  0.75905982  0.16641994\n",
      " -0.08980107  0.92187848  0.29192673  0.16080805  0.54873379  0.6918952\n",
      "  0.30833969  0.05043436  0.53420052  0.11228694  0.3253997   0.89912216\n",
      "  0.82346107  0.92293039  0.37811738  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 212\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011917520267731732 R2: 0.01411138887271346 time: 1703259750.0121787\n",
      "batch_idx: 1 loss: 0.016457862831114414 R2: 0.014145446175040544 time: 1703259756.2275498\n",
      "Training [71%] Loss: 0.014187691549423072 time: 1703259756.2275498\n",
      "weight: [ 0.44582861 -0.0942205  -0.03430552  0.41181572  0.75919706  0.16653119\n",
      " -0.08976909  0.92228724  0.29184055  0.16080805  0.54873379  0.6918952\n",
      "  0.30858169  0.05008767  0.53381876  0.11272223  0.3253997   0.8992594\n",
      "  0.82384682  0.92264141  0.37530402  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 213\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011917764337181409 R2: 0.01417844392459322 time: 1703259762.677464\n",
      "batch_idx: 1 loss: 0.016455545726150697 R2: 0.014212222510611716 time: 1703259769.0822992\n",
      "Training [71%] Loss: 0.014186655031666053 time: 1703259769.0822992\n",
      "weight: [ 0.44583126 -0.09460268 -0.0347742   0.40957853  0.75933407  0.16664183\n",
      " -0.0897358   0.92268982  0.29173463  0.16080805  0.54873379  0.6918952\n",
      "  0.30882609  0.04974036  0.53343658  0.11315339  0.3253997   0.89939642\n",
      "  0.82423072  0.92235245  0.37250087  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 214\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011918005620606563 R2: 0.01424504142926275 time: 1703259775.5523458\n",
      "batch_idx: 1 loss: 0.016453245827970104 R2: 0.014278541679194623 time: 1703259781.9152796\n",
      "Training [71%] Loss: 0.014185625724288334 time: 1703259781.9152796\n",
      "weight: [ 0.44583622 -0.09498525 -0.03524255  0.40733143  0.75947081  0.16675183\n",
      " -0.0897013   0.92308619  0.29160889  0.16080805  0.54873379  0.6918952\n",
      "  0.30907277  0.04939245  0.533054    0.11358039  0.3253997   0.89953316\n",
      "  0.82461274  0.92206353  0.36970822  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 215\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011918244201450913 R2: 0.014311181142599105 time: 1703259788.4161952\n",
      "batch_idx: 1 loss: 0.0164509631042952 R2: 0.014344403592707389 time: 1703259795.5167959\n",
      "Training [72%] Loss: 0.014184603652873056 time: 1703259795.5167959\n",
      "weight: [ 0.44584347 -0.0953682  -0.03571056  0.40507422  0.75960723  0.16686115\n",
      " -0.08966569  0.92347631  0.29146325  0.16080805  0.54873379  0.6918952\n",
      "  0.30932162  0.04904396  0.53267106  0.1140032   0.3253997   0.89966958\n",
      "  0.82499283  0.92177466  0.36692637  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 216\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011918480158635638 R2: 0.014376863056450473 time: 1703259802.2471476\n",
      "batch_idx: 1 loss: 0.016448697517877316 R2: 0.014409808411427028 time: 1703259808.9774523\n",
      "Training [72%] Loss: 0.014183588838256477 time: 1703259808.9774523\n",
      "weight: [ 0.445853   -0.09575149 -0.0361782   0.40280667  0.7597433   0.16696976\n",
      " -0.08962906  0.92386017  0.29129766  0.16080805  0.54873379  0.6918952\n",
      "  0.30957254  0.04869492  0.53228776  0.11442182  0.3253997   0.89980564\n",
      "  0.82537096  0.92148586  0.36415562  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 217\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011918713567934733 R2: 0.014442087403785497 time: 1703259815.8146658\n",
      "batch_idx: 1 loss: 0.0164464490249804 R2: 0.014474756528417232 time: 1703259822.265558\n",
      "Training [72%] Loss: 0.014182581296457566 time: 1703259822.265558\n",
      "weight: [ 0.44586477 -0.09613511 -0.03664544  0.40052858  0.75987895  0.16707762\n",
      " -0.08959148  0.92423774  0.29111203  0.16080805  0.54873379  0.6918952\n",
      "  0.30982544  0.04834534  0.53190415  0.11483623  0.3253997   0.8999413\n",
      "  0.82574711  0.92119717  0.36139622  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 218\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011918944503191859 R2: 0.014506854662439839 time: 1703259828.7635999\n",
      "batch_idx: 1 loss: 0.016444217574908 R2: 0.014539248560870743 time: 1703259835.0455656\n",
      "Training [73%] Loss: 0.014181581039049929 time: 1703259835.0455656\n",
      "weight: [ 0.44587879 -0.09651903 -0.03711227  0.3982397   0.76001416  0.16718471\n",
      " -0.08955305  0.924609    0.29090632  0.16080805  0.54873379  0.6918952\n",
      "  0.3100802   0.04799524  0.53152023  0.11524642  0.3253997   0.9000765\n",
      "  0.82612123  0.92090859  0.35864845  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 219\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011919173036058104 R2: 0.014571165547833664 time: 1703259841.6973681\n",
      "batch_idx: 1 loss: 0.016442003110937776 R2: 0.014603285351495132 time: 1703259848.1424434\n",
      "Training [73%] Loss: 0.014180588073497941 time: 1703259848.1424434\n",
      "weight: [ 0.44589501 -0.09690322 -0.03757867  0.39593982  0.76014887  0.16729099\n",
      " -0.08951383  0.92497394  0.29068047  0.16080805  0.54873379  0.6918952\n",
      "  0.31033674  0.04764464  0.53113603  0.11565238  0.3253997   0.90021121\n",
      "  0.82649329  0.92062014  0.35591256  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 220\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011919399234875613 R2: 0.01463502099886127 time: 1703259854.4271016\n",
      "batch_idx: 1 loss: 0.016439805571348963 R2: 0.014666867970330122 time: 1703259860.8825817\n",
      "Training [73%] Loss: 0.014179602403112288 time: 1703259860.8825817\n",
      "weight: [ 0.44591344 -0.09728768 -0.03804461  0.39362871  0.76028304  0.16739643\n",
      " -0.08947391  0.92533255  0.29043441  0.16080805  0.54873379  0.6918952\n",
      "  0.31059497  0.04729356  0.53075158  0.11605412  0.3253997   0.90034538\n",
      "  0.82686327  0.92033184  0.35318879  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 221\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01191962316423607 R2: 0.014698422168048175 time: 1703259867.252147\n",
      "batch_idx: 1 loss: 0.01643762488935522 R2: 0.014729997707911613 time: 1703259873.622433\n",
      "Training [74%] Loss: 0.014178624026795645 time: 1703259873.622433\n",
      "weight: [ 0.44593404 -0.09767236 -0.03851007  0.39130611  0.76041662  0.16750099\n",
      " -0.08943337  0.92568482  0.29016811  0.16080805  0.54873379  0.6918952\n",
      "  0.3108548   0.04694202  0.53036689  0.11645164  0.3253997   0.90047897\n",
      "  0.82723113  0.92004372  0.35047737  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 222\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011919844885709813 R2: 0.01476137041986858 time: 1703259879.942167\n",
      "batch_idx: 1 loss: 0.016435460992254278 R2: 0.014792676061955223 time: 1703259886.1774495\n",
      "Training [74%] Loss: 0.014177652938982046 time: 1703259886.1774495\n",
      "weight: [ 0.44595679 -0.09805727 -0.03897504  0.3889718   0.76054958  0.16760465\n",
      " -0.08939226  0.92603074  0.28988152  0.16080805  0.54873379  0.6918952\n",
      "  0.31111614  0.04659004  0.52998199  0.11684493  0.3253997   0.90061193\n",
      "  0.82759685  0.9197558   0.34777852  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 223\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011920064458791133 R2: 0.014823867330482354 time: 1703259892.6521623\n",
      "batch_idx: 1 loss: 0.016433313800995736 R2: 0.01485490472669082 time: 1703259898.9896054\n",
      "Training [74%] Loss: 0.014176689129893436 time: 1703259898.9896054\n",
      "weight: [ 0.44598169 -0.09844236 -0.0394395   0.38662552  0.76068187  0.16770738\n",
      " -0.08935066  0.92637031  0.2895746   0.16080805  0.54873379  0.6918952\n",
      "  0.3113789   0.04623764  0.52959689  0.11723403  0.3253997   0.90074421\n",
      "  0.8279604   0.91946808  0.34509246  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 224\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011920281941016011 R2: 0.014885914681164403 time: 1703259905.6025002\n",
      "batch_idx: 1 loss: 0.016431183230691766 R2: 0.014916685589114409 time: 1703259912.4024684\n",
      "Training [75%] Loss: 0.01417573258585389 time: 1703259912.4024684\n",
      "weight: [ 0.4460087  -0.09882763 -0.03990343  0.38426702  0.76081344  0.16780913\n",
      " -0.08930863  0.92670352  0.28924732  0.16080805  0.54873379  0.6918952\n",
      "  0.31164302  0.04588484  0.52921162  0.11761892  0.3253997   0.90087578\n",
      "  0.82832175  0.9191806   0.34241937  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 225\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192049738736822 R2: 0.01494751444613207 time: 1703259918.9730253\n",
      "batch_idx: 1 loss: 0.016429069191406305 R2: 0.014978020727219765 time: 1703259925.1722064\n",
      "Training [75%] Loss: 0.014174783289387262 time: 1703259925.1722064\n",
      "weight: [ 0.44603782 -0.09921306 -0.04036681  0.38189606  0.76094425  0.16790989\n",
      " -0.08926623  0.9270304   0.28889964  0.16080805  0.54873379  0.6918952\n",
      "  0.31190841  0.04553164  0.5288262   0.11799965  0.3253997   0.9010066\n",
      "  0.82868089  0.91889336  0.33975946  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 226\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011920710849922977 R2: 0.01500866878179763 time: 1703259931.5524356\n",
      "batch_idx: 1 loss: 0.016426971588352747 R2: 0.01503891240348032 time: 1703259937.972292\n",
      "Training [75%] Loss: 0.014173841219137862 time: 1703259937.972292\n",
      "weight: [ 0.44606902 -0.09959862 -0.04082962  0.37951236  0.76107426  0.16800961\n",
      " -0.08922352  0.92735093  0.28853154  0.16080805  0.54873379  0.6918952\n",
      "  0.312175    0.04517808  0.52844064  0.11837621  0.3253997   0.90113661\n",
      "  0.82903777  0.9186064   0.3371129   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 227\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011920922378240488 R2: 0.015069380021426104 time: 1703259944.1625252\n",
      "batch_idx: 1 loss: 0.0164248903214958 R2: 0.015099363053449721 time: 1703259950.4812534\n",
      "Training [76%] Loss: 0.014172906349868144 time: 1703259950.4812534\n",
      "weight: [ 0.44610228 -0.09998429 -0.04129186  0.37711567  0.76120344  0.16810828\n",
      " -0.08918056  0.92766512  0.288143    0.16080805  0.54873379  0.6918952\n",
      "  0.31244272  0.04482416  0.52805496  0.11874864  0.3253997   0.90126578\n",
      "  0.82939239  0.91831972  0.33447986  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 228\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192113202004439 R2: 0.015129650671883543 time: 1703259957.1458626\n",
      "batch_idx: 1 loss: 0.0164228252852811 R2: 0.01515937527507849 time: 1703259963.5746536\n",
      "Training [76%] Loss: 0.014171978652662745 time: 1703259963.5746536\n",
      "weight: [ 0.44613758 -0.10037007 -0.0417535   0.37470572  0.76133173  0.16820586\n",
      " -0.0891374   0.927973    0.287734    0.16080805  0.54873379  0.6918952\n",
      "  0.3127115   0.04446991  0.52766919  0.11911696  0.3253997   0.90139407\n",
      "  0.82974472  0.91803335  0.33186051  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 229\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011921339821459959 R2: 0.015189483407018645 time: 1703259969.973107\n",
      "batch_idx: 1 loss: 0.016420776368952127 R2: 0.015218951822511428 time: 1703259976.532131\n",
      "Training [76%] Loss: 0.014171058095206043 time: 1703259976.532131\n",
      "weight: [ 0.44617492 -0.10075592 -0.04221453  0.37228224  0.76145909  0.16830233\n",
      " -0.0890941   0.92827458  0.28730452  0.16080805  0.54873379  0.6918952\n",
      "  0.31298127  0.04411533  0.52728334  0.11948121  0.3253997   0.90152144\n",
      "  0.83009474  0.9177473   0.32925498  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 230\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011921545826741741 R2: 0.015248881057092767 time: 1703259982.914216\n",
      "batch_idx: 1 loss: 0.01641874345713999 R2: 0.015278095601991004 time: 1703259989.8322008\n",
      "Training [77%] Loss: 0.014170144641940864 time: 1703259989.8322008\n",
      "weight: [ 0.44621426 -0.10114184 -0.04267493  0.36984495  0.7615855   0.16839764\n",
      " -0.08905069  0.92856987  0.28685455  0.16080805  0.54873379  0.6918952\n",
      "  0.31325198  0.04376045  0.52689742  0.1198414   0.3253997   0.90164784\n",
      "  0.83044242  0.9174616   0.32666343  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 231\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011921750078051017 R2: 0.015307846598462738 time: 1703259997.1425498\n",
      "batch_idx: 1 loss: 0.016416726430144555 R2: 0.015336809665297935 time: 1703260004.152559\n",
      "Training [77%] Loss: 0.014169238254097785 time: 1703260004.152559\n",
      "weight: [ 0.44625559 -0.1015278  -0.0431347   0.36739358  0.7617109   0.16849179\n",
      " -0.08900723  0.92885889  0.28638409  0.16080805  0.54873379  0.6918952\n",
      "  0.31352355  0.04340529  0.52651145  0.12019758  0.3253997   0.90177325\n",
      "  0.83078776  0.91717626  0.32408599  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 232\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011921952615688949 R2: 0.015366383146717788 time: 1703260010.6926575\n",
      "batch_idx: 1 loss: 0.016414725163806685 R2: 0.015395097199887076 time: 1703260017.143643\n",
      "Training [77%] Loss: 0.014168338889747816 time: 1703260017.143643\n",
      "weight: [ 0.44629889 -0.1019138  -0.04359382  0.36492784  0.76183527  0.16858474\n",
      " -0.08896376  0.92914167  0.28589314  0.16080805  0.54873379  0.6918952\n",
      "  0.31379594  0.04304984  0.52612546  0.12054979  0.3253997   0.90189762\n",
      "  0.83113073  0.9168913   0.32152278  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 233\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011922153478570428 R2: 0.015424493951713858 time: 1703260023.6822999\n",
      "batch_idx: 1 loss: 0.01641273952939294 R2: 0.015452961519045294 time: 1703260029.9122705\n",
      "Training [78%] Loss: 0.014167446503981683 time: 1703260029.9122705\n",
      "weight: [ 0.44634414 -0.10229981 -0.04405228  0.36244744  0.76195856  0.16867647\n",
      " -0.08892032  0.92941824  0.2853817   0.16080805  0.54873379  0.6918952\n",
      "  0.31406908  0.04269414  0.52573945  0.12089805  0.3253997   0.90202091\n",
      "  0.83147131  0.91660674  0.31897391  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 234\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011922352704466661 R2: 0.015482182390907262 time: 1703260036.4425325\n",
      "batch_idx: 1 loss: 0.01641076939383387 R2: 0.015510406054870307 time: 1703260042.8673868\n",
      "Training [78%] Loss: 0.014166561049150265 time: 1703260042.8673868\n",
      "weight: [ 0.44639133 -0.10268581 -0.04451006  0.3599521   0.76208074  0.16876695\n",
      " -0.08887696  0.92968862  0.28484977  0.16080805  0.54873379  0.6918952\n",
      "  0.31434292  0.0423382   0.52535344  0.12124241  0.3253997   0.90214308\n",
      "  0.83180949  0.9163226   0.31643949  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 235\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011922550329906753 R2: 0.01553945196010098 time: 1703260049.2923698\n",
      "batch_idx: 1 loss: 0.016408814620172005 R2: 0.015567434352992238 time: 1703260055.7025247\n",
      "Training [78%] Loss: 0.014165682475039378 time: 1703260055.7025247\n",
      "weight: [ 0.44644044 -0.1030718  -0.04496716  0.35744152  0.76220177  0.16885615\n",
      " -0.08883371  0.92995285  0.28429737  0.16080805  0.54873379  0.6918952\n",
      "  0.3146174   0.04198202  0.52496745  0.12158292  0.3253997   0.90226411\n",
      "  0.83214526  0.91603889  0.31391961  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 236\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011922746390062229 R2: 0.015596306264095694 time: 1703260062.147442\n",
      "batch_idx: 1 loss: 0.01640687506784915 R2: 0.015624050066064266 time: 1703260068.5173056\n",
      "Training [79%] Loss: 0.01416481072895569 time: 1703260068.5173056\n",
      "weight: [ 0.44649144 -0.10345776 -0.04542356  0.35491541  0.76232162  0.16894406\n",
      " -0.08879061  0.93021095  0.2837245   0.16080805  0.54873379  0.6918952\n",
      "  0.31489248  0.04162563  0.5245815   0.12191961  0.3253997   0.90238396\n",
      "  0.83247859  0.91575563  0.31141438  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 237\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192294091890185 R2: 0.015652749009516298 time: 1703260075.3670754\n",
      "batch_idx: 1 loss: 0.01640495059272708 R2: 0.01568025694514641 time: 1703260081.9924736\n",
      "Training [79%] Loss: 0.014163945755814466 time: 1703260081.9924736\n",
      "weight: [ 0.44654433 -0.10384367 -0.04587925  0.35237346  0.76244025  0.16903064\n",
      " -0.08874769  0.93046298  0.2831312   0.16080805  0.54873379  0.6918952\n",
      "  0.31516811  0.04126903  0.52419559  0.12225253  0.3253997   0.90250259\n",
      "  0.83280948  0.91547284  0.30892387  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 238\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011923133949518902 R2: 0.015708783999102804 time: 1703260088.2532685\n",
      "batch_idx: 1 loss: 0.01640304104708636 R2: 0.015736058830932764 time: 1703260094.6142707\n",
      "Training [79%] Loss: 0.01416308749830263 time: 1703260094.6142707\n",
      "weight: [ 0.44659908 -0.10422952 -0.04633423  0.34981538  0.76255763  0.16911589\n",
      " -0.08870498  0.93070896  0.28251747  0.16080805  0.54873379  0.6918952\n",
      "  0.31544424  0.04091225  0.52380974  0.12258173  0.3253997   0.90261997\n",
      "  0.83313792  0.91519054  0.30644815  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 239\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011923325514334228 R2: 0.015764415125173903 time: 1703260101.0075016\n",
      "batch_idx: 1 loss: 0.016401146279835042 R2: 0.015791459646759987 time: 1703260107.2925124\n",
      "Training [80%] Loss: 0.014162235897084634 time: 1703260107.2925124\n",
      "weight: [ 0.44665567 -0.10461529 -0.04678848  0.34724086  0.76267373  0.16919977\n",
      " -0.08866252  0.93094894  0.28188335  0.16080805  0.54873379  0.6918952\n",
      "  0.31572082  0.04055529  0.52342396  0.12290726  0.3253997   0.90273607\n",
      "  0.83346388  0.91490874  0.30398729  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 240\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011923515645078677 R2: 0.015819646361455164 time: 1703260113.6575947\n",
      "batch_idx: 1 loss: 0.0163992661368628 R2: 0.01584646339287854 time: 1703260120.1924005\n",
      "Training [80%] Loss: 0.01416139089097074 time: 1703260120.1924005\n",
      "weight: [ 0.44671409 -0.10500098 -0.04724199  0.34464958  0.76278852  0.16928227\n",
      " -0.08862034  0.93118296  0.28122886  0.16080805  0.54873379  0.6918952\n",
      "  0.31599782  0.04019817  0.52303828  0.12322917  0.3253997   0.90285086\n",
      "  0.83378736  0.91462746  0.30154136  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 241\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192370437274356 R2: 0.01587448175477868 time: 1703260126.472519\n",
      "batch_idx: 1 loss: 0.016397400461308727 R2: 0.015901074140127203 time: 1703260132.9858627\n",
      "Training [80%] Loss: 0.014160552417026143 time: 1703260132.9858627\n",
      "weight: [ 0.44677432 -0.10538657 -0.04769476  0.34204126  0.76290196  0.16936336\n",
      " -0.08857846  0.93141107  0.28055404  0.16080805  0.54873379  0.6918952\n",
      "  0.31627518  0.0398409   0.52265269  0.1235475   0.3253997   0.90296431\n",
      "  0.83410835  0.91434671  0.2991104   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 242\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011923891727689599 R2: 0.015928925418135176 time: 1703260139.4499776\n",
      "batch_idx: 1 loss: 0.016395549093660915 R2: 0.01595529602227974 time: 1703260145.862197\n",
      "Training [81%] Loss: 0.014159720410675257 time: 1703260145.862197\n",
      "weight: [ 0.44683635 -0.10577204 -0.04814678  0.33941556  0.76301404  0.16944303\n",
      " -0.08853691  0.93163333  0.27985891  0.16080805  0.54873379  0.6918952\n",
      "  0.31655287  0.0394835   0.52226722  0.1238623   0.3253997   0.90307638\n",
      "  0.83442683  0.91406652  0.29669447  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 243\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011924077739868939 R2: 0.01598298152476152 time: 1703260152.0373425\n",
      "batch_idx: 1 loss: 0.016393711871831522 R2: 0.016009133228246375 time: 1703260158.6325154\n",
      "Training [81%] Loss: 0.01415889480585023 time: 1703260158.6325154\n",
      "weight: [ 0.44690014 -0.10615738 -0.04859804  0.33677218  0.76312471  0.16952126\n",
      " -0.08849572  0.93184977  0.27914353  0.16080805  0.54873379  0.6918952\n",
      "  0.31683084  0.03912597  0.52188188  0.12417364  0.3253997   0.90318706\n",
      "  0.8347428   0.9137869   0.2942936   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 244\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011924262438976038 R2: 0.01603665430180612 time: 1703260165.4041748\n",
      "batch_idx: 1 loss: 0.01639188863135764 R2: 0.0160625899953859 time: 1703260172.1472468\n",
      "Training [81%] Loss: 0.014158075535166839 time: 1703260172.1472468\n",
      "weight: [ 0.44696569 -0.10654258 -0.04904852  0.3341108   0.76323396  0.16959804\n",
      " -0.08845491  0.93206045  0.27840793  0.16080805  0.54873379  0.6918952\n",
      "  0.31710907  0.03876833  0.52149667  0.12448157  0.3253997   0.9032963\n",
      "  0.83505626  0.91350785  0.29190782  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 245\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011924445854457972 R2: 0.016089948022984357 time: 1703260178.9773924\n",
      "batch_idx: 1 loss: 0.01639007920569769 R2: 0.016115670603659038 time: 1703260185.2351227\n",
      "Training [82%] Loss: 0.014157262530077831 time: 1703260185.2351227\n",
      "weight: [ 0.44703297 -0.10692763 -0.04949823  0.33143112  0.76334175  0.16967333\n",
      " -0.0884145   0.93226543  0.27765215  0.16080805  0.54873379  0.6918952\n",
      "  0.31738751  0.03841059  0.52111162  0.12478613  0.3253997   0.9034041\n",
      "  0.83536718  0.9132294   0.28953718  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 246\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011924628015495653 R2: 0.016142867001107675 time: 1703260191.5873282\n",
      "batch_idx: 1 loss: 0.016388283426480337 R2: 0.01616837936945492 time: 1703260198.1835635\n",
      "Training [82%] Loss: 0.014156455720987995 time: 1703260198.1835635\n",
      "weight: [ 0.44710198 -0.10731252 -0.04994716  0.3287328   0.76344806  0.16974714\n",
      " -0.08837452  0.93246477  0.27687625  0.16080805  0.54873379  0.6918952\n",
      "  0.31766612  0.03805276  0.52072674  0.12508738  0.3253997   0.90351041\n",
      "  0.83567556  0.91295157  0.28718169  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 247\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192480895107741 R2: 0.016195415581455208 time: 1703260204.41227\n",
      "batch_idx: 1 loss: 0.016386501123650275 R2: 0.016220720638579045 time: 1703260210.692172\n",
      "Training [82%] Loss: 0.014155655037363843 time: 1703260210.692172\n",
      "weight: [ 0.44717268 -0.10769723 -0.0503953   0.32601554  0.76355287  0.16981944\n",
      " -0.08833499  0.93265852  0.27608028  0.16080805  0.54873379  0.6918952\n",
      "  0.31794488  0.03769486  0.52034203  0.12538537  0.3253997   0.90361522\n",
      "  0.8359814   0.91267436  0.28484136  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 248\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011924988690142035 R2: 0.01624759813579757 time: 1703260216.9826303\n",
      "batch_idx: 1 loss: 0.016384732125597053 R2: 0.01627269877912163 time: 1703260223.2374024\n",
      "Training [83%] Loss: 0.014154860407869544 time: 1703260223.2374024\n",
      "weight: [ 0.44724506 -0.10808175 -0.05084264  0.32327902  0.76365615  0.16989022\n",
      " -0.08829592  0.93284674  0.27526429  0.16080805  0.54873379  0.6918952\n",
      "  0.31822375  0.03733689  0.5199575   0.12568017  0.3253997   0.9037185\n",
      "  0.83628469  0.91239779  0.28251622  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 249\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192516726167735 R2: 0.016299419056167053 time: 1703260229.8225138\n",
      "batch_idx: 1 loss: 0.016382976259362667 R2: 0.016324318174967156 time: 1703260236.102623\n",
      "Training [83%] Loss: 0.014154071760520009 time: 1703260236.102623\n",
      "weight: [ 0.4473191  -0.10846608 -0.05128918  0.32052292  0.76375789  0.16995947\n",
      " -0.08825734  0.93302951  0.27442833  0.16080805  0.54873379  0.6918952\n",
      "  0.31850269  0.03697887  0.51957318  0.12597183  0.3253997   0.90382023\n",
      "  0.83658542  0.91212187  0.28020626  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 250\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011925344694729088 R2: 0.016350882747961593 time: 1703260243.122552\n",
      "batch_idx: 1 loss: 0.016381233350910848 R2: 0.01637558321982513 time: 1703260249.939327\n",
      "Training [83%] Loss: 0.014153289022819968 time: 1703260249.939327\n",
      "weight: [ 0.44739479 -0.10885019 -0.0517349   0.31774693  0.76385804  0.17002716\n",
      " -0.08821926  0.93320687  0.27357248  0.16080805  0.54873379  0.6918952\n",
      "  0.31878168  0.03662081  0.51918907  0.1262604   0.3253997   0.90392039\n",
      "  0.83688358  0.91184662  0.2779115   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 251\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011925521018387574 R2: 0.016401993622934707 time: 1703260256.5473514\n",
      "batch_idx: 1 loss: 0.01637950322537117 R2: 0.016426498311006066 time: 1703260262.9481938\n",
      "Training [84%] Loss: 0.014152512121879372 time: 1703260262.9481938\n",
      "weight: [ 0.4474721  -0.10923408 -0.05217981  0.31495074  0.76395661  0.1700933\n",
      " -0.08818171  0.93337891  0.27269679  0.16080805  0.54873379  0.6918952\n",
      "  0.31906068  0.03626272  0.51880518  0.12654594  0.3253997   0.90401895\n",
      "  0.83717918  0.91157205  0.27563192  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 252\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011925696261827688 R2: 0.01645275609265373 time: 1703260269.2847185\n",
      "batch_idx: 1 loss: 0.01637778570722296 R2: 0.016477067842630366 time: 1703260275.4791663\n",
      "Training [84%] Loss: 0.014151740984525324 time: 1703260275.4791663\n",
      "weight: [ 0.44755101 -0.10961774 -0.0526239   0.31213405  0.76405355  0.17015786\n",
      " -0.08814469  0.93354567  0.27180132  0.16080805  0.54873379  0.6918952\n",
      "  0.31933966  0.03590462  0.51842152  0.1268285   0.3253997   0.9041159\n",
      "  0.83747219  0.91129816  0.27336754  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 253\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011925870454388377 R2: 0.016503174562311207 time: 1703260282.076007\n",
      "batch_idx: 1 loss: 0.016376080620472723 R2: 0.01652729619869331 time: 1703260288.6022692\n",
      "Training [84%] Loss: 0.014150975537430551 time: 1703260288.6022692\n",
      "weight: [ 0.44763151 -0.11000115 -0.05306717  0.30929653  0.76414887  0.17022084\n",
      " -0.08810823  0.93370725  0.27088615  0.16080805  0.54873379  0.6918952\n",
      "  0.3196186   0.03554651  0.5180381   0.12710815  0.3253997   0.90421121\n",
      "  0.83776263  0.91102498  0.27111833  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 254\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192604362562155 R2: 0.01655325342430325 time: 1703260295.1321974\n",
      "batch_idx: 1 loss: 0.016374387788884094 R2: 0.016577187746467437 time: 1703260301.457626\n",
      "Training [85%] Loss: 0.014150215707252823 time: 1703260301.457626\n",
      "weight: [ 0.44771357 -0.11038431 -0.0535096   0.3064379   0.76424253  0.17028223\n",
      " -0.08807234  0.93386369  0.26995134  0.16080805  0.54873379  0.6918952\n",
      "  0.31989747  0.03518841  0.51765494  0.12738493  0.3253997   0.90430487\n",
      "  0.83805048  0.91075251  0.26888428  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 255\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011926215805282839 R2: 0.016602997051323287 time: 1703260307.8343828\n",
      "batch_idx: 1 loss: 0.01637270703624994 R2: 0.01662674683012577 time: 1703260314.282608\n",
      "Training [85%] Loss: 0.014149461420766389 time: 1703260314.282608\n",
      "weight: [ 0.44779717 -0.11076721 -0.05395119  0.30355786  0.76433452  0.17034202\n",
      " -0.08803703  0.93401509  0.26899697  0.16080805  0.54873379  0.6918952\n",
      "  0.32017623  0.03483032  0.51727205  0.12765891  0.3253997   0.90439686\n",
      "  0.83833574  0.91048076  0.26666537  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 256\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011926387023305982 R2: 0.016652409789301714 time: 1703260320.740224\n",
      "batch_idx: 1 loss: 0.01637103818665385 R2: 0.016675977764117778 time: 1703260327.2163312\n",
      "Training [85%] Loss: 0.014148712604979916 time: 1703260327.2163312\n",
      "weight: [ 0.4478823  -0.11114982 -0.05439193  0.30065611  0.76442481  0.17040018\n",
      " -0.08800232  0.9341615   0.26802312  0.16080805  0.54873379  0.6918952\n",
      "  0.32045486  0.03447226  0.51688943  0.12793013  0.3253997   0.90448716\n",
      "  0.83861841  0.91020975  0.2644616   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 257\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011926557309806653 R2: 0.01670149595053505 time: 1703260333.762422\n",
      "batch_idx: 1 loss: 0.016369381064700764 R2: 0.01672488482608514 time: 1703260340.3164935\n",
      "Training [86%] Loss: 0.014147969187253709 time: 1703260340.3164935\n",
      "weight: [ 0.44796893 -0.11153215 -0.05483183  0.29773237  0.76451341  0.17045673\n",
      " -0.08796823  0.93430301  0.26702987  0.16080805  0.54873379  0.6918952\n",
      "  0.32073334  0.03411424  0.5165071   0.12819866  0.3253997   0.90457575\n",
      "  0.83889848  0.90993948  0.26227292  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 258\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192672669510615 R2: 0.01675025980692746 time: 1703260346.903594\n",
      "batch_idx: 1 loss: 0.016367735495750803 R2: 0.016773472249590582 time: 1703260353.2925904\n",
      "Training [86%] Loss: 0.014147231095428477 time: 1703260353.2925904\n",
      "weight: [ 0.44805705 -0.11191419 -0.05527088  0.29478637  0.76460028  0.17051165\n",
      " -0.08793476  0.93443969  0.26601729  0.16080805  0.54873379  0.6918952\n",
      "  0.32101163  0.03375627  0.51612507  0.12846455  0.3253997   0.90466262\n",
      "  0.83917594  0.90966996  0.26009931  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 259\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192689520973181 R2: 0.016798705582952267 time: 1703260359.8527129\n",
      "batch_idx: 1 loss: 0.016366101306192503 R2: 0.016821744216933165 time: 1703260366.508921\n",
      "Training [86%] Loss: 0.014146498257962156 time: 1703260366.508921\n",
      "weight: [ 0.44814663 -0.11229591 -0.05570906  0.29181784  0.76468541  0.17056492\n",
      " -0.08790194  0.93457161  0.26498548  0.16080805  0.54873379  0.6918952\n",
      "  0.32128972  0.03339836  0.51574334  0.12872785  0.3253997   0.90474775\n",
      "  0.8394508   0.90940121  0.25794075  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 260\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011927062884378259 R2: 0.016846837448199037 time: 1703260373.6300404\n",
      "batch_idx: 1 loss: 0.016364478323748537 R2: 0.016869704851961043 time: 1703260380.3824658\n",
      "Training [87%] Loss: 0.014145770604063398 time: 1703260380.3824658\n",
      "weight: [ 0.44823764 -0.11267732 -0.05614638  0.28882652  0.76476879  0.17061655\n",
      " -0.08786977  0.93469886  0.26393451  0.16080805  0.54873379  0.6918952\n",
      "  0.32156758  0.03304052  0.51536194  0.12898862  0.3253997   0.90483113\n",
      "  0.83972305  0.90913322  0.25579719  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 261\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011927229749855736 R2: 0.01689465950969471 time: 1703260387.2011082\n",
      "batch_idx: 1 loss: 0.01636286637778148 R2: 0.016917358212587885 time: 1703260394.5663905\n",
      "Training [87%] Loss: 0.014145048063818607 time: 1703260394.5663905\n",
      "weight: [ 0.44833008 -0.1130584  -0.05658283  0.28581216  0.7648504   0.17066653\n",
      " -0.08783826  0.93482151  0.26286448  0.16080805  0.54873379  0.6918952\n",
      "  0.32184518  0.03268277  0.51498086  0.12924692  0.3253997   0.90491274\n",
      "  0.83999269  0.90886601  0.25366862  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 262\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011927395837052577 R2: 0.016942175804213866 time: 1703260401.9437273\n",
      "batch_idx: 1 loss: 0.016361265299589436 R2: 0.016964708282888874 time: 1703260408.4336941\n",
      "Training [87%] Loss: 0.014144330568321007 time: 1703260408.4336941\n",
      "weight: [ 0.44842392 -0.11343914 -0.05701841  0.28277455  0.76493023  0.17071484\n",
      " -0.08780743  0.93493965  0.26177549  0.16080805  0.54873379  0.6918952\n",
      "  0.3221225   0.03232512  0.51460012  0.12950279  0.3253997   0.90499257\n",
      "  0.84025971  0.90859959  0.25155498  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 263\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011927561176905015 R2: 0.016989390290483408 time: 1703260414.9372504\n",
      "batch_idx: 1 loss: 0.016359674922713424 R2: 0.017011758964930457 time: 1703260421.2121906\n",
      "Training [88%] Loss: 0.01414361804980922 time: 1703260421.2121906\n",
      "weight: [ 0.44851913 -0.11381953 -0.0574531   0.27971345  0.76500826  0.17076149\n",
      " -0.08777728  0.93505335  0.26066763  0.16080805  0.54873379  0.6918952\n",
      "  0.32239952  0.03196757  0.51421973  0.1297563   0.3253997   0.90507061\n",
      "  0.84052412  0.90833395  0.24945623  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 264\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.0119277258003471 R2: 0.017036306841085258 time: 1703260427.8722112\n",
      "batch_idx: 1 loss: 0.01635809508327791 R2: 0.01705851407052339 time: 1703260434.1775422\n",
      "Training [88%] Loss: 0.014142910441812505 time: 1703260434.1775422\n",
      "weight: [ 0.44861569 -0.11419956 -0.05788692  0.27662867  0.76508449  0.17080647\n",
      " -0.08774784  0.9351627   0.25954099  0.16080805  0.54873379  0.6918952\n",
      "  0.32267621  0.03161013  0.5138397   0.1300075   0.3253997   0.90514684\n",
      "  0.8407859   0.90806912  0.24737234  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 265\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011927889738232672 R2: 0.017082929233931177 time: 1703260440.6236684\n",
      "batch_idx: 1 loss: 0.01635652562035985 R2: 0.017104977312798364 time: 1703260447.2256596\n",
      "Training [88%] Loss: 0.014142207679296262 time: 1703260447.2256596\n",
      "weight: [ 0.44871358 -0.11457922 -0.05831984  0.27352002  0.7651589   0.17084976\n",
      " -0.08771911  0.93526779  0.2583957   0.16080805  0.54873379  0.6918952\n",
      "  0.32295256  0.03125283  0.51346004  0.13025643  0.3253997   0.90522125\n",
      "  0.84104505  0.90780509  0.24530326  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 266\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011928053021245725 R2: 0.017129261143463692 time: 1703260453.785158\n",
      "batch_idx: 1 loss: 0.016354966376366298 R2: 0.017151152297452676 time: 1703260460.0622807\n",
      "Training [89%] Loss: 0.014141509698806011 time: 1703260460.0622807\n",
      "weight: [ 0.44881278 -0.1149585  -0.05875187  0.27038734  0.76523148  0.17089138\n",
      " -0.0876911   0.93536869  0.25723184  0.16080805  0.54873379  0.6918952\n",
      "  0.32322853  0.03089567  0.51308076  0.13050315  0.3253997   0.90529383\n",
      "  0.84130158  0.90754186  0.24324895  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 267\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011928215679814985 R2: 0.017175306131704593 time: 1703260466.6525612\n",
      "batch_idx: 1 loss: 0.016353417197415683 R2: 0.01719704251358445 time: 1703260472.9523084\n",
      "Training [89%] Loss: 0.014140816438615334 time: 1703260472.9523084\n",
      "weight: [ 0.44891327 -0.11533739 -0.059183    0.26723049  0.76530222  0.1709313\n",
      " -0.08766383  0.93546549  0.25604954  0.16080805  0.54873379  0.6918952\n",
      "  0.32350411  0.03053866  0.51270187  0.13074771  0.3253997   0.90536457\n",
      "  0.84155548  0.90727946  0.24120934  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 268\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192837774402736 R2: 0.017221067639086485 time: 1703260479.5734851\n",
      "batch_idx: 1 loss: 0.016351877933736568 R2: 0.01724265132424685 time: 1703260486.2461047\n",
      "Training [89%] Loss: 0.014140127838881963 time: 1703260486.2461047\n",
      "weight: [ 0.44901501 -0.11571588 -0.05961323  0.26404935  0.76537111  0.17096953\n",
      " -0.0876373   0.93555829  0.25484892  0.16080805  0.54873379  0.6918952\n",
      "  0.32377927  0.03018181  0.51232338  0.13099017  0.3253997   0.90543345\n",
      "  0.84180675  0.90701787  0.23918441  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 269\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011928539243523661 R2: 0.01726654897495239 time: 1703260493.1156445\n",
      "batch_idx: 1 loss: 0.016350348440096345 R2: 0.017287981956811915 time: 1703260500.1398666\n",
      "Training [90%] Loss: 0.014139443841810003 time: 1703260500.1398666\n",
      "weight: [ 0.449118   -0.11609395 -0.06004255  0.26084382  0.76543813  0.17100607\n",
      " -0.08761153  0.93564716  0.25363009  0.16080805  0.54873379  0.6918952\n",
      "  0.324054    0.02982514  0.5119453   0.13123058  0.3253997   0.90550048\n",
      "  0.84205538  0.9067571   0.23717408  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 270\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011928700207372593 R2: 0.017311753307687527 time: 1703260507.047475\n",
      "batch_idx: 1 loss: 0.01634882857625545 R2: 0.017333037493117764 time: 1703260513.5326228\n",
      "Training [90%] Loss: 0.01413876439181402 time: 1703260513.5326228\n",
      "weight: [ 0.44922219 -0.11647161 -0.06047096  0.25761385  0.76550329  0.1710409\n",
      " -0.08758652  0.93573221  0.25239318  0.16080805  0.54873379  0.6918952\n",
      "  0.32432827  0.02946865  0.51156765  0.13146897  0.3253997   0.90556563\n",
      "  0.84230138  0.90649716  0.23517832  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 271\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011928860663933828 R2: 0.01735668365457066 time: 1703260520.07627\n",
      "batch_idx: 1 loss: 0.01634731820743456 R2: 0.017377820859313498 time: 1703260526.8172278\n",
      "Training [90%] Loss: 0.014138089435684193 time: 1703260526.8172278\n",
      "weight: [ 0.44932758 -0.11684883 -0.06089845  0.2543594   0.76556656  0.17107403\n",
      " -0.08756228  0.93581351  0.25113832  0.16080805  0.54873379  0.6918952\n",
      "  0.32460206  0.02911236  0.51119042  0.13170541  0.3253997   0.90562891\n",
      "  0.84254474  0.90623805  0.23319707  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 272\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011929020640718926 R2: 0.017401342871457115 time: 1703260533.4682007\n",
      "batch_idx: 1 loss: 0.016345817204792074 R2: 0.017422334815381024 time: 1703260540.102412\n",
      "Training [91%] Loss: 0.0141374189227555 time: 1703260540.102412\n",
      "weight: [ 0.44943413 -0.11722562 -0.06132501  0.25108046  0.76562794  0.17110545\n",
      " -0.08753883  0.93589118  0.24986566  0.16080805  0.54873379  0.6918952\n",
      "  0.32487535  0.02875628  0.51081364  0.13193995  0.3253997   0.90569029\n",
      "  0.84278545  0.90597977  0.23123026  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 273\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011929180164246466 R2: 0.01744573364227553 time: 1703260546.9424546\n",
      "batch_idx: 1 loss: 0.016344325445919378 R2: 0.01746658194445838 time: 1703260553.177564\n",
      "Training [91%] Loss: 0.014136752805082923 time: 1703260553.177564\n",
      "weight: [ 0.44954183 -0.11760195 -0.06175065  0.24777708  0.76568743  0.17113516\n",
      " -0.08751618  0.93596529  0.24857534  0.16080805  0.54873379  0.6918952\n",
      "  0.32514813  0.02840042  0.51043731  0.13217263  0.3253997   0.90574978\n",
      "  0.84302353  0.90572232  0.22927785  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 274\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011929339259881556 R2: 0.01748985846829132 time: 1703260559.7644281\n",
      "batch_idx: 1 loss: 0.01634284281535956 R2: 0.01751056464204026 time: 1703260566.1622126\n",
      "Training [91%] Loss: 0.01413609103762056 time: 1703260566.1622126\n",
      "weight: [ 0.44965064 -0.11797781 -0.06217536  0.24444932  0.76574501  0.17116315\n",
      " -0.08749433  0.93603595  0.24726751  0.16080805  0.54873379  0.6918952\n",
      "  0.32542037  0.02804479  0.51006144  0.13240349  0.3253997   0.90580736\n",
      "  0.84325895  0.90546571  0.22733978  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 275\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011929497951658584 R2: 0.017533719657165103 time: 1703260572.7522378\n",
      "batch_idx: 1 loss: 0.016341369205144982 R2: 0.017554285105092382 time: 1703260580.0156648\n",
      "Training [92%] Loss: 0.014135433578401784 time: 1703260580.0156648\n",
      "weight: [ 0.44976054 -0.11835321 -0.06259913  0.2410973   0.76580068  0.17118943\n",
      " -0.08747329  0.93610326  0.24594234  0.16080805  0.54873379  0.6918952\n",
      "  0.32569205  0.02768941  0.50968605  0.13263259  0.3253997   0.90586303\n",
      "  0.84349173  0.90520994  0.22541598  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 276\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011929656262094284 R2: 0.01757731931192341 time: 1703260587.2376647\n",
      "batch_idx: 1 loss: 0.016339904515343725 R2: 0.017597745321059954 time: 1703260594.2673657\n",
      "Training [92%] Loss: 0.014134780388719004 time: 1703260594.2673657\n",
      "weight: [ 0.44987151 -0.11872812 -0.06302196  0.23772118  0.76585443  0.17121399\n",
      " -0.08745308  0.93616731  0.24459999  0.16080805  0.54873379  0.6918952\n",
      "  0.32596316  0.02733427  0.50931113  0.13285998  0.3253997   0.90591678\n",
      "  0.84372187  0.90495501  0.2235064   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 277\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192981421199671 R2: 0.017620659319941834 time: 1703260600.6555076\n",
      "batch_idx: 1 loss: 0.016338448654611835 R2: 0.01764094705684105 time: 1703260607.3924139\n",
      "Training [92%] Loss: 0.014134131433304272 time: 1703260607.3924139\n",
      "weight: [ 0.44998352 -0.11910255 -0.06344384  0.23432116  0.76590625  0.17123683\n",
      " -0.0874337   0.93622821  0.24324065  0.16080805  0.54873379  0.6918952\n",
      "  0.32623368  0.02697941  0.50893671  0.13308569  0.3253997   0.9059686\n",
      "  0.84394935  0.90470092  0.22161098  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 278\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01192997182026832 R2: 0.017663741341995776 time: 1703260613.7924755\n",
      "batch_idx: 1 loss: 0.016337001540753095 R2: 0.017683891847855127 time: 1703260620.1980598\n",
      "Training [93%] Loss: 0.014133486680510707 time: 1703260620.1980598\n",
      "weight: [ 0.45009655 -0.11947647 -0.06386478  0.2308975   0.76595615  0.17125794\n",
      " -0.08741516  0.93628607  0.2418645   0.16080805  0.54873379  0.6918952\n",
      "  0.32650358  0.02662481  0.50856279  0.13330977  0.3253997   0.90601849\n",
      "  0.84417418  0.90444768  0.21972966  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 279\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011930129103697725 R2: 0.01770656680143512 time: 1703260626.7923374\n",
      "batch_idx: 1 loss: 0.016335563101286404 R2: 0.017726580987330154 time: 1703260633.302643\n",
      "Training [93%] Loss: 0.014132846102492064 time: 1703260633.302643\n",
      "weight: [ 0.45021056 -0.11984987 -0.06428475  0.2274505   0.7660041   0.17127733\n",
      " -0.08739747  0.93634099  0.24047175  0.16080805  0.54873379  0.6918952\n",
      "  0.32677286  0.02627051  0.50818938  0.13353227  0.3253997   0.90606645\n",
      "  0.84439636  0.90419527  0.21786237  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 280\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011930286076741453 R2: 0.017749136873556948 time: 1703260639.7654266\n",
      "batch_idx: 1 loss: 0.016334133274013764 R2: 0.017769015515904263 time: 1703260646.6022115\n",
      "Training [93%] Loss: 0.014132209675377608 time: 1703260646.6022115\n",
      "weight: [ 0.45032555 -0.12022276 -0.06470377  0.22398052  0.76605012  0.171295\n",
      " -0.08738064  0.93639309  0.2390626   0.16080805  0.54873379  0.6918952\n",
      "  0.32704149  0.02591651  0.50781649  0.13375323  0.3253997   0.90611246\n",
      "  0.84461588  0.90394371  0.21600906  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 281\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011930442751301045 R2: 0.017791452475357517 time: 1703260653.8877096\n",
      "batch_idx: 1 loss: 0.016332712007578794 R2: 0.017811196211628744 time: 1703260661.072651\n",
      "Training [94%] Loss: 0.01413157737943992 time: 1703260661.072651\n",
      "weight: [ 0.45044148 -0.12059512 -0.06512183  0.22048799  0.76609419  0.17131094\n",
      " -0.08736467  0.93644247  0.23763729  0.16080805  0.54873379  0.6918952\n",
      "  0.32730945  0.02556281  0.50744414  0.13397268  0.3253997   0.90615653\n",
      "  0.84483276  0.90369299  0.21416965  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 282\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01193059913650016 R2: 0.017833514255806593 time: 1703260667.992286\n",
      "batch_idx: 1 loss: 0.016331299262009984 R2: 0.017853123580502506 time: 1703260674.6124415\n",
      "Training [94%] Loss: 0.014130949199255071 time: 1703260674.6124415\n",
      "weight: [ 0.45055832 -0.12096693 -0.06553892  0.21697339  0.76613631  0.17132516\n",
      " -0.08734958  0.93648926  0.23619606  0.16080805  0.54873379  0.6918952\n",
      "  0.32757674  0.02520944  0.50707232  0.13419068  0.3253997   0.90619865\n",
      "  0.84504697  0.90344312  0.21234409  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 283\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01193075523846137 R2: 0.017875322586784592 time: 1703260681.0063381\n",
      "batch_idx: 1 loss: 0.016329895009244803 R2: 0.017894797847735067 time: 1703260687.6026332\n",
      "Training [94%] Loss: 0.014130325123853087 time: 1703260687.6026332\n",
      "weight: [ 0.45067606 -0.1213382  -0.06595504  0.21343726  0.76617647  0.17133765\n",
      " -0.08733537  0.93653358  0.23473915  0.16080805  0.54873379  0.6918952\n",
      "  0.32784334  0.02485639  0.50670105  0.13440726  0.3253997   0.90623882\n",
      "  0.84525854  0.90319409  0.21053231  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 284\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011930911060082046 R2: 0.017916877554794763 time: 1703260693.8745751\n",
      "batch_idx: 1 loss: 0.016328499233629105 R2: 0.01793621894990749 time: 1703260700.0672824\n",
      "Training [95%] Loss: 0.014129705146855576 time: 1703260700.0672824\n",
      "weight: [ 0.45079466 -0.12170891 -0.06637018  0.20988021  0.76621469  0.17134842\n",
      " -0.08732204  0.93657554  0.23326684  0.16080805  0.54873379  0.6918952\n",
      "  0.32810923  0.02450369  0.50633035  0.13462246  0.3253997   0.90627703\n",
      "  0.84546746  0.9029459   0.20873425  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 285\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011931066600811002 R2: 0.017958178953659587 time: 1703260706.4676473\n",
      "batch_idx: 1 loss: 0.01632711193238194 R2: 0.017977386528200245 time: 1703260712.7826624\n",
      "Training [95%] Loss: 0.014129089266596472 time: 1703260712.7826624\n",
      "weight: [ 0.4509141  -0.12207905 -0.06678435  0.20630292  0.76625095  0.17135747\n",
      " -0.08730961  0.93661528  0.23177942  0.16080805  0.54873379  0.6918952\n",
      "  0.32837439  0.02415134  0.5059602   0.13483632  0.3253997   0.90631329\n",
      "  0.84567372  0.90269856  0.20694985  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 286\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01193122185643257 R2: 0.017999226278371583 time: 1703260719.5375834\n",
      "batch_idx: 1 loss: 0.016325733116013656 R2: 0.01801829992286319 time: 1703260726.43754\n",
      "Training [95%] Loss: 0.014128477486223113 time: 1703260726.43754\n",
      "weight: [ 0.45103437 -0.12244862 -0.06719754  0.20270615  0.76628525  0.17136479\n",
      " -0.08729807  0.93665293  0.23027719  0.16080805  0.54873379  0.6918952\n",
      "  0.32863882  0.02379935  0.50559063  0.13504888  0.3253997   0.90634759\n",
      "  0.84587733  0.90245206  0.20517903  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 287\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011931376818861798 R2: 0.018040018720325346 time: 1703260733.1030123\n",
      "batch_idx: 1 loss: 0.016324362808688417 R2: 0.018058958169105965 time: 1703260739.5673196\n",
      "Training [96%] Loss: 0.014127869813775108 time: 1703260739.5673196\n",
      "weight: [ 0.45115542 -0.12281761 -0.06760974  0.19909071  0.76631759  0.1713704\n",
      " -0.08728744  0.93668862  0.22876048  0.16080805  0.54873379  0.6918952\n",
      "  0.32890249  0.02344773  0.50522165  0.13526017  0.3253997   0.90637993\n",
      "  0.8460783   0.90220641  0.20342173  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 288\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011931531475953036 R2: 0.0180805551641251 time: 1703260746.1723378\n",
      "batch_idx: 1 loss: 0.01632300104852285 R2: 0.01809935999464196 time: 1703260752.8535507\n",
      "Training [96%] Loss: 0.014127266262237943 time: 1703260752.8535507\n",
      "weight: [ 0.45127725 -0.12318601 -0.06802096  0.19545751  0.76634798  0.1713743\n",
      " -0.08727772  0.93672249  0.22722963  0.16080805  0.54873379  0.6918952\n",
      "  0.32916541  0.02309649  0.50485325  0.13547022  0.3253997   0.90641032\n",
      "  0.84627663  0.9019616   0.20167789  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 289\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011931685811323542 R2: 0.018120834186122203 time: 1703260759.2327225\n",
      "batch_idx: 1 loss: 0.0163216478878113 R2: 0.018139503819089242 time: 1703260765.713589\n",
      "Training [96%] Loss: 0.01412666684956742 time: 1703260765.713589\n",
      "weight: [ 0.45139983 -0.12355381 -0.06843119  0.1918075   0.76637641  0.17137648\n",
      " -0.0872689   0.9367547   0.22568501  0.16080805  0.54873379  0.6918952\n",
      "  0.32942756  0.02274563  0.50448545  0.13567907  0.3253997   0.90643875\n",
      "  0.84647231  0.90171764  0.19994744  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 290\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011931839804196027 R2: 0.0181608540549133 time: 1703260772.1364856\n",
      "batch_idx: 1 loss: 0.016320303393166376 R2: 0.018179387755418275 time: 1703260778.2756808\n",
      "Training [97%] Loss: 0.014126071598681202 time: 1703260778.2756808\n",
      "weight: [ 0.45152314 -0.123921   -0.06884043  0.18814175  0.76640289  0.17137696\n",
      " -0.087261    0.93678539  0.22412701  0.16080805  0.54873379  0.6918952\n",
      "  0.32968892  0.02239518  0.50411825  0.13588675  0.3253997   0.90646523\n",
      "  0.84666536  0.90147453  0.19823032  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 291\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.01193199342926548 R2: 0.01820061273399831 time: 1703260784.678406\n",
      "batch_idx: 1 loss: 0.01631896764556271 R2: 0.01821900961361833 time: 1703260790.792627\n",
      "Training [97%] Loss: 0.014125480537414095 time: 1703260790.792627\n",
      "weight: [ 0.45164715 -0.12428759 -0.06924868  0.18446136  0.76642742  0.17137573\n",
      " -0.08725401  0.93681471  0.22255604  0.16080805  0.54873379  0.6918952\n",
      "  0.32994949  0.02204513  0.50375167  0.1360933   0.3253997   0.90648976\n",
      "  0.84685578  0.90123226  0.19652645  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 292\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011932146656595016 R2: 0.018240107886773216 time: 1703260797.0058537\n",
      "batch_idx: 1 loss: 0.016317640740274496 R2: 0.01825836690675353 time: 1703260803.9523282\n",
      "Training [97%] Loss: 0.014124893698434756 time: 1703260803.9523282\n",
      "weight: [ 0.45177184 -0.12465356 -0.06965594  0.18076754  0.76645001  0.17137282\n",
      " -0.08724794  0.93684283  0.22097254  0.16080805  0.54873379  0.6918952\n",
      "  0.33020927  0.02169549  0.5033857   0.13629873  0.3253997   0.90651235\n",
      "  0.84704358  0.90099084  0.19483578  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 293\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011932299451543595 R2: 0.01827933688401573 time: 1703260810.802619\n",
      "batch_idx: 1 loss: 0.01631632278669834 R2: 0.0182974568595895 time: 1703260817.789429\n",
      "Training [98%] Loss: 0.014124311119120967 time: 1703260817.789429\n",
      "weight: [ 0.45189719 -0.12501891 -0.07006221  0.17706154  0.76647065  0.17136822\n",
      " -0.08724279  0.93686991  0.21937698  0.16080805  0.54873379  0.6918952\n",
      "  0.33046823  0.02134627  0.50302034  0.13650309  0.3253997   0.906533\n",
      "  0.84722876  0.90075027  0.19315822  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 294\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011932451774727662 R2: 0.018318296814011426 time: 1703260824.532345\n",
      "batch_idx: 1 loss: 0.016315013908054534 R2: 0.018336276419894437 time: 1703260831.0426664\n",
      "Training [98%] Loss: 0.014123732841391099 time: 1703260831.0426664\n",
      "weight: [ 0.45202319 -0.12538364 -0.07046749  0.17334472  0.76648937  0.17136193\n",
      " -0.08723854  0.93689612  0.21776985  0.16080805  0.54873379  0.6918952\n",
      "  0.33072638  0.02099747  0.50265562  0.13670639  0.3253997   0.90655171\n",
      "  0.84741133  0.90051056  0.19149373  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 295\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011932603582020892 R2: 0.018356984495389694 time: 1703260837.5225298\n",
      "batch_idx: 1 loss: 0.016313714240957945 R2: 0.018374822272530955 time: 1703260843.7822802\n",
      "Training [98%] Loss: 0.014123158911489419 time: 1703260843.7822802\n",
      "weight: [ 0.45214981 -0.12574773 -0.07087178  0.16961848  0.76650616  0.17135398\n",
      " -0.08723521  0.93692165  0.21615166  0.16080805  0.54873379  0.6918952\n",
      "  0.33098372  0.02064911  0.50229152  0.13690866  0.3253997   0.9065685\n",
      "  0.8475913   0.9002717   0.18984222  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 296\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011932754824595517 R2: 0.018395396492793537 time: 1703260850.072277\n",
      "batch_idx: 1 loss: 0.0163124239348536 R2: 0.018413090856351233 time: 1703260856.2772896\n",
      "Training [99%] Loss: 0.014122589379724559 time: 1703260856.2772896\n",
      "weight: [ 0.45227704 -0.1261112  -0.07127508  0.16588429  0.76652103  0.17134438\n",
      " -0.08723279  0.93694668  0.21452296  0.16080805  0.54873379  0.6918952\n",
      "  0.33124023  0.02030118  0.50192806  0.13710993  0.3253997   0.90658338\n",
      "  0.84776868  0.90003371  0.18820364  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 297\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011932905449008345 R2: 0.01843352913537155 time: 1703260862.6424222\n",
      "batch_idx: 1 loss: 0.016311143151313007 R2: 0.018451078383942443 time: 1703260868.8426692\n",
      "Training [99%] Loss: 0.014122024300160676 time: 1703260868.8426692\n",
      "weight: [ 0.45240485 -0.12647403 -0.0716774   0.1621437   0.766534    0.17133312\n",
      " -0.08723127  0.93697139  0.21288434  0.16080805  0.54873379  0.6918952\n",
      "  0.33149592  0.01995369  0.50156522  0.13731022  0.3253997   0.90659634\n",
      "  0.84794349  0.89979658  0.1865779   0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 298\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011933055397332411 R2: 0.018471378538090333 time: 1703260875.3922415\n",
      "batch_idx: 1 loss: 0.016309872063191774 R2: 0.018488780864152288 time: 1703260881.937301\n",
      "Training [99%] Loss: 0.014121463730262093 time: 1703260881.937301\n",
      "weight: [ 0.45253324 -0.12683623 -0.07207873  0.15839828  0.76654507  0.17132023\n",
      " -0.08723065  0.93699599  0.2112364   0.16080805  0.54873379  0.6918952\n",
      "  0.33175078  0.01960664  0.50120303  0.13750954  0.3253997   0.90660742\n",
      "  0.84811573  0.89956031  0.18496495  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 299\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011933204607334942 R2: 0.0185089406257839 time: 1703260888.4074624\n",
      "batch_idx: 1 loss: 0.016308610853649975 R2: 0.018526194127324702 time: 1703260895.5526757\n",
      "Training [100%] Loss: 0.014120907730492458 time: 1703260895.5526757\n",
      "weight: [ 0.45266218 -0.12719779 -0.07247909  0.1546497   0.76655426  0.17130572\n",
      " -0.08723092  0.93702067  0.20957978  0.16080805  0.54873379  0.6918952\n",
      "  0.33200481  0.01926004  0.50084147  0.13770792  0.3253997   0.90661661\n",
      "  0.84828543  0.89932493  0.18336471  0.5683086   0.09367477  0.3677158 ]\n",
      "epoch 300\n",
      "-------------------------------\n",
      "batch_idx: 0 loss: 0.011933353012702264 R2: 0.018546211159802595 time: 1703260902.6376657\n",
      "batch_idx: 1 loss: 0.016307359715038765 R2: 0.01856331385308896 time: 1703260909.9522393\n",
      "Training [100%] Loss: 0.014120356363870514 time: 1703260909.9522393\n",
      "weight: [ 0.45279167 -0.12755871 -0.07287847  0.15089964  0.76656158  0.1712896\n",
      " -0.08723207  0.93704564  0.20791514  0.16080805  0.54873379  0.6918952\n",
      "  0.33225802  0.01891389  0.50048055  0.13790538  0.3253997   0.90662393\n",
      "  0.84845259  0.89909042  0.18177712  0.5683086   0.09367477  0.3677158 ]\n",
      "train_MSE: 0.014133111036065548\n",
      "train_RMSE: 0.11888276172795427\n",
      "train_MAE: 0.10323898502907058\n",
      "train_MAPE: 0.22804988797728284\n",
      "train_R2: 0.01856331385308896\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "weights =[0.68756473 , 1.2864305 ,  0.5411694 , -0.01524038 , 0.5852044 ,  0.6762012,\n",
    "  0.35593897 , 0.22628789,  0.38244092,  0.35140917,  0.86482936 , 0.8531242,\n",
    "  0.09241156 , 0.6720707  , 0.38071635,  0.95416117 , 0.63409   ,  0.40179932,\n",
    "  0.7345088  , 0.6243114  , 0.3178202 , -0.2618623  , 0.18122938,  1.0447433,\n",
    "  0.48699683 , 0.7739934  , 0.38703147 , 0.48046085,  0.5525667 ,  0.52838504,\n",
    "  0.28538367 , 0.30099392,  0.74503726 , 0.67772216,  0.3839896 ,  0.417687]\n",
    "weights = npp.array(weights, requires_grad=True)\n",
    "'''\n",
    "weights = params\n",
    "loss_list=[]\n",
    "r2_list=[]\n",
    "\n",
    "n_epochs=300\n",
    "\n",
    "start_time=time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss  = []\n",
    "    total_r2  = []\n",
    "    print(f\"epoch {epoch+1}\\n-------------------------------\")\n",
    "    for it, batch_index in enumerate (chain(*[X_batches])):\n",
    "        # Update the weights by one optimizer step\n",
    "        \n",
    "        X_batch = X[batch_index]\n",
    "        Y_batch = Y[batch_index]\n",
    "        weights, _, _ = opt.step(cost, weights, X_batch, Y_batch)\n",
    "\n",
    "        # Compute accuracy\n",
    "        predictions = [QNN(weights, x) for x in X]\n",
    "        r2 = R2(Y, predictions)\n",
    "        cost_t=cost(weights,X_batch,Y_batch)\n",
    "        total_loss.append(cost_t)\n",
    "        total_r2.append(r2)\n",
    "        end_timet=time.time()\n",
    "        print(\"batch_idx:\",it,\"loss:\",cost_t,\"R2:\",r2,\"time:\",end_timet)\n",
    "    loss_list.append(sum(total_loss)/len(total_loss))\n",
    "    r2_list.append(sum(total_r2)/len(total_r2))\n",
    "    print('Training [{:.0f}%]'.format(100. * (epoch + 1) / n_epochs),\"Loss:\", loss_list[-1], \"time:\",end_timet)\n",
    "    print(\"weight:\",weights)\n",
    "    \n",
    "    \n",
    "predictions = [QNN(weights, x) for x in X]\n",
    "\n",
    "train_R2 = R2(Y, predictions)\n",
    "train_MSE=metrics.mean_squared_error(Y,predictions)\n",
    "train_RMSE=train_MSE**(1/2)\n",
    "train_MAE=metrics.mean_absolute_error(Y,predictions)\n",
    "train_MAPE=metrics.mean_absolute_percentage_error(Y,predictions)\n",
    "\n",
    "print(\"train_MSE:\",train_MSE)\n",
    "print(\"train_RMSE:\",train_RMSE)\n",
    "print(\"train_MAE:\",train_MAE)\n",
    "print(\"train_MAPE:\",train_MAPE)\n",
    "print(\"train_R2:\",train_R2)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "plt.title(\"MSE value against epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot(range(len(loss_list)), np.log(loss_list))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90deb265-6d46-4ecd-882d-03a49eb3e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=xtv_test_log\n",
    "Y_test=ytv_test_log\n",
    "test_predictions = [QNN(weights, x) for x in X_test]\n",
    "\n",
    "test_R2 = R2(Y_test, test_predictions)\n",
    "test_MSE=metrics.mean_squared_error(Y_test,test_predictions)\n",
    "test_RMSE=test_MSE**(1/2)\n",
    "test_MAE=metrics.mean_absolute_error(Y_test,test_predictions)\n",
    "test_MAPE=metrics.mean_absolute_percentage_error(Y_test,test_predictions)\n",
    "\n",
    "print(\"test_MSE:\",test_MSE)\n",
    "print(\"test_RMSE:\",test_RMSE)\n",
    "print(\"test_MAE:\",test_MAE)\n",
    "print(\"test_MAPE:\",test_MAPE)\n",
    "print(\"test_R2:\",test_R2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
